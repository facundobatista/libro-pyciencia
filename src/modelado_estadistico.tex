
% Copyright 2020-2023 Facundo Batista y Manuel Carlevaro
% Licencia CC BY-NC-SA 4.0
% Para más info visitar https://github.com/facundobatista/libro-pyciencia/

 \chapter{Modelado estadístico} \label{ch:modest}   

 \section{Introducción}

\begin{wraptable}{r}{5cm}
\begin{modulesinfo}
\begin{center}
{\small
    \begin{tabular}{l r}
        \toprule
        \textbf{Módulo} & \textbf{Versión} \\
        \midrule
        Matplotlib & 3.5.3 \\
        Pandas & 1.4.4 \\
        SciPy & 1.9.1 \\
        statsmodels & 0.13.2 \\
        \bottomrule
    \end{tabular}
    \vspace{0.75em}
    
    \href{https://github.com/facundobatista/libro-pyciencia/tree/master/código/modelado_estadistico/}{Código disponible}
}
\end{center}
\end{modulesinfo}
\end{wraptable}

Los modelos estadísticos, construidos a partir de un conjunto de datos observados, son utilizados en las diferentes disciplinas científicas con dos propósitos: explicación causal y predicción empírica. Ambos enfoques no son, naturalmente, mutuamente excluyentes.

Estos modelos generalmente contienen uno o varios parámetros, que pueden determinarse a partir de los datos a través de un procedimiento de ajuste para obtener los valores que mejor expliquen los datos observados. Posteriormente se pueden utilizar para predecir los valores de nuevas observaciones dados los valores de las variables independientes del modelo.

Una vez establecido el modelo, se pueden realizar comparaciones estadísticas entre los valores observados y los generados por el modelo para determinar cuán bien el modelo explica los datos, y qué parámetros contribuyen más al poder predictivo.

Abordaremos el siguiente problema: dado un conjunto de variables dependientes $Y$ (o respuesta), y variables independientes $X$, queremos determinar una relación matemática (o \textit{modelo}) como función $Y = f(X)$.

Si no conocemos la función $f$ pero tenemos acceso a datos provenientes de observaciones como pares $\{x_i, y_i\}$, podemos parametrizar la función y ajustar los valores de dichos parámetros utilizando los datos. Por ejemplo, podríamos tener el modelo lineal $f(X) = \beta_0 + \beta_1 X$ cuyos parámetros son $\beta_0$ y $\beta_1$. Por lo general tenemos más datos que parámetros en el modelo, y en tales casos podemos realizar un ajuste de mínimos cuadrados que minimiza la norma del residuo $r = Y - f(X)$.

Hemos presentado el modelo matemático representado por la función $f$, pero el ingrediente que hace al modelo \textit{estadístico} lo constituye cierta incertidumbre contenida en los datos $\{x_i, y_i\}$ en virtud de, por ejemplo, errores en las mediciones u otros factores fuera de control. Esta incerteza en los datos puede ser descripta en términos de variables aleatorias, por ejemplo, $Y = f(X) + \varepsilon$, donde $\varepsilon$ es una variable estocástica. Dependiendo de cómo aparece la variable aleatoria en el modelo, y de la distribución que la describe, podemos obtener distintos tipos de modelos estadísticos, los cuales requieren diferentes abordajes para sus análisis.

Una situación de uso habitual de los modelos estadísticos consiste en describir un conjunto $y_i$ de valores experimentales que se obtienen registrando en cada observación $x_i$ variables de control. Un elemento $x_i$ puede o no ser relevante para predecir (o explicar) la respuesta $y_i$, por lo que un aspecto importante del modelado estadístico consiste en determinar cuáles son las variables independientes relevantes. Por supuesto, también puede suceder que haya factores relevantes que no hayan sido incluidos en el conjunto de variables independientes $x_i$, pero que aporten a la determinación de $y_i$. En este caso puede no ser posible explicar con precisión los datos obtenidos con el modelo. La determinación de cuán precisa es la explicación de los datos con el modelo es otro aspecto importante del modelado estadístico.

\section{Coeficiente de correlación}
Al explorar conjuntos de datos de dos variables es natural preguntarse si existe alguna relación entre ellas. Por ejemplo, querríamos saber si al cambiar una de las variables, la otra también lo hace. Cuando detectamos que existe correlación entre las variables, es posible obtener una relación predictiva que puede resultar útil. Por ejemplo, la correlación existente entre la temperatura ambiente y el consumo de energía eléctrica permite anticipar que habrá un pico de consumo en días de calor extremo.

Si ambas variables están distribuidas normalmente, la medida mas común de dependencia entre estas cantidades es el coeficiente de correlación producto-momento de Pearson, o simplemente ``\textit{coeficiente de correlación de Pearson}''. Para el caso de muestras representadas por pares $\{x_i, y_i\}$, este coeficiente se define como:
\[ r_{xy} =\frac{\sum\limits_{i=1}^n{(x_i - \bar{x})(y_i - \bar{y})}} {\sqrt{\sum\limits_{i=1}^n(x_i - \bar{x})^2} \sqrt{\sum\limits_{i=1}^n(y_i - \bar{y})^2}}  \]

Si definimos la covariancia de la muestra como
\[ s_{xy} = \frac{\sum\limits_{i=1}^n {(x_i - \bar{x}) (y_i - \bar{y})}} {n - 1}  \]
y $s_x$ y $s_y$ como las desviaciones estándar muestrales de $x$ y $y$, respectivamente, el coeficiente de correlación $r_{xy}$ se puede escribir como
\[ r_{xy} = \frac{s_{xy}}{s_x \cdot s_y}  \]

El coeficiente de correlación de Pearson resulta entonces una cantidad adimensional que puede tomar cualquier valor entre $-1$ y $+1$. Un coeficiente de correlación nulo indica que no existe una relación lineal entre las variables, mientras que los valores extremos indican correlación lineal perfecta. Si la correlación es un número positivo, las variables están directamente relacionadas (esto es, el aumento de una variable implica el aumento de la otra), mientras que si el número es negativo, las variables están inversamente relacionadas (cuando una aumenta la otra disminuye su valor). La figura \ref{fig:me-pearson} muestra varios conjuntos de puntos y el valor del coeficiente de correlación correspondiente. 

\begin{figure}[t]
 \centering
 \includegraphics[width=1.0\textwidth]{Chapters/modelado_estadistico/figs/Correlation_examples2.pdf}
 % Correlation_examples2.pdf: 0x0 px, 300dpi, 0.00x0.00 cm, bb=
 \caption{Distintos conjuntos de valores $\{x, y\}$ con el coeficiente de Pearson para cada caso. En la primera file puede notarse que el coeficiente refleja tanto la dirección de una relación lineal como el apartamiento de la misma, pero no la pendiente de la relación lineal (como se aprecia en la línea media, no tampoco relaciones no lineales (línea inferior). Notar que la figura en el centro tiene pendiente nula, pero el coeficiente de correlación es indefinido. Fuente: \href{https://commons.wikimedia.org/w/index.php?curid=15165296}{Wikipedia} (accedida el 20.06.2022). }
 \label{fig:me-pearson}
%  \footnote{By DenisBoigelot, original uploader was Imagecreator - Own work, original uploader was Imagecreator, CC0, }
\end{figure}

Veremos un ejemplo de cálculo de coeficiente de correlación de Pearson analizando un conjunto de datos de atletas olímpicos recolectados desde los juegos de Atenas de 1896 hasta Río de Janeiro en 2016, con el propósito de obtener una estimación cuantitativa de las correlaciones entre el peso de los atletas y sus alturas y edades. Estos datos pueden descargarse libremente de kaggle\footnote{\url{https://www.kaggle.com/datasets/heesoo37/120-years-of-olympic-history-athletes-and-results/download}}, y analizaremos esta información utilizando las funcionalidades de Pandas.

En la celda 1 importamos los módulos requeridos para este ejemplo, y en la 2 generamos el \textit{dataset} a partir del archivo de datos descargados.


\jupynotex[1-2]{Chapters/modelado_estadistico/code/correlation.ipynb}

En la celda 3 exploramos la estructura del \textit{dataset}, y vemos que disponemos de 206.165 registros para analizar.

\jupynotex[3]{Chapters/modelado_estadistico/code/correlation.ipynb}

Para inspeccionar la forma en que se agrupan los puntos tomando los pares altura-peso y edad-peso, los graficamos en forma de puntos:

\jupynotex[4]{Chapters/modelado_estadistico/code/correlation.ipynb}

Obtenemos el coeficiente de correlación de Pearson entre las variables peso, altura y edad simplemente utilizando el método \mip{corr()} de los \textit{dataset} de Pandas:

\jupynotex[5]{Chapters/modelado_estadistico/code/correlation.ipynb}

Naturalmente, la correlación de una variable con si misma es perfecta, por lo que los elementos diagonales toman el valor uno. Tal como anticipa la figura de la celda 4, existe una mayor correlación entre la altura y peso de los atletas que entre la edad y el peso. Esta tabla muestra también una menor correlación aún entre la edad y la altura de los atletas.

El coeficiente de correlación de Pearson puede verse afectado por la existencia de valores extremos, que pueden exagerar o atenuar la intensidad de la relación lineal, y por lo tanto es inapropiado su uso cuando una o ambas variables no están normalmente distribuidas. Una de las formas más usadas para evaluar la correlación entre dos variables, tanto continuas como discretas, que obedecen a distribuciones sesgadas es la $\rho$ de Spearman. Esta métrica utiliza el \textit{orden} (o \textit{ranking}) en que se presentan los datos, en vez de sus valores en sí. Por ejemplo, podemos crear un \textit{dataframe} reducido del original conservando solo los primeros diez registros de las columnas peso y altura, y luego agregar tres columnas más conteniendo el orden de los valores peso y altura (generados con el método \mip{rankdata()} de \mip{scipy.stats}, y la diferencia $d$ entre ambos valores para el mismo registro:

\jupynotex[6]{Chapters/modelado_estadistico/code/correlation.ipynb}

Lo que muestra la celda 6 es que, para el registro 1, tanto el peso como la altura tienen el orden más bajo (1). Cuando los valores se repiten se toma el valor promedio del orden (en este caso, el valor de altura 185 ocupa los órdenes desde el 3 hasta el 8:  $(3+4+5+6+7+8)/6=5.5$). Con esta diferencia de orden $d$ se calcula el coeficiente de correlación de Spearman como:

\[ \rho = º - \frac{6 \sum\limits_{i=1}^n d_i^2}{n(n^2-1)}  \]
donde $d_i$ es la diferencia de orden entre los valores de $x$ y $y$. Podemos obtener el coeficiente de correlación de Spearman para el \textit{dataset} completo de los atletas olímipicos invocando nuevamente el método \mip{corr()} de Pandas, pero esta vez indicando que el método que queremos utilizar para el cálculo es \mip{'spearman'}:

\jupynotex[7]{Chapters/modelado_estadistico/code/correlation.ipynb}

El coeficiente de correlación de Spearman es más robusto ante la presencia de valores extremos, y es una medida del grado de correlación entre funciones monótonas (más que de la correlación lineal como el coeficiente de Pearson). También toma los valores extremos $+1$ y $-1$ cuando la correlación es perfecta (directa o inversa, respectivamente).


\section{Definición de modelos estadísticos con \textit{patsy}}

Al realizar un modelado estadístico es necesario especificar la relación matemática entre las variables dependientes $Y$ y las variables independientes $X$. Por ejemplo, en el caso de los modelos lineales, la variable $Y$ puede expresarse como una combinación lineal de las variables independientes $X$, o de funciones de estas variables. Por ejemplo, podríamos proponer que esta relación es:
\[ Y = \beta_1 X + \beta_2 X^3 + \cos(X)  \]

Es importante notar que no necesariamente $Y$ debe ser función lineal de $X$ en los modelos lineales, sino que debe ser lineal respecto de los coeficientes desconocidos $\beta_i$. Por otro lado, el modelo $Y = \exp(\beta_0 + \beta_1 X)$ es un ejemplo de modelo no lineal, ya que en este caso $Y$ no es una función lineal de $\beta_0$ y $\beta_1$. Sin embargo, podemos transformar este modelo en uno lineal tomando logaritmos en ambos miembros lo que conduce a $\widetilde{Y} = \beta_0 + \beta_1 X$ con $\widetilde{Y} = \log{Y}$. La clase de problemas que pueden transformarse en modelos lineales pueden ser tratados con el modelo lineal generalizado.

El paquete \textit{patsy}\footnote{\url{https://patsy.readthedocs.io/}.} de Python provee el acceso a un mini lenguaje que permite describir fórmulas para representar relaciones entre variables dependientes e independientes, y está basado en la notación introducida por Wilkinson y Rogers en 1973\cite{wilkinson1973}. Con este paquete, podemos escribir
\[ Y \sim X \]
para representar la relación directa entre la variable $Y$ y $X$. Podemos representar una relación más compleja en la que $Y$ depende de las variables $X$, $a$ y $b$ de la siguiente manera:
\[ Y \sim X + a + b + a : b \]

Los símbolos que se pueden utilizar para construir estas expresiones pueden verse en la Tabla \ref{tbl:me01}. La descripción detallada y completa del lenguaje se puede ver en la documentación del paquete.

\begin{table}[th] \small
\centering
 \caption{Elementos principales de la sintaxis de fórmulas con \textit{patsy}.}
 \label{tbl:me01}
%  \begin{tabular}{p{4.4cm} p{4.2cm} p{4cm}}
 \begin{tabular}{c p{0.75\textwidth}}
  \toprule
  \textbf{Operador} & \textbf{Significado}\\
  \midrule
  $\sim$ & Separa el lado derecho del izquierdo en la expresión. Si se omite se asume que solo existe el lado derecho. \\
  $+$ & Combina términos en ambos lados de la expresión (unión de conjuntos). \\
  $-$ & Elimina términos en el lado derecho del conjunto de términos del lado izquierdo (diferencia de conjuntos).\\
  $:$ & Término de interacción (por ejemplo $a:b$  denota $a \cdot b$ \\
  $*$ & $a * b$ es una forma abreviada de $a + b + a : b$ \\
  $/$ & $a / b$ es una forma abreviada de $a + a : b$ \\
  $**$ & Toma un conjunto de términos en el lado izquierdo y un entero $n$ en el derecho, y calcula el resultado de $*$ sobre el conjunto de términos con sí mismo $n$ veces.\\
  \mip{f(x)} & Función arbitraria de Python (o NumPy) utilizada para transformar términos en la expresión (por ejemplo: \mip{np.log(x)} \\
  \mip{C(x)} & Trata a la variable \mip{x} como categórica \\
  \bottomrule
\end{tabular}
\end{table}

Una vez definida la relación funcional entre las variables que constituyen el modelo, el paso siguiente es construir las llamadas matrices de diseño. Para un modelo lineal simple $Y = X \beta + \epsilon$ son necesarias dos matrices: una con los valores de la variable dependendiente $Y$ y otra con las variables dependientes del segundo miembro (más una columna opcional de unos que representan la ordenada al origen)\footnote{Se suelen nombrar también \textit{vector observación} $y$ y dejar específicamente \textit{matriz de diseño} para $X$. Usaremos estas notaciones indistintamente.}.
Los elementos $X_{ij}$ de la matriz de diseño $X$ son los valores de funciones de las variables independientes correspondientes a cada coeficiente $\beta_i$ y observación $y_i$. Por ejemplo, si los valores observados son $y = [ 1, 2, 3, 4, 5 ]$ para dos variables independientes con los valores correspondientes $x_1 = [6, 7, 8, 9, 10 ]$ y $x_2 = [13, 14, 15, 16, 17 ]$, y si el modelo en consideración es $Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_1 X_2$, la matriz de diseño para el lado derecho es $X = [\bm{1}, x_1, x_2, x_1 x_2]$. En las celdas del siguiente \textit{notebook} podemos ver de qué forma construimos el modelo con \textit{patsy} y obtenemos las matrices de diseño.

\jupynotex[1-3]{Chapters/modelado_estadistico/code/patsy_matrices.ipynb}

En las celdas 1 y 2 importamos los módulos necesarios y definimos los valores de nuestras variables dependiente e independientes, respectivamente. En la celda 3 utilizamos la función \mip{dmatrices()} para obtener las matrices de diseño correspondientes al modelo definido como argumento de dicha función, que se construyen utilizando \mip{data} ingresado como segundo argumento. Si solo queremos la matriz de diseño sin los valores de $y$, podemos usar \mip{dmatrix()} como se muestra en la celda siguiente:

\jupynotex[4]{Chapters/modelado_estadistico/code/patsy_matrices.ipynb}

La definición del modelo con \textit{patsy} consiste en definir una cadena de caracteres que representa la relación matemática entre las variables (nótese que no figuran los coeficientes $\beta_i$ en la fórmula ya que se asume implícitamente que cada témino de la fórmula tiene un parámetro del modelo como coeficiente), mientras que los datos utilizan una representación con diccionarios.

La matriz de diseño obtenida con \mip{dmatrix} es un objeto del tipo \mip{DesignMatrix} de \textit{patsy}. Este objeto es completamente compatible con los arrays de NumPy, e incluso se muestra como tal como podemos ver a continuación:

\jupynotex[5]{Chapters/modelado_estadistico/code/patsy_matrices.ipynb}

En forma alternativa, y dada la popularidad de Pandas para la representación de conjuntos de datos, podemos establecer el argumento \mip{return_type} de \mip{dmatrices} como \mip{"dataframe"}. Además, dado que los objetos \mip{DataFrame} se comportan como diccionarios, los podemos utilizar para definir los datos como segundo argumento de la función \mip{dmatrices}:

\jupynotex[6]{Chapters/modelado_estadistico/code/patsy_matrices.ipynb}

Una vez obtenidas las matrices de diseño con \textit{patsy}, requeridas para resolver un modelo estadístico, podemos utilizar un método de mínimos cuadrados como el de NumPy \mip{np.linalg.lstsq}:

\jupynotex[7]{Chapters/modelado_estadistico/code/patsy_matrices.ipynb}
O también usar cualquiera de los muchos modelos estadísticos provistos por la biblioteca statsmodels.

\section{Regresión lineal}
El modelado estadístico suele involucrar un análisis interactivo de los datos, comenzando con una inspección visual, buscando correlaciones y relaciones funcionales. A partir de esta primera inspección, se propone un modelo estadístico que podría describir los datos. En los casos más simples, las relaciones entre los datos se puede describir con un modelo lineal:

\[ Y = k X + d \]

A continuación se procede a determinar los parámetros del modelo (en este caso, $k$ y $d$). Luego es necesario determinar la calidad del modelo, y finalmente se analizan los residuos (diferencias entre los valores observados y los predichos) para verificar si el modelo propuesto falla en describir algunas características de los datos.

Si los residuos presentan valores altos u \textit{outliers}, o si sugieren una relación funcional diferente a la propuesta, es necesario modificar el modelo. Este procedimiento se repite hasta que los resultados sean satisfactorios.

La biblioteca statsmodels \cite{seabold2010} provee diversos modelos estadísticos que comparten el mismo modo de uso, lo que facilita cambiar entre diferentes modelos. Estos modelos estadísticos están representados por clases, que pueden ser inicializadas usando tanto las matrices de diseño como expresiones en patsy, y un \textit{dataframe} u otro objeto de tipo diccionario conteniendo los datos. Los pasos para establecer y analizar un modelo son los siguientes (asumiendo que importamos \mip{statsmodels.api} como \mip{sm} y \mip{statsmodels.formula.api} como \mip{smf}):
\begin{enumerate}
 \item Crear una instancia de la clase modelo, por ejemplo, \mip{modelo = sm.MODEL(Y, X)} o \\ \mip{modelo = smf.model(formula, data)}, utilizando las mayúsculas cuando usamos las matrices de diseño y las minúsculas para expresiones de patsy. \mip{MODEL} o \mip{model} denotan el nombre de un modelo particular como OLS (mínimos cuadrados ordinarios), GLS (mínimos cuadrados generalizados), etc.
 \item Instanciar un modelo no realiza ningún cálculo. Para ajustar el modelo a los datos se utiliza el método \mip{fit}, por ejemplo, \mip{resultados = modelo.fit()}, que realiza el ajuste y devuelve un objeto que contiene atributos y métodos para análisis posteriores.
 \item Inspeccionar el resumen de las estadísticas contenidas en el objeto resultado. Este objeto varía levemente entre diferentes modelos, pero la mayoría implementa el método \mip{summary} que produce una salida que describe el resultado del ajuste.
 \item Postprocesamiento de los resultados. Además del método \mip{summary}, el objeto resultado tiene métodos y atributos para obtener los parámetros ajustados (\mip{params}), los residuos (\mip{resid}), los valores ajustados (\mip{fittedvalues}) y un método para predecir los valores de las variables dependientes para nuevos valores de las variables independientes (\mip{predict}).
 \item Opcionalmente, suele ser útil visualizar el resultado del ajuste. Para ello se puede utilizar Matplotlib o algunas de las rutinas gráficas provistas por la bilbioteca statsmodels.
 \end{enumerate}

Recorreremos estos pasos utilizando un ejemplo en el cual conocemos la relación entre la variable dependiente y las independientes, y ajustaremos un modelo que genere una predicción cercana a estos datos a los que agregaremos ruido distribuido normalmente.

Vamos a considerar que la relación matemática que vincula las variables independientes $x_1$ y $x_2$ con la variable dependiente $y$ está dada por 
\begin{equation} \label{eq:rel_lin}
y = 4 + x_1 - 2 x_2 + 3 x_1 x_2
 \end{equation}

Para ello generaremos un conjunto de números aleatorios (estableciendo previamente un valor de la semilla del generador para poder reproducir los valores muestreados) y construiremos a partir de estos valores los correspondientes a la variable dependiente según la ecuación \eqref{eq:rel_lin}. En las siguientes dos celdas del \textit{notebook} importamos primero los módulos necesarios y construimos un \textit{dataframe} de Pandas conteniendo los valores de las variables dependientes $x_1$ y $x_2$ y la variable dependiente $y$:

\jupynotex[1-2]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb}

En la celda siguiente generamos un array de números aleatorios (\mip{epsilon}) que sumaremos a la variable dependiente, de modo de simular la presencia de ``ruido'' en los datos que queremos ajustar:

\jupynotex[3]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb}

Construimos ahora un primer modelo en el que suponemos que la relación entre las variables independientes y la dependiente es solo la suma de las primeras, omitiendo el término que contiene el producto $x_1 x_2$ de la ecuación \eqref{eq:rel_lin}, es decir, $Y = \beta_0 + \beta_1 x_1 + \beta_2 x_2$ que representamos con la fórmula de patsy \mip{"y ~ x1 + x2"} en la que hacemos uso de los datos ``con ruido'' de la variable dependiente. Utilizaremos el método de mínimos cuadrados ordinarios (u OLS por su sigla en inglés: \textit{Ordinary Least Squares}) para ajustar el modelo, para lo cual instanciaremos un objeto \mip{modelo_1} de la clase \mip{smf.ols}, e invocaremos al método \mip{fit()} de ese objeto. Para ver el resultado del ajuste, invocaremos el método \mip{summary()}:

\jupynotex[4]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb} 

La descripción detallada de la salida de \mip{summary()} excede el alcance de este libro, pero señalaremos algunos indicadores de la calidad del ajuste en la primera tabla. Entre ellos, \mip{R-squared} es un estadístico que indica la proporción de variación en la variable dependiente debida a la variación en las variables independientes, y comprende el rango entre 0 y 1, tomando el valor 1 cuando el ajuste es perfecto. Además de estar informado en la salida de \mip{summary()}, está contenido en el atributo \mip{rsquared} de \mip{resultado_1}, tal como se ve en la celda siguiente:

\jupynotex[5]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb} 

En este caso vemos que el ajuste es muy pobre, ya que apenas llega a $0.094$. El valor ajustado de $R^2$ (\mip{Adj. R-squared}) es una corrección de $R^2$ que penaliza el número de parámetros en el modelo. Otros indicadores útiles para comparar la calidad de ajustes entre modelos son \mip{AIC} (\textit{Akaike Information Criterion}) y \mip{BIC} (\textit{Bayesian Information Criterion}). Estos dos indicadores no nos dicen qué tan bueno es un ajuste individual, pero son útiles para decidir entre diferentes modelos ya que resulta mejor el que tiene valores menores de estos indicadores.

La segunda tabla de \mip{summary()} contiene los coeficientes $\beta_i$ obtenidos por el ajuste del modelo. Estos coeficientes (o pesos) de la regresión lineal están contenidos en el atributo \mip{params} de \mip{resultado_1} y se devuelve como un objeto \mip{Series} de pandas:

\jupynotex[6]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb} 

Asumiendo que los residuos están normalmente distribuidos, la columna \mip{std err} ofrece una estimación de los errores estándar de los coeficientes del modelo, así como el estadístico $t$ y su correspondiente $p$-valor para el test estadístico según el cual la hipótesis nula es que el correspondiente coeficiente es cero. Esta columna entonces (\mip{P>|t|}) nos permite juzgar cuáles variables independientes tienen coeficientes con probabilidad de ser diferente de cero, lo que significa que tienen un poder predictivo significativo. La hipótesis alternativa es que el predictor contribuye a la respuesta. Como es usual, establecemos un umbral $\alpha = 0.05$ o $0.001$, y si el $p$-valor es menor que el umbral ($P(T \geq |t|) < \alpha$), podemos rechazar la hipótesis nula. El $t$-\textit{test} nos permite evaluar la importancia de los diferentes predictores siempre que los residuos del modelo tengan una distribución normal alrededor de cero. Si esto no es el caso, el $t$-\textit{test} sugiere la existencia de alguna no linealidad entre las variables y por lo tanto no se puede utilizar para estimar la importancia de los predictores individuales.

Para tener una idea más precisa sobre el supuesto de que los errores están normalmente distribuidos, es necesario analizar los residuos del modelo que ajustamos a los datos. Los errores estándar de los coeficientes son las raíces cuadradas de los elementos diagonales de la matriz de covarianza que resulta de
\[ C = cov(\beta) = \sigma^2 (\bm{X} \bm{X}^T)^{-1} \]
donde $\bm{X}$ es la matriz de diseño y $\sigma^2$ es la varianza, o error cuadrático medio, de los residuos.

Podemos analizar los residuos para justificar la suposición de distribución normal. Éstos son accesibles mediante el atributo \mip{resid} del objeto \mip{resultado_1}:

\jupynotex[7]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb}

Podemos invocar la función \mip{normaltest()} de \mip{scipy.stats} para verificar la hipótesis que los residuos constituyen una muestra de una variable aleatoria con distribución normal (hipótesis nula). Si establecemos un $\alpha = 0.001$, y calculamos el $p$-valor:

\jupynotex[8]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb}

\noindent vemos que $p < \alpha$, con lo que podemos rechazar la hipótesis nula (es decir, podemos concluir que la suposición de que los residuos están normalmente distribuidos no se cumple). Una forma visual de verificar la normalidad de los residuos es a través de un gráfico Q-Q (\textit{Quantile-Quantile plot}), que compara los cuantiles empíricos de la muestra con los valores teóricos, y que si los valores muestrales están normalmente distribuidos la figura debería aproximarse a una línea recta. Para nuestro caso, podemos generar el gráfico Q-Q mediante \mip{smg.qqplot}:

\jupynotex[9]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb}

Como se puede ver en la salida de la celda 9, los puntos se desvían notablemente de una recta, sugiriendo que es poco probable que los residuos observados sean una muestra de una variable aleatoria distribuida normalmente. 

Por último, mencionamos que en la segunda tabla de la salida de \mip{summary()} se muestran los intervalos de confianza para cada predictor. Un intervalo de confianza pequeño indica que el ajuste permite establecer con mayor precisión el valor de estos predictores, mientras que intervalos más grandes indican una mayor incerteza o varianza en estos parámetros. Al igual que los valores de \mip{AIC} y \mip{BIC}, los intervalos de confianza resultan útiles a la hora de comparar el grado de ajuste entre diferentes modelos.

Los resultados previos sugieren que el modelo propuesto no es suficiente y que necesitamos mejorarlo. Podemos entonces incluir el término de interacción en la fórmula de patsy y repetir los análisis:


\jupynotex[10]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb}

Ahora vemos que varios indicadores muestran un mejor ajuste del modelo. Para empezar, $R^2$ está más cerca de la unidad:

\jupynotex[11]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb}

También los valores de \mip{AIC} y \mip{BIC} son menores que en el caso anterior, indicando una mejora en la calidad del modelo. Si realizamos un $t$-\textit{test} sobre los residuos obtenemos:

\jupynotex[12]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb}

\noindent y ahora el $p$-valor es mayor que nuestro umbral, con lo que no podemos rechazar la hipótesis nula que indica la normalidad de la distribución de residuos. Esto se visualiza con el gráfico Q-Q, que se aproxima mejor a una recta:

\jupynotex[13]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb}

Los parámetros de ajustes del modelo mejorado son:

\jupynotex[14]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb}

\noindent que son bastante cercanos a los modelos ``verdaderos'' con los cuales generamos los datos (sin el ruido agregado). Finalmente, podemos comparar los valores que se obtienen mediante la función correcta y los predichos por el modelo en forma gráfica:


\jupynotex[15-16]{Chapters/modelado_estadistico/code/modelo_lineal.ipynb}

En el panel de la izquierda mostramos los valores obtenidos utilizando la relación inicial verdadera entre las variables independientes $x_1$ y $x_2$, mientras que en el panel de la derecha visualizamos la predicción. De este modo podemos cuantificar en forma visual el grado de precisión de nuestro modelo.

\section{Regresión discreta}

Los modelos de regresión discreta son aquellos en los que la variable dependiente toma valores discretos. El más simple de los casos es el que la variable dependiente $y$ es binaria (solo puede tomar dos valores, que por conveniencia y sin perder generalidad, denotamos por $0$ y $1$). Por ejemplo, podemos definir que $y = 1$ si un individuo es adulto y $y = 0$ si no lo es; $1$ si hace deportes y $0$ en caso contrario; $1$ si un vehículo tiene motor eléctrico y $0$ si no; etc.

Cuando la variable dependiente toma más de dos valores, podemos clasificar los diferentes casos en variables categóricas y no categóricas. Como ejemplo del primer caso, podemos categorizar individuos según su rango etario:
\begin{itemize}
 \item $y = 1$ si la edad es menor a 20 años,
 \item $y = 2$ si la edad es entre 20 y 60 años,
 \item $y = 3$ si la edad es mayor a 60 años,
\end{itemize}

Como ejemplo de una variable no categórica podemos mencionar el número de unidades vendidas por una empresa durante un día. En este caso $y$ puede tomar los valores $0, 1, 2, \ldots$, es decir, valores discretos sin ser una variable categórica.

Los métodos usuales de regresión lineal que vimos hasta ahora no son suficientes, dado que en esos casos la variable de respuesta obtenida es continua, y no puede ser utilizada directamente para modelar una variable que solo puede tomar valores discretos. Sin embargo, es posible mapear un predictor lineal mediante una transformación adecuada a un intervalo que se puede interpretar como una probabilidad para diferentes resultados discretos. Para el caso de variables de respuesta binaria, una transformación muy utilizada es la función logística:
 \[ \log \left( \frac{p}{1-p} \right) = \beta_0 + \beta_1 x \] 
 o
 \[ p = \left( 1 + \exp(-\beta_0 -\beta_1 x) \right)^{-1} \]
 que mapea $x \in (-\infty, \infty)$ a $p \in (0, 1)$, y cuya representación gráfica se muestra en la figura \ref{fig:logplot}. Es decir, que la variable independiente (continua o discreta) $x$ es mapeada a través de los parámetros del modelo lineal $\beta_0$ y $\beta_1$, y posteriormente utilizando la función logística transformamos la salida del modelo lineal en una probabilidad $p$. Si $p < 0.5$ podemos decir que $y = 0$, y si $p \geq 0.5$, la predicción es que $y = 1$. 
 
 \begin{figure}[ht]
 \centering
 \includegraphics[width=0.6\textwidth]{Chapters/modelado_estadistico/figs/log-plot.pdf}
 \caption{Representación gráfica de la función logística con los valores $\beta_0 = 0$ y $\beta_1 = 1$.}
 \label{fig:logplot}
\end{figure}


 La biblioteca statsmodels ofrece varios métodos para regresiones discretas, incluyendo la clase \mip{Logit} para regresiones binarias, la clase para regresión logística multinomial \mip{MNLogit} (para más de dos categorías), y la clase \mip{Poisson} para variables no categóricas con distribución de Poisson (enteros positivos). Vamos a desarrollar un ejemplo de regresión discreta binaria utilizando el famoso conjunto de datos de flor Iris de Fisher\footnote{Se puede leer más sobre este conjunto de datos en la entrada de \href{https://es.wikipedia.org/wiki/Conjunto_de_datos_flor_iris}{Wikipedia}.}. Este conjunto contiene 50 muestras de cada una de las tres especies de Iris: setosa, virginica e versicolor. En cada muestra se midieron el largo y el ancho del sépalo y del pétalo (en centímetros). En el ejemplo, tomaremos un subconjunto de los datos pertenecientes solo a dos especies e intentaremos construir un modelo que prediga, en términos de los valores de las dimensiones del sépalo y del pétalo, a cuál de las dos especies corresponden esos valores.
 
 Como es habitual, en la primera celda del \textit{jupyter-notebook} importamos los módulos necesarios. A continuación, utilizamos el submódulo \mip{datasets} de statsmodels cuya función \mip{get_rdataset} provee acceso a conjuntos de datos utilizados en el popular lenguaje R\footnote{Los conjuntos de datos están disponibles en \url{https://vincentarelbundock.github.io/Rdatasets/}.}. 
 
 \jupynotex[1-2]{Chapters/modelado_estadistico/code/discreto.ipynb}
 
 Extrayendo los valores únicos en la columna \mip{Species} de nuestro \textit{dataset}, vemos que están las tres especies de Iris:
 
 \jupynotex[3]{Chapters/modelado_estadistico/code/discreto.ipynb}
 
 Construimos ahora un nuevo \textit{dataframe} que constituye un subconjunto del original, incluyendo solo las especies versicolor y setosa, y reasignamos los valores de la columna \mip{Species} a los valores \mip{0} o \mip{1} para versicolor y virginica respectivamente. Finalmente, antes de proceder con la construcción del modelo, es necesario renombrar las columnas del \textit{dataframe} reemplazando los puntos por otro carácter de modo que Python pueda interpretar correctamente su nombre. Si pasamos como nombre de variable en expresiones Patsy \mip{"Sepal.Length"}, Python intentará encontrar una variable llamada \mip{Sepal} que tiene un atributo \mip{Length}. Evitamos esto reemplazando los puntos por, por ejemplo, el carácter \mip{"_"}:
 
 \jupynotex[4]{Chapters/modelado_estadistico/code/discreto.ipynb}
 
 Podemos construir ahora el modelo utilizando la clase \mip{Logit}, y realizamos el ajuste con su método \mip{fit()} utilizando como variables independientes las medidas del pétalo (largo y ancho):
 
  
 \jupynotex[5]{Chapters/modelado_estadistico/code/discreto.ipynb}
 
 Como ya vimos, los resultados del ajuste se pueden mostrar con el método \mip{summary()}:
 
 \jupynotex[6]{Chapters/modelado_estadistico/code/discreto.ipynb}
 
 Al igual que en los ejemplos precedentes, no haremos un análisis exhaustivo de toda la información que muestra \mip{summary()} sino que solo centraremos nuestra atención en los parámetros del ajuste (\mip{Intercept}, \mip{Petal_Length} y \mip{Petal_Width}) ya que éstos nos permitirán realizar predicciones sobre nuevos valores de mediciones del largo y ancho del pétalo. Podemos simular nuevas mediciones generando valores con variables aleatorias dentro de los rangos de variación de estas magnitudes, los cuales registramos en un nuevo \textit{dataframe} \mip{df_nuevo}. Utilizando el ajuste del modelo podemos predecir a qué especie pertenece cada ``nuevo'' valor, tomando el criterio de asignar el valor \mip{1} o \mip{0} según la predicción sea mayor o menor a $0.5$:
 
 \jupynotex[7]{Chapters/modelado_estadistico/code/discreto.ipynb}
 
 Para tener una apreciación visual del desempeño del modelo en la clasificación de las especies y en la predicción de los resultados, podemos realizar un gráfico que muestre los valores originales (de los cuales conocemos a qué especie pertenecen), la recta que separa ambas clasificaciones, y las predicciones sobre los nuevos valores obtenidos aleatoriamente:
 
 \jupynotex[8]{Chapters/modelado_estadistico/code/discreto.ipynb}
 
 Vemos que algunos valores originales próximos a la recta de separación se encuentran en el lado erróneo de la clasificación, pero las predicciones sobre los valores generados aleatoriamente son muy buenas.
 
 Como último ejemplo de modelado sobre variables discretas, intentaremos ahora realizar un ajuste sobre un conjunto de datos que registra el número de visitas al médico en las últimas dos semanas para una muestra de 5.190 adultos de la Encuesta de Salud Australiana 1977-78\footnote{El conjunto de datos se puede descargar desde \href{https://vincentarelbundock.github.io/Rdatasets/csv/AER/DoctorVisits.csv}{este enlace}.}. Este y otros conjuntos de datos acerca del uso de servicios de salud fueron objeto de análisis por A. Cameron y P. Trivedi en el contexto de un modelo económico \cite{cameron1986}.
 
 El modelo discreto más simple que podemos considerar para modelar datos que representan conteos es el de Poisson, que expresa la probabilidad de un número dado de eventos que ocurren en un intervalo de tiempo (o espacio) fijo, en el caso en que estos eventos ocurran con una frecuencia media constante e independientemente del intervalo de tiempo desde el último evento. Si suponemos que los eventos ocurren aleatoriamente en el tiempo de modo que se cumplan las siguientes condiciones:
 \begin{itemize}
  \item La probabilidad de ocurrencia de al menos un evento en un intervalo dado es proporcional a la longitud del intervalo.
  \item La probabilidad de dos o más ocurrencias de eventos en un intervalo muy pequeño es despreciable.
  \item El número de ocurrencias de eventos en intervalos de tiempos disjuntos son mutuamente independientes.
 \end{itemize}
 entonces la distribución de probabilidad del número de ocurrencias de eventos en un intervalo de tiempo fijo es una distribución de Poisson con media $\mu = \lambda \, t$, donde $\lambda$ es la frecuencia de ocurrencia del evento por unidad de tiempo y $t$ es la longitud del intervalo de tiempo. Un proceso que satisface las tres condiciones mencionadas se denomina proceso de Poisson. Ejemplos modelados como procesos de Poisson que podemos mencionar son: la cantidad de goles anotados en un partido de fútbol, potenciales de acción emitidos por una neurona o la cantidad de vehículos que pasan por una calle. 
 
 Formalmente, si $Y_i$ denota el número de ocurrencias para el $i$-ésimo de $N$ individuos, de un evento de interés en un intervalo dado de tiempo, la densidad de probabilidad está dada por:
 
 \[ P(Y_i = y_i) = \frac{\lambda_i^{y_i} e^{-\lambda_i}}{y_i!}, \quad y_i = 0, 1, 2, \ldots, \quad i = 1, 2, \ldots, N  \]
 donde $y_i$ es el valor obtenido de la variable aleatoria. Esta es una distribución con un solo parámetro en la cual el valor medio y varianza de $Y_i$ son ambos iguales a $\lambda_i$. Para incoporar un conjunto de variables independientes $X_{ij}(j = 1, \ldots, K)$, incluyendo un término constante, el parámetro $\lambda$ resulta:
 \[ \lambda_i = \exp(\bm{X}_i \bm{\beta}) \]

Volviendo al ejemplo, vemos en la celda 9 que leemos el archivo \mip{DoctorVisits.csv} y almacenamos sus datos en el \textit{dataframe} \mip{df}. Invocando el método \mip{info()} podemos ver los nombres de las columnas, la cantidad de valores registrados y los tipos de datos. En la celda siguiente podemos ver que la columna \mip{'visits'} contiene 10 valores y la cantidad de veces que aparece cada uno de ellos en los datos.
 
\jupynotex[9-10]{Chapters/modelado_estadistico/code/discreto.ipynb}

En la celda 11 instanciamos el objeto \mip{modelo} especificando que es un modelo de Poisson en el cual la columna \mip{'visits'} del \textit{dataframe} constituye nuestra variable dependiente, a la que queremos explicar solo a través de una constante, sin relacionarla aún con otros campos del \textit{dataframe}. A continuación realizamos el ajuste del modelo con el método \mip{fit()} asignándolo al objeto \mip{resultado}, y mostramos los resultados del dicho ajuste mediante el método \mip{summary()}:

\jupynotex[11]{Chapters/modelado_estadistico/code/discreto.ipynb}

El resultado que estamos buscando aquí es el valor de \mip{Intercept}, que es el que podemos vincular con el parámetro $\lambda$ de la distribución. En la celda siguiente calculamos este parámetro y generamos tres distribuciones de Poisson: una con el parámetro recién obtenido, y dos utilizando los valores extremos del intervalo de confianza del 95\% de \mip{Intercept} informados en la celda 11:

\jupynotex[12]{Chapters/modelado_estadistico/code/discreto.ipynb}

Utilizando estas tres distribuciones, compararemos el histograma de visitas de los datos (normalizado con el número total de visitas) con las funciones de masa de probabilidad de las distribuciones, obtenidas mediante el método \mip{pmf()}\footnote{La función de masa de probabilidad devuelve la probabilidad de que una variable aleatoria discreta sea exactamente igual a un valor dado.}. Generamos esta comparación en forma gráfica, como se ve en la celda a continuación:

\jupynotex[13]{Chapters/modelado_estadistico/code/discreto.ipynb}

Podemos ver que los valores del histograma no están comprendidos entre las distribuciones con los límites superior e inferior del intervalo de confianza del parámetro ajustado, por lo que debemos rechazar la hipótesis de que el número de visitas al médico en las últimas dos semanas es un proceso de Poisson. No tener éxito en el ajuste de un modelo a un conjunto de datos es habitual durante el proceso de modelado estadístico, y tal vez podríamos obtener mejores resultados si intentamos explicar nuestra variable dependiente con algunas de las columnas del \textit{dataframe}. Por ejemplo, podríamos reemplazar la expresión del modelo en la celda 11 por \mip{"visits ~ 1 + age + illness * income"}. Dejamos aquí al lector la tarea de explorar estas posibilidades, que resultan bastante más complejas que el caso que usamos como ejemplo.
  
\section{Series temporales}

Una serie temporal consiste en el registro de datos sucesivos en el tiempo. Las técnicas utilizadas para analizar series temporales tratan de identificar patrones de los datos en el pasado e intentan anticipar (o predecir) los valores futuros. Estos datos se encuentran ordenados cronológicamente:
\[ y_0, y_1, \ldots, y_{t-2}, y_{t-1}, y_{t} \]

El análisis de series temporales presenta una característica distintiva respecto de los análisis realizados hasta aquí: no se puede considerar que un conjunto de observaciones de una serie temporal representen una muestra de valores aleatorios de una población, ya que existe una fuerte correlación entre observaciones que están cercanas en el tiempo. Por otra parte, se pueden identificar los valores futuros de una serie temporal como las variables dependientes, que son predichas por los valores pasados de la serie (variables independientes). Esto se refuerza cuando existen autocorrelaciones como ciclos diarios o anuales, o fuertes tendencias que se mantienen en el tiempo. Ejemplos de series temporales son las observaciones climáticas, orográficas, poblacionales, precios de acciones, etc.

El modelo más simple que podemos considerar es el autoregresivo AR (\textit{``autoregressive model''}), que utiliza una combinación lineal de $p$ valores pasados para estimar el presente. La expresión matemática para el modelo AR($p$) es:
\[ y_t = \phi_0 + \sum_{n=1}^p {\phi_n y_{t-n}} + \varepsilon_t \]

Aquí, $p$ es el orden del modelo y $\varepsilon_t$ representa ruido blanco sin correlación. Los modelos autoregresivos se aplican a procesos estacionarios, es decir, aquellos en los cuales sus propiedades estadísticas no varían en el tiempo. Esta condición impone restricciones sobre los valores que pueden tener los coeficientes $\phi_i$.

Otro algoritmo estadístico simple para modelar series temporales es el de la media móvil MA (\textit{``moving average''}), que utiliza la dependencia entre una observación dada y los residuos calculados a partir de observaciones previas. Estos residuos consisten en la diferencia entre los valores registrados en el instante $t$ y el promedio (o media móvil) de un subconjunto precedente de $q$ datos. Matemáticamente el modelo MA$(q)$ se expresa como:
\[ y_t = \mu + \sum_{n=1}^q {\theta_n \varepsilon_{t-n}} + \varepsilon_t \]
donde $\theta_i$ son los parámetros del modelo, $\mu$ es la media de la serie y $\varepsilon_i$ los residuos.

Estos abordajes pueden combinarse dando origen al modelo ARMA$(p, q)$, pero tal como mencionamos, solo puede aplicarse a series que sean estacionarias. Vamos a intentar modelar ahora una serie temporal que registra mediciones atmosféricas de CO$_2$ desde 1958 en el Observatorio Mauna Loa \cite{keeling2005}, en partes por millón (ppm). Para ello utilizaremos las herramientas disponibles en el módulo statsmodels.

Como es usual, iniciamos el \textit{notebook} importando los módulos necesarios:
\jupynotex[1]{Chapters/modelado_estadistico/code/co2.ipynb}

En la celda 2 leemos los datos desde los \textit{datasets} disponibles en statsmodels, podemos ver que obtenemos un \textit{dataframe} de pandas con 2284 registros, ordenados por fecha:
\jupynotex[2]{Chapters/modelado_estadistico/code/co2.ipynb}

Antes de comenzar el análisis, vamos a transformar nuestra serie temporal original en otra cuyos índices temporales sean el primer día de cada mes conteniendo el valor medio de los datos del mes correspondiente. Además buscamos fechas sin registros (encontramos 5) y completamos esos registros artificialmente con el valor precedente. Finalmente graficamos la serie temporal:
\jupynotex[3]{Chapters/modelado_estadistico/code/co2.ipynb}

De este modo, finalizamos con una serie temporal con 526 valores. En la figura se observa claramente que existe una tendencia creciente de la concentración de CO$_2$ con el tiempo, a la que se le superpone una oscilación estacional. Además de la inspección visual, es útil disponer de una herramienta cuantitativa para determinar la estacionalidad de una serie temporal. En nuestro caso, haremos uso del \textit{test} de Dickey-Fuller aumentado\footnote{Ver la siguiente \href{https://es.wikipedia.org/wiki/Prueba_de_Dickey-Fuller_aumentada}{entrada} en Wikipedia.} cuya hipótesis nula consiste en que la serie no es estacionaria, por lo que al obtener el $p$-valor del \textit{test} por encima del valor $0.05$ no podemos asegurar que la serie no es estacionaria con el 95\% de confianza. En la celda siguiente definimos una función que calcular este $p$-valor y nos informa para distintos valores críticos:
\jupynotex[4]{Chapters/modelado_estadistico/code/co2.ipynb}

Aplicamos esta función con nuestros datos, y obtenemos obviamente lo que vimos en la figura:
\jupynotex[5]{Chapters/modelado_estadistico/code/co2.ipynb}

Una herramienta práctica para descomponer nuestra serie original en términos de tendencia, estacionales y residuos, es la función \mip{seasonal_decompose()} de \mip{statsmodels.tsa.seasonal}. Usando nuestros datos \mip{y} como argumento, y utilizando un método aditivo para la descomposición, obtenemos:
\jupynotex[6]{Chapters/modelado_estadistico/code/co2.ipynb}

Podemos entonces repetir nuestro \textit{test} sobre la parte estacional para ver si es estacionario, lo que se confirma claramente:
\jupynotex[7]{Chapters/modelado_estadistico/code/co2.ipynb}

Es posible transformar una serie temporal no estacionaria en una que sí lo es a través de calcular diferencias entre los sucesivos valores. Si, por ejemplo, la línea de tendencia es lineal, generar una nueva serie temporal a través de la primera diferencia $\bar{y}_t = y_t - y_{t-1}$ produce una serie estacionaria, a la cual le podemos aplicar un modelo ARMA. Aplicando esto a nuestra serie temporal obtenemos:

\jupynotex[8]{Chapters/modelado_estadistico/code/co2.ipynb}

Podemos generar un modelo que contemple la ``desestacionalización'' mediante diferencias de orden $d$ con un modelo ARIMA$(p, d, q)$, donde I$(d)$ es el preprocesamiento que realiza el cálculo de las diferencias.

El tratamiento de series temporales con componentes estacionales se puede realizar directamente con un modelo SARIMAX, con considera además de la no estacionalidad, la posibilidad de incoporar otras variables exógenas al modelo. Intentaremos entonces trabajar sobre nuestra serie original ajustándola con un modelo SARIMAX que requiere, además de los parámetros $p$, $d$ y $q$, parámetros adicionales para tratar con la componente estacional: $P$, $Q$, $D$ y $m$, siendo este último parámetro el número de pasos temporales que comprende un período estacional.

Una forma de estimar estos parámetros es utilizando los gráficos de las funciones de autocorrelación (ACF) y autocorrelación parcial (PACF). La ACF es la correlación de una serie temporal con sí misma en función del retraso o \textit{lag}, mientras que la PACF es la correlación entre la serie temporal y una versión retrasada de si misma, sustrayendo el efecto de la correlación de \textit{lags} menores. En el caso de nuestro ejemplo, podemos graficar estas funciones con las herramientas provistas por statsmodels:

\jupynotex[9]{Chapters/modelado_estadistico/code/co2.ipynb}

En estas figuras, las zonas sombreadas representan el intervalo de confianza del 95\%, por lo que los valores que quedan dentro de estas regiones podrían considerarse nulos. Se puede ver que para la serie analizada existen fuertes correlaciones de período 12 (que representan un año en nuestros datos) en la PACF, y un decaimiento muy suave para la dfunción de autocorrelación. Se suele utilizar como parámetro $p$ el último valor del \textit{lag} de la PACF antes del primero que se anula, y el correspondiente \textit{lag} de la ACF para determinar $q$. La figura de la PACF sugiere entonces usar un valor de $p=2$, pero no es concluyente para determinar $d$ debido a que el decaimiento de la función es muy lento.

Entonces podemos recurrir a otros indicadores para determinar los parámetros del modelo. Al realizar un ajuste SARIMAX$(p, d, q)(P, D, Q)_m$, obtenemos los valores de AIC (\textit{Akaike Information Criterion}) y BIC (\textit{Bayesian Information Criterion}). En ambos casos, menores valores de estos indicadores sugieren mejores modelos. Tanto AIC como BIC penalizan modelos con muchos parámetros, aunque difieren en la forma en que penalizan la complejidad del mismo. BIC penaliza órdenes adicionales del modelo más que AIC, por lo que el criterio BIC suele sugerir modelos más simples que AIC. En muchas ocasiones, ambos sugieren el mismo modelo, pero cuando no lo hacen la elección depende de las prioridades: si queremos identificar un mejor modelo predictivo, AIC es una mejor elección, mientras que si lo que buscamos identificar un buen modelo explicativo, entonces podemos basar nuestra decisión en el que produzca el menor valor de BIC.

Vamos a realizar entonces una búsqueda de los parámetros en una grilla, avaluando los mismos para los valores 0, 1 y 2, lo que nos da un total de $3^6 = 729$ modelos. Almacenaremos los valores de los parámetros, junto con los resultados obtenidos para AIC y BIC en una lista, y luego examinaremos las combinaciones de parámetros con menores valores de AIC y BIC. 

Previamente, dividiremos nuestra serie temporal en un conjunto de datos para el ajuste del model (\mip{y_train}) conteniendo el primer 85\% del total de datos, y reservaremos el restante 15\% (\mip{y_valid}) para contrastar las predicciones obtenidas por nuestro mejor modelo con los datos registrados, de modo de tener una medida de cuán buenas son las predicciones:
\jupynotex[10]{Chapters/modelado_estadistico/code/co2.ipynb}

A continuación generamos la búsqueda, iterando sobre todas las tuplas $(p, d, q)$ y $(P, D, Q)$ con valores entre 0 y 2 (dejamos fijo $m = 12$). Los resultados de cada ajuste los guardamos en la lista \mip{model_evals}. 
\jupynotex[11]{Chapters/modelado_estadistico/code/co2.ipynb}

Mostramos ahora los parámetros obtenidos para los modelos con AIC y BIC menores a 300, siendo este valor sugerido por ajustes exploratorios para algunos pocas tuplas:

\jupynotex[12]{Chapters/modelado_estadistico/code/co2.ipynb}

Podemos ver que algunas combinaciones de parámetros generan valores muy bajos de AIC y BIC. Sin embargo, dichos modelos no ajustan bien a la serie temporal (esto se muestra en los \textit{warnings} que desactivamos en la celda 1 para evitar salidas muy largas, que informan sobre problemas de convergencia del método de ajuste o inestabilidades numéricas). Descartando estos valores, el conjunto $(p, d, q) = (1, 1, 1)$ y $(P, D, Q, 12) = (2, 1, 2, 12)$ coinciden en bajos valores tanto de AIC como BIC, por lo que construiremos un modelo SARIMAX basado en estos parámetros:
\jupynotex[13]{Chapters/modelado_estadistico/code/co2.ipynb}

Con el método \mip{summary()} del objeto \mip{model_fit} podemos obtener los parámetros del ajuste y sus correspondientes $p$-valores. En todos los casos, estos $p$-valores muestran que los coeficientes obtenidos son significativos. 

También podemos examinar la calidad del ajuste a través del análisis de los residuos de los datos que utilizamos como ajuste (\mip{y_train}). Estos residuos son la diferencia entre los valores generados por el modelo y los datos reales. El modelo ideal de ajuste debería tener residuos que sean ruido blanco gaussiano centrados en cero y sin correlación. El método \mip{plot_diagnostics} genera cuatro gráficos que permiten evaluar estas características de los residuos:
\jupynotex[14]{Chapters/modelado_estadistico/code/co2.ipynb}

El panel superior izquierdo muestra los residuos para cada valor registrado en los datos de ajuste. Si nuestro modelo funciona adecuadamente, no se debería apreciar ningún patrón obvio en estos residuos, tal como muestra la figura. El panel superior derecho muestra la distribución de dichos residuos, tanto en forma de histograma como su KDE, y muestra también para facilitar la comparación una distribución normal con media cero y varianza 1. Se puede ver que la KDE de los residuos es muy cercana a la distribución normal.

En el panel inferior izquierdo se muestra el diagrama Q-Q de los residuos. Este diagrama compara la distribución teórica de los residuos con la observada. En el caso ideal deberíamos obtener una línea recta como la roja. Esto sucede en casi todos los valores excepto en los extremos. Finalmente, en el panel inferior derecho se muestra un correlograma que representa la ACF de los residuos. El 95\% de las correlaciones para \textit{lags} mayores que cero no deberían ser significativos (es decir, deben estar dentro de la región sombreada). Si hubiese una correlación en los residuos, podríamos interpretar que existe información en los datos que no fue capturada por el modelo. En nuestro caso, no parecen haber correlaciones para \textit{lags} mayores que cero, lo que indica que el ajuste es bueno.

Finalmente, podemos utilizar el modelo ajustado para predecir valores de la serie temporal posteriores a los que utilizamos para el ajuste, y compararlos con los valores que reservamos con este fin, \mip{y_valid}:
\jupynotex[15]{Chapters/modelado_estadistico/code/co2.ipynb}

Podemos ver que el modelo es capaz de predecir con buena precisión los valores posteriores a los utilizados en el ajuste del modelo, pese a que alguna diferencia es notoria. 

Hemos visto un ejemplo de análisis estadístico de una serie temporal no estacionaria, ajustándola con un modelo relativamente simple. Además de statsmodels, existen otras herramientas poderosas para el ajuste de modelos, entre las que podemos destacar Darts\footnote{\url{https://unit8co.github.io/darts/}} y muchas bibliotecas de redes neuronales. El proceso no es simple y hay que analizar con espíritu crítico cada resultado obtenido.


\section{Lecturas recomendadas}
\begin{itemize}
 \item Una definición formal rigurosa de modelo estadístico puede verse en \fullcite{mcCullagh2002}.
 \item Galit Shmueli realiza una detallada discusión sobre la diferencia entre modelos explicativos 
y modelos predictivos en \fullcite{shmueli2010}.
 \item Un libro moderno, con muchos ejemplos y código (en R), y que mantiene una versión 
\textit{online} con licencia Creative Commons, es \fullcite{holmes2019}. Se puede acceder \href{https://web.stanford.edu/class/bios221/book/index.html}{aquí}\footnote{\url{https://web.stanford.edu/class/bios221/book/index.html}.}.
\item Un libro clásico sobre el uso de técnicas estadísticas aplicadas al análisis de datos experimentales: \fullcite{box2005}.
\item El libro de Haslwanter tiene una buena introducción al modelado estadístico utilizando Python: \fullcite{haslwanter2016}.
\item Cameron y Trivedi son autores de un libro de referencia para el modelado de conjuntos de datos discretos: \fullcite{cameron2013}.
\item El libro de T. Mills presenta una guía práctica y muy completa para la modelización y predicción de series temporales: \fullcite{mills2019}.
\item Un libro muy completo (aunque con código en R) es: \fullcite{box2015}.
\end{itemize}
