
% Copyright 2020-2024 Facundo Batista y Manuel Carlevaro
% Licencia CC BY-NC-SA 4.0
% Para más info visitar https://github.com/facundobatista/libro-pyciencia/

\chapter{Aritmética de punto flotante} \label{ch:puntoflotante}
 
\begin{wraptable}{r}{5cm}
\begin{modulesinfo}
\begin{center}
{\small
    \href{https://github.com/facundobatista/libro-pyciencia/tree/master/código/punto_flotante/}{Código disponible}
}
\end{center}
\end{modulesinfo}
\end{wraptable}


Los números de punto flotante se basan en la necesidad de acotar la cantidad de bits necesarios para almacenar el número deseado.

La problemática surge cuando queremos trabajar tanto con números muy grandes como con números muy chicos. Usemos el ejemplo de tener que representar tanto la masa de la Tierra como la del electrón (en gramos, claro). Hacerlo de la siguiente manera representaría un desperdicio de espacio realmente inviable:

\begin{verbatim}
T = 5972000000000000000000000.00000000000000000000000000000000 g
e = 0000000000000000000000000.00000000000000000000000000091093 g
\end{verbatim}

La solución es utilizar una estructura donde tenemos una ``mantisa'' multiplicada por la base (10 en el caso de números decimales, 2 en el caso de binarios) elevada a un determinado ``exponente'':

\begin{equation*}
m \times b^{n}
\end{equation*}

De esta manera ese exponente ``nos mueve la coma'', y una vez especificada la cantidad de dígitos en la mantisa (que llamamos ``precisión'') y cómo guardamos el exponente, nos queda una estructura fija que permite guardar esos números ocupando poca cantidad de bits.

Para nuestro ejemplo, supongamos que nuestra estructura de punto flotante tiene una precisión de 5 y la base es 10, por lo que los dos números indicados arriba se convierten en:

\begin{align*}
\text{5.9720} &\times 10^{\text{24}}\\
\text{9.1093} &\times 10^{\text{-28}}
\end{align*}

En realidad, la estructura (que incluye el punto decimal) y la base se sobreentienden (están definidas y fijas en el punto flotante utilizado), entonces lo único que guardamos para ambos números son los dígitos \texttt{59720} y el exponente \texttt{24} en un caso, y los dígitos \texttt{91093} y el exponente \texttt{-28} en el otro (tener en cuenta que no son números enteros los que guardamos, sino los dígitos en sí). Además también guardamos el signo (que en el ejemplo no vimos porque ambos números son positivos).

Almacenar los números de esta manera nos da varias ventajas, no sólo la de ocupar poco espacio. Es evidente que con esta estructura podemos representar números de órdenes de magnitud muy diferentes (estamos sólo limitados por el exponente, como veremos abajo), y nos proporciona la misma precisión relativa para todos esos números (como mencionamos arriba, limitados por la longitud de la mantisa).

También nos permite algunos cálculos entre esos números muy grandes y muy pequeños (multiplicarlos, por ejemplo), pero tenemos que tener en cuenta que la suma y la resta posiblemente se vean afectadas por la precisión. Siguiendo el ejemplo de arriba, si sumamos la masa de la Tierra y la del electrón, realmente no veremos cambio en el resultado:

\begin{equation*}
\text{5972000000000000000000000.00000000000000000000000000091093 = 5.9720} \times 10^{\text{24}}
\end{equation*}

Esto es lo que se llama ``error de representación'' en los números de punto flotante, algo que no podemos evitar mientras manejemos una precisión finita. Tengamos en cuenta que este error no sólo se presenta en casos poco reales como el del ejemplo (nadie anda por la vida sumando la masa de la Tierra y la de un electrón), sino todo el tiempo, en operaciones mucho más triviales. Por ejemplo, en nuestra estructura ficticia ``un tercio'' sería \texttt{33333\texttimes 10\textsuperscript{-5}}, y claramente para que el número sea exacto necesitaríamos ``infinitos tres'' en esa mantisa. Hablaremos más sobre estos errores luego.

También tenemos errores de cálculo cuando llegamos al límite del exponente. Por ejemplo si en nuestra estructura ficticia determinamos que tenemos (además de los cinco para la mantisa) dos dígitos para el exponente, el número más grande que soportará la estructura es el \texttt{99999\texttimes 10\textsuperscript{99}} y el número más chico el \texttt{00001\texttimes 10\textsuperscript{-99}}. Si el número es mayor al máximo seguramente tengamos un error de \emph{overflow}, y si el número es menor al mínimo lo más probable es que redondee a cero (aunque los sistemas de manejo de punto flotante permiten mostrar que sucedió un \emph{underflow} e incluso se pueden configurar para que eso sea un error, no un redondeo).

Además, en punto flotante, tenemos los valores especiales ``infinito'' (positivo y negativo, resultado por ejemplo de dividir algún número por cero), y el valor especial \verb|NaN|, que significa que no es un número (por el inglés \textit{not a number}) sino el resultado de una operación indefinida (como multiplicar cero por infinito). Tengamos en cuenta que como mencionaba arriba, muchos de estos casos por default generan un error, pero puede configurarse el sistema para manejarlos.

El tema da para mucho más, se nos escapa del alcance del libro. Para profundizar pueden explorar la página de Wikipedia al respecto \cite{wikipedia-floating-point} o la especificación de aritmética para punto flotante decimal \cite{aritmetica-punto-flotante-decimal}, que detalla toda la implementación de un punto flotante de forma genérica sin entrar en la complicación de hacerlo súper optimizado en binario pensado para hardware, como lo hace la norma IEEE 754 \cite{ieee-754}, para punto flotante binario, que veremos a continuación.


\section{Punto flotante binario}\label{sec:pfbin}

Hasta ahora hablamos genéricamente de estructuras de punto flotante, y en los ejemplos con los que jugamos usamos números decimales. 

Aunque la base puede ser cualquiera, en la práctica se manejan dos grupos, los puntos flotantes decimales y los binarios. Obviamente los primeros son con dígitos en base 10 y los segundos son en base 2.

El punto flotante binario es el punto flotante nativo en todos los lenguajes modernos, debido a que su aritmética está incluida en los procesadores de las computadoras desde hace más de medio siglo, y desde la década de 1990 la mayoría de las implementaciones se basan en el estándar IEEE 754. El punto flotante decimal, aunque hubieron algunos procesadores que lo traian nativo, en la mayoría de los lenguajes está implementado en software, y normalmente siguiendo todos el mismo estándar \cite{decimal-spec}. 

En Python al punto flotante binario lo tenemos como tipo de dato integrado en el lenguaje, es el \mip{float}, mientras que al punto flotante decimal lo tenemos implementado en el módulo \mip{decimal}, como ya mencionamos en la subsección de Números \ref{sec:numeros}.

Ojo, que el punto flotante binario que usa Python, aunque se llame \emph{float}, corresponde en precisión al que en otros lenguajes (como C) se denomina \emph{double} (por ``doble precisión'', que usa 8 bytes por número), mientras que allí el \emph{float} usa precisión simple, ocupando 4 bytes por número. Mientras que el \emph{float} integrado en Python está bien que sea de doble precisión (porque eso minimiza todo lo posible los errores), en NumPy podemos indicar explícitamente cual queremos utilizar:

\jupynotex[1-2]{Chapters/punto_flotante/code/punto_flotante.ipynb}

Más allá de esos detalles, la característica más importante que debemos mencionar aquí es que el punto flotante binario no puede representar fracciones decimales de forma exacta, por lo que si usamos punto flotante binario no podemos garantizar los mismos resultados que si usáramos aritmética decimal. 

Muchas veces no vemos esto porque Python redondea el número usando los 17 dígitos más significativos, entonces si escribimos \verb|1/10| vemos \verb|0.1|, aunque realmente ese número no puede representarse en binario, de la misma manera que no podemos representar \verb|1/3| en notación decimal. 

\jupynotex[3]{Chapters/punto_flotante/code/punto_flotante.ipynb}

El tipo de datos Decimal, cuando lo inicializamos con un float, nos muestra el número exacto que ese float representa:

\jupynotex[4]{Chapters/punto_flotante/code/punto_flotante.ipynb}

Incluso nos puede sorprender que punto flotante binario represente de la misma manera dos números distintos en decimal:

\jupynotex[5-6]{Chapters/punto_flotante/code/punto_flotante.ipynb}

Veamos por ejemplo qué sucede al dividir varias veces un número por diez, primero directamente:

\jupynotex[7]{Chapters/punto_flotante/code/punto_flotante.ipynb}

Esto parece tener mucho sentido, pero si usamos Decimal para ver cual es el número exacto que está manejando el procesador, vemos que son sólo aproximaciones (a veces por arriba, a veces por abajo):

\jupynotex[8]{Chapters/punto_flotante/code/punto_flotante.ipynb}

Entonces, realizar más o menos operaciones vamos incorporando (de mayor o menor manera) un error en el resultado final. Podemos acotar este error, sin embargo, ya que según la implementación del punto flotante binario tendremos un límite superior del error relativo debido al redondeo; este valor se llama ``épsilon'' y en Python lo podemos encontrar (junto a otros valores que dependen de la implementación) en la estructura \mip{sys.float_info}:

\jupynotex[9]{Chapters/punto_flotante/code/punto_flotante.ipynb}

En definitiva esto hace que reglas que aprendimos en la escuela como ``sumar un número N veces es lo mismo que multiplicar ese número por N'' no se cumplen en punto flotante binario

\jupynotex[10-11]{Chapters/punto_flotante/code/punto_flotante.ipynb}

Esto es aún más sorpresivo en operaciones más cortas, donde a veces ``parece'' estar todo bien pero en otros casos, para la misma operación, encontramos que no:

\jupynotex[12-13]{Chapters/punto_flotante/code/punto_flotante.ipynb}

Por esto es que siempre recomendamos no comparar por igualdad números de punto flotante, sino que lo ideal es evaluar si la diferencia entre los dos números que queremos comparar está acotada a un error máximo que definamos (técnica llamada ``comparación con épsilon''):

\jupynotex[14-15]{Chapters/punto_flotante/code/punto_flotante.ipynb}

En estos ejemplos el épsilon fue elegido en función de los números con los que operábamos, pero en realidad no podemos usar siempre un valor absoluto, ya que si los números que estamos comparando son muy pequeños, ese épsilon nos quedó demasiado grande. Por eso siempre se usa un épsilon relativo, en función de los números a comparar. Ojo que este épsilon no es el mismo número que mencionábamos arriba, con el mismo nombre, pero que hacía referencia al máximo error relativo en función de la implementación.

Tenemos que tener en cuenta que todo esto está en la naturaleza básica del punto flotante binario: no es un bug en Python, ni en el procesador, ni en nuestro código, y por eso encontramos el mismo comportamiento en todos los lenguajes que soportan la aritmética de punto flotante de los procesadores (aunque los distintos lenguajes pueden tener distintas reglas sobre cuando y cómo representar estas diferencias).

Estos detalles (y otros) hacen extremadamente difícil desarrollar y probar valores comerciales y financieros, y es por eso que se usa punto flotante decimal en esos casos.

¿Pero qué pasa con aplicaciones científicas?

Cuando tenemos que realizar miles o millones de operaciones, no tenemos alternativa a utilizar punto flotante binario, porque es el que está implementado en hardware: cualquier otra opción sería demasiado lenta.

Entonces, tenemos que lidiar con los errores inherentes a estos números. No los podemos evitar, pero sí podemos controlar su magnitud, no sólo para obtener un resultado final lo más preciso posible, sino también para poder conocer y acotar el error que tenemos al final de una serie de cálculos.

El análisis que debemos hacer se suma, obviamente, al que ya realizábamos sobre los errores de los números originales de esos cálculos (que si son valores teóricos son exactos, pero cuando manejamos mediciones reales, esos mismos números ya vienen con un determinado error que necesitamos conocer).

Por lo general los errores de redondeo son muy pequeños cuando se trabaja con doble precisión y operaciones aisladas. Sin embargo hay situaciones donde pueden aparecer problemas con operaciones simples o al acumularse en cálculos repetidos. La multiplicación y la división son operaciones seguras, pero la suma y la resta son peligrosas (porque para realizarlas hay que operar sobre la mantisa luego de igualar los exponentes, entonces volvemos a tener el problema mostrado al inicio del capítulo cuando mezclamos números de magnitudes muy diferentes).

Veamos un ejemplo de esto, mostrando como no es lo mismo el orden cuando sumamos o restamos:

\jupynotex[16-18]{Chapters/punto_flotante/code/punto_flotante.ipynb}

Cuantas más operaciones se realice en un cálculo más atención hay que darle a este problema (especialmente con los métodos iterativos de resolución de algoritmos), por eso es que siempre es recomendable utilizar algoritmos diseñados específicamente para minimizar estos errores (particularmente, en lo posible, siempre que se puede utilicemos las bibliotecas de NumPy y SciPy en vez de escribir los algoritmos nosotros).

Para profundizar en el control de error en operaciones aritméticas usando punto flotante les recomendamos el documento \textit{What Every Computer Scientist Should Know About Floating-Point Arithmetic}, de David Goldberg \cite{compscientist-floatingpoint}.
