
% Copyright 2020-2024 Facundo Batista y Manuel Carlevaro
% Licencia CC BY-NC-SA 4.0
% Para más info visitar https://github.com/facundobatista/libro-pyciencia/

\chapter{Elementos de estadística} \label{ch:estadistica}


\section{Introducción}

\begin{wraptable}{r}{5cm}
\begin{modulesinfo}
\begin{center}
{\small
    \begin{tabular}{l r}
        \toprule
        \textbf{Módulo} & \textbf{Versión} \\
        \midrule
        Matplotlib & 3.9.2 \\
        NumPy & 1.26.4 \\
        SciPy & 1.14.1 \\
        \bottomrule
    \end{tabular}
    \vspace{0.75em}

    \href{https://github.com/facundobatista/libro-pyciencia/tree/master/código/estadistica/}{Código disponible}
}
\end{center}
\end{modulesinfo}
\end{wraptable}

La estadística comprende la colección, organización y análisis de datos para comprender fenómenos y tomar decisiones. Los métodos estadísticos son necesarios cuando tenemos información incompleta sobre un fenómeno, debido a que es imposible obtener datos de todos los miembros de una ``población'', o porque las mediciones que realizamos poseen intrínsecamente una incerteza. Cuando no podemos determinar completamente los valores de una población, podemos obtener una ``muestra'' elegida aleatoriamente (muestreo sistemático, estratificado, de conglomerados, etc.) y por medio de métodos estadísticos obtener parámetros que permitan hacer inferencias sobre las propiedades de la población, en forma sistemática y con estimaciones.

Los métodos estadísticos se fundamentan en la teoría de la probabilidad, con la cual podemos modelar incertezas e información incompleta usando variables aleatorias. Como un ejemplo muy común, a partir de la selección aleatoria de una muestra de una población podemos inferir las propiedades de esta última. En la teoría de la probabilidad, cada salida posible de una observación tiene una probabilidad dada, y la probabilidad de todas las posibles salidas constituye la distribución de probabilidad. Dada una función de distribución de probabilidad, podemos calcular las propiedades de la población tales como la media y la varianza, pero si solo conocemos los valores para una muestra obtenida aleatoriamente  podemos estimar los valores esperados, o medios, de la población.

\section{Números aleatorios}

La bibloteca estándar de Python provee el módulo \mip{random} que implementa generadores de números pseudo aleatorios para diversas distribuciones, utilizando el algoritmo Mersenne-Twister como generador \cite{matsumoto1998}. Por otra parte, el módulo \mip{random} de NumPy brinda una funcionalidad similar con métodos que generan arrays de NumPy, además de ofrecer un número mayor de distribuciones de probabilidad.

Las rutinas para generar números pseudo aleatorios de NumPy utilizan una combinación de un \mip{BitGenerator}, que son objetos que proveen secuencias de bits aleatorios\footnote{Sabiendo que los algoritmos solo son capaces de generar números ``pseudo aleatorios'', utilizaremos indistintamente el término ``aleatorio'' para referirnos a los que estrictamente son números ``pseudo aleatorios''.} compuestos de ``palabras'' de enteros sin signo con secuencias de 32 o 64 bits, y \mip{Generators}, que transforman las secuencias de bits aleatorios de un \mip{BitGenerator} en secuencias de números con una distribución de probabilidad específica (uniforme, normal o binomial, por ejemplo), en un intervalo dado.

Un detalle importante es la semilla utilizada para iniciar la secuencia de números aleatorios. Esta semilla se pasa como argumento al objeto \mip{BitGenerator} en forma de entero o array de enteros. En caso de no pasar ninguna semilla al inicializar el \mip{BitGenerator}, se tomará un valor del generador de entropía del sistema operativo\footnote{En los sistemas operativos del tipo Unix, estas semillas se originan de los archivos especiales \mip{/dev/random}, \mip{/dev/urandom} y \mip{/dev/arandom}, que colectan ruido ambiental de los dispositivos de la computadora. Se puede ver la implementación en Linux \href{https://git.kernel.org/pub/scm/linux/kernel/git/stable/linux.git/tree/drivers/char/random.c?id=refs/tags/v3.15.6\#n52}{aquí}.}. En las aplicaciones científicas o tecnológicas es importante emular un proceso aleatorio, pero también lo es que estas secuencias sean reproducibles, por lo que el control y registro de las semillas conforman una buena práctica.

Veamos entonces cómo se utilizan estas herramientas. Como muestra el \textit{jupyter-notebook} a continuación, en la celda 1 importamos \mip{Generator} y \mip{PCG64} desde \mip{numpy.random}, además de NumPy para la utilización de arrays. \mip{PCG64} es el objeto \mip{BitGenerator} que inicializa el generador pseudo aleatorio PCG-64, que a su vez implementa el generador congruencial de permutación de O'Neill \cite{oneill2014}. Luego establecemos primero la semilla del generador utilizando el número de Hardy-Ramanujan\footnote{1729 es el número natural más pequeño que puede expresarse como suma de dos cubos positivos, de dos formas diferentes: $1729 = 1^3 + 12^3 = 9^3 + 10^3$. También es conocido como el ``número Taxi'', por la anécdota que le da nombre.}, de modo de hacer reproducible los resultados, e instanciamos el objeto \mip{rng} de \mip{Generator}. Luego obtenemos el primer número de la secuencia con el método \mip{random} de \mip{rng}, que devuelve un real con distribución uniforme en $[0, 1)$.

\jupynotex[1]{Chapters/estadistica/code/estadistica.ipynb}

Si queremos obtener un número aleatorio con distribución uniforme en $[a, b)$ es necesario reescalar la salida de \mip{random} multiplicándola por $(b - a)$ y sumándole $a$. Por ejemplo, si el intervalo es $[-3,  0)$:

\jupynotex[2]{Chapters/estadistica/code/estadistica.ipynb}

Tal como anticipamos, podemos obtener no solo números aleatorios aislados sino arrays con las dimensiones deseadas:

\jupynotex[3]{Chapters/estadistica/code/estadistica.ipynb}


El objeto \mip{Generator} también ofrece la posibilidad de generar números enteros en un intevalo dado $[a, b)$ (por defecto semiabierto) mediante el método \mip{integers()}. En el ejemplo siguiente generamos un array de cinco elementos en el intervalo  cerrado $[10, 20]$, estableciendo que el extremo superior del intevalo puede ser generado con la opción \mip{endpoint=True}.

\jupynotex[4]{Chapters/estadistica/code/estadistica.ipynb}

Otra funcionalidad muy útil que brinda \mip{Generator} es la generación de una muestra aleatoria de un determinado array, usando para esto el método \mip{choice()}. En el ejemplo a continuación simulamos una serie de 5 lanzamientos de tres dados. Para ello, en la celda 5 asignamos en la variable \mip{dado} los posibles resultados del lanzamiento de un dado. En la celda 7 invocamos el método \mip{choice} sobre el objeto \mip{rng}, generando los 5 lanzamientos con tres dados, asumiendo que el dado es ``honrado'' y que cada cara tiene probabilidad $1/6$:

\jupynotex[5-6]{Chapters/estadistica/code/estadistica.ipynb}

El argumento \mip{replace} permite establecer si el muestreo es con o sin reemplazo. La opción por defecto es \mip{True}, lo que significa que los elementos del array \mip{dado} pueden ser elegidos en múltiples oportunidades.

Por supuesto, podemos asignar probabilidades diferentes para cada elemento del array del cual queremos tomar la muestra. Para ello es necesario especificar mediante otro array la distribución deseada, cumpliendo la condición obvia que la suma debe ser $1$. Este array de probabilidades se pasa como argumento \mip{p} del método \mip{choice}. En las celdas 7--8 siguientes ``cargamos'' el dado en la cara 4 asignándole mayor probabilidad que al resto de las caras, y repetimos el experimento de los cinco lanzamientos de los tres dados:

\jupynotex[7-8]{Chapters/estadistica/code/estadistica.ipynb}

Podemos comparar la frecuencia con la que son seleccionados los diferentes elementos del array \mip{dado} para ambos casos, generando un número relativamente grande de selecciones y graficando el histograma correspondiente, tal como vemos en la celda siguiente:

\jupynotex[9]{Chapters/estadistica/code/estadistica.ipynb}

Por último, \mip{Generator} provee métodos para reordenar o permutar elementos de una secuencia: \mip{shuffle} y \mip{permutation}. La principal diferencia entre estos métodos consiste en que \mip{shuffle} reordena los elementos de la secuencia modificando ésta (\textit{in place}), por lo que la secuencia debe ser un objeto ``mutable'', mientras que \mip{permutation} devuelve una copia si como argumento se le pasa un array, o un rango reordenado si como argumento se le pasa un entero.  Podemos ver cómo se reordenan arrays, listas o rangos en las siguientes celdas:

\jupynotex[10-12]{Chapters/estadistica/code/estadistica.ipynb}

En el caso que necesitemos reordenar arrays multidimensionales, podemos establecer sobre qué dimensión hacer el reordenamiento indicándolo en el argumento \mip{axis}. Por ejemplo, si queremos ordenar ``por columna'':

\jupynotex[13-14]{Chapters/estadistica/code/estadistica.ipynb}


mientras que si queremos hacerlo ``por fila'':

\jupynotex[15]{Chapters/estadistica/code/estadistica.ipynb}




\section{Variables aleatorias y distribuciones}


En la teoría de la probabilidad, el conjunto de todos los resultados de un experimento aleatorio se denomina ``espacio muestral'', y cada uno de los resultados se llama ``punto muestral''. Por lo general habrá más de un espacio muestral para describir los resultados de un experimento, pero usualmente hay solo uno que suministra la mayoría de la información (por ejemplo, para el experimento de lanzar un dado podríamos tener como espacio muestral el número sobre la cara superior, o si esa cara representa un número par o impar).

Si a cada punto muestral le asignamos un número, podemos definir una ``función aleatoria'' (o ``estocástica'') que mapea desde el espacio muestral al conjunto de números reales o enteros, que de este modo constituye una ``variable aleatoria'' o ``estocástica''. Por ejemplo, nuestro experimento podría ser lanzar una moneda dos veces de tal forma que nuestro espacio muestral es $E = \{CC, CS, SC, SS\}$ donde $C$ representa cara y $S$, sello. Con cada punto muestral podemos asociar un número $X$ que sea el número de caras. Nuestra variable aleatoria queda definida entonces por la siguiente tabla:

\begin{center}
 \begin{tabular} {ccccc}
 \toprule
  \textbf{Punto muestral} & \textbf{CC} & \textbf{CS} & \textbf{SC} & \textbf{SS} \\
  $X$ & $2$ & $1$ & $1$ & 0 \\
  \bottomrule
 \end{tabular}
\end{center}

Una variable aleatoria que toma un número finito o infinito contable de valores se denomina ``variable aleatoria discreta'', mientras que una que toma un número infinito no contable de valores se llama ``variable aleatoria continua''. Un problema estadístico usual consiste en mapear el resultado de experimentos en valores numéricos y determinar la distribución de probabilidad de esos valores. En consecuencia, una variable aleatoria se caracteriza por los posibles valores que puede tomar (el ``recorrido'' de la variable aleatoria) y por su función de distribución de probabilidad, y en la práctica esto significa trabajar con estas distribuciones. Según la notación tradicional, en el caso  discreto se denomina ``función de distribución de probabilidades'' mientras que para el caso continuo la denominación es ``función de densidad de probabilidad''. En lo que sigue usaremos indistintamente la notación FDP según el caso sea discreto o continuo.

Los objetos \mip{Generator} de NumPy permiten generar secuencias de números aleatorios a partir de numerosas distribuciones\footnote{Se puede ver la lista completa de distribuciones disponibles en la \href{https://numpy.org/doc/stable/reference/random/generator.html\#distributions}{documentación} de NumPy.}. Como ejemplo, en la celda siguiente construimos un objeto \mip{Generator} invocando (a diferencia del ejemplo previo) el constructor por defecto \mip{default_random()} que lo inicializa con el \mip{BitGenerator} \mip{PCG64}, y al que le pasamos la semilla del generador (lo cual es opcional\footnote{En este ejemplo utilizamos el \href{https://es.wikipedia.org/wiki/Número\_feliz}{primer primo feliz}.}). Luego utilizamos ese objeto para generar arrays con un número \mip{n_samples} de elementos con distintas distribuciones, que se muestran como histogramas en la figura.

\jupynotex[16]{Chapters/estadistica/code/estadistica.ipynb}

Como vimos, un objeto \mip{Generator} de NumPy es capaz de producir una secuencia de números aleatorios con una distribución dada. Esto es, de algún modo, equivalente a realizar experimentos estocásticos con esa distribución y obtener salidas de las variables aleatorias que resultan de ese experimento. Una descripción más completa a nivel estadístico se obtiene a partir del submódulo \mip{stats} de SciPy.

El módulo \mip{scipy.stats} provee clases para representar variables aleatorias con una extensa lista de distribuciones de probabilidad. Tiene dos clases base para variables aleatorias continuas y discretas: \mip{rv.continuous} y \mip{rv.discrete}. Estas clases no son utilizadas directamente sino que se usan como clases base para variables estocásticas con distribuciones específicas, y definen una interfaz común para todas las variables aleatorias en este módulo. A continuación haremos una revisión breve de los principales conceptos que se utilizan para caracterizar una distribución, y la forma de obtener descriptores utilizando convenientemente métodos de NumPy o SciPy.

\subsection{Medidas de centralidad}

Cuando tenemos un conjunto de datos como muestra de una distribución, podemos caracterizar el centro de la distribución con diferentes parámetros: según su valor o por su rango en función del orden en una lista de acuerdo a una magnitud.

\subsubsection{Media}

Por la media de una distribución nos estamos refiriendo a la media aritmética $\bar{x}$:
\begin{equation*}
    \bar{x} = \frac{1}{n} \sum_{i=1}^n x_i
\end{equation*}

Tomemos como ejemplos para analizar las distribuciones obtenidas en la celda 16. Con el método \mip{mean()} de NumPy podemos obtener las medias:

\jupynotex[17]{Chapters/estadistica/code/estadistica.ipynb}

Un problema común que acontece es que el conjunto de datos con el que trabajamos tiene valores perdidos (por errores de medición, registro, etc.) que se representan por \mip{NaN} (que significa ``\textit{not a number}'', o un no-número). En estos casos, NumPy posee métodos que pueden obtener resultados ignorando estos NaN. Por ejemplo, en la celda 18 agregamos un NaN (que numpy representa con \mip{numpy.nan}) a nuestro array con distribución normal. Si calculamos la media de esta nueva distribución, obtenemos un NaN como resultado, pero con el método \mip{nanmean()} recuperamos el resultado correcto:

\jupynotex[18-19]{Chapters/estadistica/code/estadistica.ipynb}

\subsubsection{Mediana}

La mediana es el valor que separa la mitad superior de la mitad inferior de una lista ordenada de valores. En caso en que dicha lista tenga un número par de valores, la mediana es la media aritmética de los dos valores centrales.

En la celda siguiente podemos ver el resultado de obtener la mediana para dos arrays con números par e impar de elementos:

\jupynotex[20]{Chapters/estadistica/code/estadistica.ipynb}

Se puede observar que cuando las distribuciones de valores son simétricas, la media y la mediana coinciden, pero no así cuando las distribuciones no lo son:

\jupynotex[21]{Chapters/estadistica/code/estadistica.ipynb}

En el caso en que tengamos una función de densidad de probabilidad (variable continua), la mediana es el valor tal que si integramos dicha función desde $-\infty$ hasta la mediana, la integral da $0.5$.


\subsubsection{Moda}
La moda de un conjunto de datos es el valor que aparece con mayor frecuencia (si la distribución es continua, es el valor máximo). Por ejemplo, si hacemos cinco experimentos lanzando cuatro dados cargados según la FDP que mostramos en la celda 7, podemos obtener la moda en cada eje del array resultante, o de todas las salidas, utilizando el método \mip{mode} de \mip{scipy.stats} como se muestra en la celda 23:

\jupynotex[22]{Chapters/estadistica/code/estadistica.ipynb}

En la primera salida del cálculo de la moda obtenemos un array que muestra cuál es el valor más frecuente en cada columna (\mip{axis=0} por defecto) y un array con el número de veces que aparece
el valor correspondiente. Si queremos obtener la moda de todos los valores que contiene el array \mip{samples}, pasamos la opción \mip{axis=None} como argumento.

En caso que el conjunto de datos a analizar incluya uno o varios NaN, es posible establecer la respuesta de \mip{mode} mediante el parámetro opcional \mip{nan_policy}. Si este parámetro es \mip{'propagate'} el método \mip{mode} devuelve NaN (y ésta es la opción por defecto). Si le pasamos \mip{'omit'} obtenemos la moda de la distribución omitiendo los NaN y si el valor de este parámetro es \mip{'raise'} se lanza una excepción. Cuando existen varias modas en el conjunto, \mip{mode()} devuelve la menor.

\subsubsection{Media geométrica}

En algunos casos es útil la media geométrica como medida de centralidad de una distribución. Esta magnitud se puede calcular mediante la media aritmética del logaritmo de los valores:
\[ \bar{x}_g = \left( \prod_{i=1}^n x_i \right)^{1/n} = \exp \left( \frac{\sum_i \ln(x_i)}{n} \right) \]

Esta función se puede evaluar con el método \mip{gmean} de \mip{scipy.stats}. Obviamente, los valores deben ser positivos para que se pueda calcular la media geométrica. En la celda 23 podemos ver las distintas medidas de centralidad sobre la distribución F.

\jupynotex[23]{Chapters/estadistica/code/estadistica.ipynb}

En la figura \ref{fig:estad01} vemos una comparación gráfica de la moda, la mediana y la media para la distribución $F$ cuya asimetría permite distinguir las tres magnitudes. Cabe enfatizar aquí que estas medidas de centralidad corresponden a muestras específicas, si cambiamos la semilla que usamos para el generador, obtendremos otros valores dado que estaríamos analizando una salida de esas mismas variables aleatorias.

\begin{figure}[t]
 \centering
 \includegraphics[scale=0.75]{Chapters/estadistica/figs/f_centralidad.pdf}
 \caption{Moda, mediana y media para la distribución $F$.}
 \label{fig:estad01}
\end{figure}


\subsection{Medidas de dispersión}

Además de las medidas de centralidad que nos informan acerca de los valores centrales de una distribución de datos, es necesario también conocer alguna cantidad que represente la dispersión de los valores alrededor de las medidas centrales. Veremos a continuación algunas medidas frecuentes.

\subsubsection{Rango}

El rango de una distribución de datos es la diferencia entre los valores máximo y mínimo de dicha distribución, y se obtiene con el método \mip{ptp} de NumPy (que proviene del inglés \textit{peak-to-peak}):

\jupynotex[24]{Chapters/estadistica/code/estadistica.ipynb}

Esta medida de dispersión incluye los valores atípicos, u \textit{outliers}, que difieren notoriamente del resto de los valores de nuestro conjunto de datos, y pueden ser originados por errores en la medición o registro. Existen criterios para detectar estos \textit{outliers}, tal como el considerar que un valor es atípico cuando se encuentra a $1.5$ $IQR$ de distancia de los cuartiles $Q_1$ y $Q_3$, siendo $IQR = Q_3 - Q_1$ el rango intercuartílico (que veremos a continuación).

\subsubsection{Cuantiles}\label{sec:est-cuantiles}

Los cuantiles son valores específicos de la variable aleatoria que dividen el rango de la distribución de probabilidad en intervalos continuos de igual probabilidad. Los cuantiles más usuales tienen nombre propio: cuartiles (cuatro grupos), deciles (diez grupos) y percentiles (100 grupos). Un caso particular es la mediana, que divide la distribución en dos regiones de igual probabilidad.

Para analizar cuantitativamente los cuantiles es útil definir la ``función de distribución acumulada'' $\cdf$ (por su sigla en inglés) como función de la integral de la FDP:
\begin{equation}
 \cdf(x) = \int_{-\infty}^x \fdp(x') \, dx'
\end{equation}

Conocer la $\cdf$ facilita el cálculo de la probabilidad de encontrar un valor de $x$ en el intervalo $(a, b)$, dado que
\[ P(a \leq X \leq b) = \int_a^b \fdp(x) \, dx = \cdf(b) - \cdf(a) \]

Naturalmente, para distribuciones discretas, las integrales deben reemplazarse por sumas. La figura \ref{fig:estad02} muestra la representación gráfica de la $\pdf(x)$ y $\cdf(x)$ para una distribución normal, destacando en ambos paneles el valor de $\cdf(x = 1)$.

Dado que la $\cdf$ es una función monótona creciente, su inversa $\cdf^{-1}$ existe, y a partir de ésta se determinan los cuantiles. Por ejemplo, la ya mencionada mediana es simplemente $\cdf^{-1}(0.5)$. Del mismo modo se pueden encontrar rangos que incluyan un porcentaje dado de los datos. Si queremos obtener el rango $(x_{\text{inf}}, x_{\text{sup}})$ que incluye el 95 \% de la distribución, tenemos:
\[ x_{\text{inf}} = \cdf^{-1}(0.025), \quad x_{\text{sup}} = \cdf^{-1}(0.975) \]

Como mencionamos, otros cuantiles muy usados son los cuartiles $Q_1$, $Q_2$ y $Q_3$, que corresponden a los percentiles 25 ($\cdf^{-1}(0.25)$), 50 ($\cdf^{-1}(0.5)$) y 75 ($\cdf^{-1}(0.75)$), respectivamente. Estos cuartiles se utilizan para generar los gráficos de caja (\textit{boxplots}), y la diferencia $IQR = Q_3 - Q_1$ se denomina ``rango intercuartílico'', que se utiliza en uno de los métodos para identificar \textit{outliers}.


\begin{figure}[t]
 \centering
 \includegraphics[scale=0.75]{Chapters/estadistica/figs/cdf_pdf.pdf}
 \caption{Función de densidad de probabilidad (FDP, panel izquierdo) y función de distribución acumulada (CDF, panel derecho), para una distribución normal con media $\mu = 0$ y desviación estándar $\sigma = 1$.}
 \label{fig:estad02}
\end{figure}

El submódulo \mip{stats} de SciPy tiene la función \mip{iqr()} que calcula, por defecto, el rango intercuartílico de una distribución. Sin embargo, podemos modificar este comportamiento estableciendo el rango correspondiente a otros percentiles. Por ejemplo, para calcular el rango de una distribución podemos pasar como argumento \mip{rng=(0, 100)} que equivale a utilizar la función \mip{ptp}. En el ejemplo mostrado en la celda 25 vemos cómo podemos calcular la mediana como el percentil 50, y el rango intercuartílico como diferencia de los percentiles 75 y 25.

\jupynotex[25]{Chapters/estadistica/code/estadistica.ipynb}

El uso de cuartiles premite representar distribuciones mediante los populares gráficos de caja o \textit{boxplots}. En la celda 26 generamos nuevas distribuciones (normal, lognormal y Gumbel) y representamos los valores obtenidos mediante cajas que abarcan el 50 \% de los datos, desde el primer cuartil $Q_1$ hasta el tercer $Q_3$ (es decir, el rango intercuartílico $IQR$). La linea horizontal dentro de la caja representa la mediana o  segundo cuartil $Q_2$. Los ``bigotes'' (lineas negras en ambos lados la caja) se extienden hasta $Q_1 - 1.5 \, IQR$ y $Q_3 + 1.5 \, IQR$, y establecen el rango fuera del cual los valores de la distribución se consideran \textit{outliers}, que en la figura se muestran con cruces. Estos bigotes pueden representar valores alternativos como: los valores mímino y máximo, una desviación estándar alrededor de la media de los datos, los percentiles 9 y 91 o los percentiles 2 y 98\footnote{Para más información sobre cómo construir estos gráficos, invitamos a leer la \href{https://matplotlib.org/stable/api/\_as\_gen/matplotlib.pyplot.boxplot.html}{documentación de Matplotlib}.}.


\jupynotex[26]{Chapters/estadistica/code/estadistica.ipynb}



\subsubsection{Varianza y desviación estándar}

El cálculo de la varianza de una población con $n$ valores discretos se define como
\begin{equation}\label{eq:estvarpob}
 \sigma^2 = \frac{1}{n} \sum_{i=1}^n (x_i - \bar{x})^2
\end{equation}
y la correspondiente desviación estándar es simplemente $\sigma$. En caso en que los valores disponibles no correspondan a toda la población, sino solamente a una \textit{muestra} de la misma, se sabe que la ecuación \eqref{eq:estvarpob} es un estimador sesgado\footnote{Un estimador es una función que se calcula a partir de los valores muestrales, utilizado para estimar un parámetro desconocido de la población. Un estimador es sesgado cuando la diferencia entre su esperanza matemática y el valor numérico es diferente de cero.} que subestima la varianza de la población. Se puede demostrar que un estimador no sesgado está dado por
\begin{equation}\label{eq:estvarsam}
 s^2 = \frac{1}{n-1} \sum_{i=1}^n (x_i - \bar{x})^2
\end{equation}
que se conoce como la varianza muestral, y la correspondiente raíz cuadrada como la desviación estándar muestral. La notación usual consiste en representar con letras del alfabeto griego los parámetros poblacionales y con letras latinas los correspondientes muestrales. La diferencia con la varianza poblacional es que dividimos por $n-1$ en vez de $n$. La razón de esto es que se ha eliminado un grado de libertad de la muestra al calcular el valor medio $\bar{x}$, por lo que quedan disponibles $n-1$ grados de libertad. Esta diferencia en la forma de calcular varianzas y desviaciones estándar se refleja en la forma en que invocamos funciones en NumPy. En la celdas 27 y 28 vemos cómo utilizar las funciones \mip{var} y \mip{std} de Numpy para calcular estas magnitudes en ambos casos.

\jupynotex[27-28]{Chapters/estadistica/code/estadistica.ipynb}

Al igual que en otras funciones, NumPy ofrece también versiones de estas funciones que ignoran la presencia de NaN en los datos (\mip{nanvar()} y \mip{nanstd()}).


\subsubsection{Error estándar}

El error estándar (EE) es la estimación de la desviación estándar de un parámetro que se calcula a partir de una dada muestra. Por ejemplo, si estimamos la media de una distribución a partir de las medias de diversas muestras obtenidas de esa distribución, la desviación estándar de esas medias muestrales es el error estándar de la muestra (EEM).

En las celda 29 construimos una distribución normal con media $5$ y desviación estándar $1$, y a continuación tomamos \mip{n_muestras = 100} muestras de esa distribución, con \mip{n_valores = 60} valores cada una, y guardamos en el array \mip{medias} los valores medios de cada muestra. El valor medio de este array es una buena estimación de la media de la distribución. La dispersión de estos valores se representa en estas figuras como la línea anaranjada que están a una desviación estándar de la media. Para valores normalmente distribuidos, el error estándar de la media (EEM) se define como
\begin{equation}
 \text{EEM} = \frac{s}{\sqrt{n}} = \frac{1}{\sqrt{n}} \sqrt{\frac{\sum_{i=1}^n (x - \bar{x})^2}{n-1}}
\end{equation}
que se representa con las líneas de trazos verdes en la figura.

\jupynotex[29]{Chapters/estadistica/code/estadistica.ipynb}

El submódulo \mip{stats} de SciPy tiene una función especializada para el cálculo del error estándar de la media (\mip{sem}), que permite operar sobre los diferentes ejes del array de entrada y configurar la manipulación de NaN:

\jupynotex[30]{Chapters/estadistica/code/estadistica.ipynb}

\subsubsection{Intervalos de confianza}

Cuando se realiza un análisis estadístico de datos es usual informar el intervalo de confianza de la estimación de un parámetro. El intervalo de confianza (IC) del $\alpha$\% representa el rango que contiene el valor verdadero de ese parámetro con una probabilidad del $\alpha$ \%.

Si la distribución muestral es simétrica y unimodal, es posible aproximar el intervalo de confianza de la media por:

\[ \text{IC} = \bar{x} \pm Z_{\alpha} \frac{s}{\sqrt{n}}  \]

Para determinar el valor de $Z_{\alpha}$ es necesario conocer la distribución teórica del parámetro que se intenta determinar. Sin embargo, para tamaños de muestras $n$ grandes la media muestral está distribuida normalmente, por lo cual la determinación de $Z_{\alpha}$ consiste en encontrar el cuantil $(1-\alpha)/2$ de la distribución normal. Por ejemplo, para el intervalo de confianza del 95\%:

\jupynotex[31]{Chapters/estadistica/code/estadistica.ipynb}

En caso que la muestra sea pequeña ($n < 30$) suele usarse como distribución teórica la distribución $t$ de Student\footnote{Se pueden ver más detalles de la distribución $t$ de Student en la \href{https://es.wikipedia.org/wiki/Distribución_t_de_Student}{entrada} de Wikipedia.}.

\jupynotex[32]{Chapters/estadistica/code/estadistica.ipynb}


\section{Test de hipótesis}

Es frecuente la necesidad de tomar decisiones sobre poblaciones a partir de la información muestral disponible de las mismas. En estos casos conviene hacer determinados supuestos o hipótesis acerca de las poblaciones que se estudian. Incluso en algunas ocasiones se formulan hipótesis con el solo propósito de rechazarlas. Por ejemplo, si se quiere demostrar que una moneda está ``cargada'', se formula la hipótesis de que es ``honesta'', lo que significa que la probabilidad de que un lanzamiento resulte cara o sello es $0.5$, y se hace un ensayo para validar (o rechazar) estadísticamente esta hipótesis.

El proceso de ensayo o \textit{test} de hipótesis involucra entonces la evaluación de dos afirmaciones mutuamente excluyentes utilizando para ello los datos de una muestra. La idea principal es que siempre existe una situación que puede ser considerada por defecto, de no cambio (respecto de la opinión actual), de no diferencia, no mejora, \textit{statu quo}, etc., que se denomina ``hipótesis nula'' o $H_0$. Por otro lado, la hipótesis alternativa $H_1$ asume que hay algún cambio respecto de la situación actual. El propósito del test de hipótesis es determinar si los datos muestrales respaldan la hipótesis nula o la alternativa.

En virtud de la naturaleza estadística del método de ensayo, las decisiones que se toman con un número limitado de datos (la muestra) pueden ser erróneas. Si se rechaza la hipótesis nula cuando debería ser aceptada se produce un ``error tipo I'', mientras que si se no se rechaza $H_0$ siendo esta falsa en la población, se está cometiendo un ``error tipo II''. Los ensayos de hipótesis deben diseñarse entonces de forma que minimicen los errores de decisión, pero esto no es tan simple ya que en general, un intento de disminuir un tipo de error produce un incremento en el otro. En la práctica un tipo de error suele tener más importancia que el otro (generalmente el error tipo I), por lo que se suele poner una limitación al error de mayor importancia. La única forma de reducir efectivamente ambos errores es aumentar el tamaño de la muestra, pero esto no siempre es posible.

La máxima probabilidad con la que estaríamos dispuestos a arriesgarnos a un error tipo I se denomina ``nivel de significancia del test''. Esta probabilidad se suele establecer antes de generar la muestra, de modo que los resultados obtenidos no tengan influencia en esta decisión. Por lo general se utiliza un nivel de significancia de $0.05$ o $0.01$. Si, por ejemplo, decidimos usar un nivel de $0.05$ o $5\%$, habrá 5 oportunidades en 100 de que rechacemos la hipótesis cuando debería ser aceptada. Es decir, siempre que la hipótesis nula sea verdadera, tenemos un 95 \% de confianza en que tomaremos la decisión correcta.

\begin{figure}[t]
 \centering
 \includegraphics[width=1.0\textwidth]{Chapters/estadistica/figs/una-dos-colas.pdf}
 \caption{\textit{Test} de hipótesis para una distribución normal ($\mu = 0$, $\sigma = 1$). a) Ensayo de dos colas. b) Ensayo de una cola. Las sumas de las áreas en rojo correspondientes a valores en las regiones críticas representan el nivel de significancia del \textit{test}.}
 \label{fig:estad01b}
\end{figure}

Otro aspecto a tener en cuenta se refiere al tipo de comparación a realizar. Por ejemplo, podríamos establecer como hipótesis nula que el tiempo promedio que tarda una persona en llegar a su trabajo es de 30 minutos, o que este tiempo es superior a 30 minutos. En estos casos, los valores críticos que definen el rechazo de $H_0$ se toman como los valores extremos de una distribución de dos colas (con el correspondiente nivel de significancia), o como el valor extremo de una sola cola (que en este caso es la superior). En el primer caso, si el nivel de significancia es del 5\%, los valores críticos son los que limitan la distribución en $2.5$\% de su área a cada lado, mientras que en el segundo el $5$\% corresponde solo al límite superior de la distribución, tal como se ejemplifica en la figura \ref{fig:estad01b}.

Una vez que se definieron $H_0$ y $H_1$, se deben colectar los datos de la muestra (por ejemplo a través de mediciones, encuestas, etc.). El paso siguiente consiste en encontrar un estadístico que se pueda calcular a partir de la muestra y cuya distribución de probabilidad sea conocida asumiendo la hipótesis nula. Luego calculamos, a partir de los datos, la probabilidad de obtener el valor observado del estadístico utilizando la función de distribución que implica asumir la hipótesis nula. Esta probabilidad se denomina $p$-valor. Si el $p$-valor es menor que el umbral predeterminado establecido por el nivel de significancia, podemos concluir que los datos observados probablemnnnnnno son descriptos por la distribución correspondiente a $H_0$, y en este caso podemos rechazar la hipótesis nula en favor de $H_1$. El procedimiento para realizar el \textit{test} de hipótesis consiste entonces en los siguientes pasos:


\begin{enumerate}
 \item Formular la hipótesis nula y la alternativa.
 \item Seleccionar un estadístico tal que la distribución de la muestra bajo la hipótesis nula sea conocida (en forma exacta o aproximada).
 \item Obtener los datos de la muestra.
 \item Calcular el estadístico a partir de los datos y obtener su $p$-valor bajo la hipótesis nula.
 \item Si el $p$-valor es menor que el nivel de significancia $\alpha$ predeterminado, rechazamos la hipótesis nula. Si es mayor, $H_0$ no puede ser rechazada.
\end{enumerate}

En el procedimiento anterior, la mayor dificultad la plantea la determinación de la distribución de probabilidad del estadístico. Sin embargo, muchos ensayos de hipótesis pertenecen a unas pocas categorías usuales para las cuales estas distribuciones son conocidas. En la Tabla \ref{tbl:estad01} se muestran algunos casos comunes para $H_0$, las distribuciones asociadas y las funciones de SciPy que calculan el estadístico y el $p$-valor.

\begin{table}[th] \small
\centering
 \caption{Diversos ensayos de hipótesis y sus distribuciones asociadas.}
 \label{tbl:estad01}
%  \begin{tabular}{p{4.4cm} p{4.2cm} p{4cm}}
 \begin{tabular}{p{0.45\textwidth} p{0.25\textwidth} p{0.2\textwidth}}
  \toprule
  \textbf{Hipótesis nula} & \textbf{Distribuciones} & \textbf{Funciones de SciPy} \\
  \midrule
  Evaluar si la media de una población tiene un valor dado & Normal o $t$ de Student & \mip{stats.ttest_1samp} \\
  Evaluar si la media de dos variables aleatorias son iguales & $t$ de Student & \mip{stats.ttest_ind}, \mip{stats.ttest_rel} \\
  Evaluar la calidad de ajuste de una distribución continua a datos & Kolmogorov-Smirnov & \mip{stats.ktest} \\
  Evaluar la frecuencia de ocurrencia de datos categóricos & $\chi^2$ & \mip{stats.chisquare} \\
  Evaluar la igualdad de varianza en muestras de dos o mas variables & F & \mip{stats.barlett}, \mip{stats.levene} \\
  Evaluar la no correlación entre dos variables & Beta & \mip{stats.peraronr}, \mip{stats.spearmanr} \\
  Evaluar si dos o mas variables tienen la misma media poblacional (análisis de varianza - ANOVA) & $F$ & \mip{stats.f_oneway}, \mip{stats.kuskal} \\
  \bottomrule
\end{tabular}
\end{table}

Veamos un ejemplo. Uno de los casos más frecuentes es el de afirmar que la media $\mu$ de una población tiene el valor $\mu_0$. Obtenemos entonces una muestra de esa población, y utilizamos la media muestral $\bar{x}$ para construir el estadístico
\[ Z = \frac{\bar{x} - \mu_0}{\sigma / \sqrt{n}} \]
donde como es usual, $n$ es el tamaño de la muestra. Si la población es grande y se conoce su varianza $\sigma$, es razonable asumir que el estadístico $Z$ está distribuido normalmente. Si la varianza es desconocida podemos reemplazarla por la varianza muestral $s$. Entonces el estadístico sigue una distribución $t$ de Student, que en el límite de grandes números se aproxima a una distribución normal.

En la celda 33 generamos una distribución normal \mip{X} con media \mip{mu} y desviación estándar \mip{sigma}, y luego tomamos una muestra de \mip{n} valores de esta distribución que almacenamos en \mip{X_muestra}. Entonces, nuestras hipótesis son:
\begin{itemize}
 \item $H_0$: $\mu = \mu_0$
 \item $H_1$: $\mu \neq \mu_0$
\end{itemize}


\jupynotex[33]{Chapters/estadistica/code/estadistica.ipynb}

A continuación calculamos los estadísticos \mip{Z} (asumiendo una distribución normal con $\sigma$ conocida) y \mip{t} (con desviación estándar muestral $s$). Si definimos que nuestro nivel de significancia es del $5$\% ($\alpha = 0.05$), calculamos el valor crítico con una distribución normal de dos colas, utilizando para ello la función \mip{norm.ppf()} que devuelve el valor de $Z_{\alpha/2}$, como se muestra en la figura \ref{fig:estad01b} (panel a).

\jupynotex[34-36]{Chapters/estadistica/code/estadistica.ipynb}

Podemos ver que los valores de \mip{Z} y \mip{t} caen fuera del intervalo $(-1.96, 1.96)$ que corresponde a la zona de aceptación de $H_0$, lo cual nos permite rechazar la hipótesis nula con nivel de significa del $5$\%. Podemos calcular también los $p$-valores utilizando para ello tanto la distribución normal como la $t$ de Student, para calcular el área de las dos colas (por eso el factor 2) limitadas por los estadísticos correspondientes. Vemos que ambos valores están por debajo del nivel de significancia preestablecido, lo que confirma la decisión anterior de rechazar $H_0$.

\jupynotex[37]{Chapters/estadistica/code/estadistica.ipynb}

Hemos hecho estos cálculos ``manualmente'' para comprender los detalles de cómo tomar la decisión de aceptar o rechazar la hipótesis nula, pero podemos ahorrarnos este trabajo utilizando directamente las funciones provistas por SciPy para ello. En la celda 38 utilizamos la función \mip{ttest_1samp} de \mip{stats} para comparar la media de la muestra con \mip{mu_0} tal como afirma $H_0$. Esta función nos devuelve el valor del estadístico y el $p$-valor:

\jupynotex[38]{Chapters/estadistica/code/estadistica.ipynb}

Tal como vimos previamente, el $p$-valor devuelto nos permite rechazar la hipótesis nula. En la celda siguiente comparamos la distribución de datos de la muestra (histograma), con la distribución real (linea verde) y una distribución aproximada a partir de los datos de la muestra (línea roja, ver sección siguiente \ref{sec:est-kde}). Pese a que las distribuciones son muy similares, el \textit{test} de hipótesis con solo 100 valores nos permite afirmar (con una probabilidad menor al $5$\% de cometer un error tipo I) que la media muestral no es \mip{mu_0}.

\jupynotex[39]{Chapters/estadistica/code/estadistica.ipynb}

Otro caso muy frecuente consiste en la comparación de las medias de dos muestras diferentes en la cual la hipótesis nula consiste en asumir que las medias poblacionales de las dos variables aleatorias representadas por las muestras son iguales. Para realizar el \textit{test} de hipótesis, generaremos dos variables aleatorias con distribución normal y medias elegidas aleatoriamente. De cada una de estas distribuciones seleccionaremos muestras de 30 valores:

\jupynotex[40]{Chapters/estadistica/code/estadistica.ipynb}

La función \mip{ttest_ind()} realiza un ensayo para la hipótesis nula de que dos muestras independientes tienen la misma media asumiendo por defecto que tienen idéntica varianza. La distribución utilizada en este caso es la $t$ de Student con un \textit{test} de dos colas, y obtenemos el valor del estadístico y el $p$-valor:

\jupynotex[41]{Chapters/estadistica/code/estadistica.ipynb}

Vemos que el $p$-valor está muy por encima de $0.05$ (considerando un nivel de significancia del $5$\%), lo cual no nos permite rechazar la hipótesis nula. En la celda siguiente graficamos los histogramas correspondientes a cada muestra, así como sus distribuciones aproximadas. La escasa cantidad de valores no permite afirmar estadísticamente que las diferencias sean significativas.

\jupynotex[42]{Chapters/estadistica/code/estadistica.ipynb}

Sin embargo, si mostramos los valores elgidos estocásticamente para \mip{mu_1} y \mip{mu_2} vemos que son bien diferentes:

\jupynotex[43]{Chapters/estadistica/code/estadistica.ipynb}

Incrementando la muestra 50 valores, el $p$-valor obtenido es menor a $0.05$, permitiendo en ese caso rechazar $H_0$. Esto muestra la importancia del muestreo en la toma de decisiones basada en el \textit{test} de hipótesis.


Por último mostraremos un ejemplo de \textit{test} de hipótesis que generaliza el test anterior a dos o más poblaciones, siendo uno de los casos de análisis de varianza (ANOVA: ``ANalysis Of VAriance''), y que se basa en la idea de dividir la varianza total en la varianza entre diferentes grupos y la varianza dentro de cada grupo, y ver si estas distribuciones satisfacen la hipótesis nula de que todos los grupos pertenecen a la misma distribución. Las variables que distinguen los diferentes grupos se denominan habitualmente ``factores'' o ``tratamientos''. En el caso en que cada grupo corresponda a un valor diferente de una sola variable categórica, se utiliza un ANOVA ``de una vía'' o unidireccional (\textit{one-way ANOVA}), mientras que si se intenta determinar el efecto de dos variables categóricas, se utiliza un ANOVA bidireccional (o \textit{two-way ANOVA}). Es posible realizar también estudios de tres o más variables categóricas pero son poco comunes y puede ser difícil interpretar los resultados si se utilizan demasiados factores.

Un estudio ANOVA básico consiste en realizar los siguientes pasos. Primeramente se calcula la media de cada uno de los grupos, y a continuación se compara la varianza de estas medias (``intervarianza'') con la varianza promedio dentro de cada grupo (``intravarianza'', no explicada por la variable grupo, intervarianza). Según la hipótesis nula, todas las observaciones proceden de la misma población con lo que la intervarianza será igual a la varianza promedio dentro de los grupos. Se asumen las siguientes suposiciones:
\begin{itemize}
 \item Las distribuciones están normalmente distribuidas.
 \item Homogeneidad de varianza, las variaciaciones dentro de cada grupo son similares en todos los grupos.
 \item Las observaciones de cada grupo son independientes entre sí, y las observaciones dentro de cada grupo se obtuvieron en forma aleatoria.
\end{itemize}

En estas condiciones, para un ANOVA unidireccional las hipótesis son:
\begin{itemize}
 \item $H_0$ (hipótesis nula): $\mu_1 = \mu_2 = \ldots = \mu_k$ (las medias de los $k$ grupos son iguales).
 \item $H_1$ (hipótesis alternativa): al menos una media es diferente del resto.
\end{itemize}

Se construye entonces el estadístico $F$, que es el cociente entre la varianza de las medias de los grupos y el promedio de la varianza dentro de los grupos, y que tiene una distribución ``F de Fisher-Snedecor''. Si $H_0$ se satisface, $F$ adquiere un valor cercano a $1$ debido a que la intervarianza será igual a la intravarianza. Por otro lado, si las medias de los grupos difieren, mayor será la varianza entre medias en comparación con el promedio de las varianzas dentro de los grupos, generando valores de $F$ mayores a $1$ y en consecuencia, menor es la probabilidad de que la distribución adquiera valores tan extremos (obteniéndose de este modo un $p$-valor menor).

Veamos cómo realizar un ANOVA unidireccional mediante la función \mip{f_oneway} del módulo \mip{scipy.stats}. Para ello queremos comparar el rendimiento en un examen de un grupo de 30 alumnos, en función de dónde se sientan cuando asisten a clase: adelante, al medio o atrás. La hipótesis nula consiste en afirmar que la ubicación en el aula no afecta al rendimiento. La hipótesis alternativa indica entonces que la ubicación afecta al rendimiento.

En la celda 44 agrupamos el resultado de una evaluación en tres grupos según la ubicación habitual en el aula durante las clases de 30 alumnos, con 10 alumnos por grupo. Para tener una idea gráfica de estos rendimientos, generamos un gráfico con los \textit{boxplots} de cada grupo a los que hemos superpuesto los resultados individuales, lo que nos da una idea de la dispersión de los resultados.

A continuación, en la celda 45 importamos e invocamos la función \mip{f_oneway} (pasando como argumentos los grupos de datos) que realiza todos los cálculos necesarios para obtener el estadístico $F$ y calcular el $p$-valor a partir de los resultados muestrales. Vemos que el valor de $F$ es cercano a $1$, y el $p$-valor es suficientemente grande como para evitar el rechazo de $H_0$. Podemos intepretar esto afirmando que para los datos disponibles, no hay evidencia estadística que permita distinguir el efecto de la ubicación en las clases con el rendimiento de los alumnos\footnote{Por supuesto, esto es solo un ejemplo de utilización de la función \mip{f_oneway()} y no representa un estudio real (y \textit{serio}) del rendimiento en los exámenes según la ubicación de los alumnos en clase.}.

\jupynotex[44-45]{Chapters/estadistica/code/estadistica.ipynb}



\section{Estimación no paramétrica de la FDP}\label{sec:est-kde}

Dada una muestra, podemos asumir que representan valores aleatorios que siguen una determinada distribución de probabilidad caracterizada por ciertos parámetros (por ejemplo, media y varianza de una distribución normal). Estos parámetros pueden ser ajustados a partir de los valores muestrales usando, por ejemplo, una optimización de máxima verosimilitud. De este modo, los métodos estadísticos basados en estas funciones de distribución (tales como los \textit{tests} de hipótesis) representan ``métodos paramétricos''. En estos casos estamos realizando una suposición fuerte al determinar la función de distribución que sigue nuestro proceso aleatorio.

Una forma alternativa consiste en intentar describir la distribución a partir de los datos sin hacer ninguna suposición sobre su distribución. Tal vez la manera más frecuente es la de realizar un histograma que divide el rango de valores que contiene la muestra en intervalos iguales (o \textit{bins}) y calcula la cantidad de datos en cada intevalo, o la frecuencia si normalizamos el histograma de modo que el área total de todos los \textit{bins} sume $1$. Como resultado obtenemos una descripción empírica discontinua de la distribución que depende del número de intervalos y de sus localizaciones, por lo que la elección de estos parámetros pueden conducir a representaciones cuantitativamente diferentes de la distribución.

Una \textit{kernel density estimation} (KDE) es una función cuyo propósito es el de estimar la función de distribución de probabilidad de una manera no paramétrica, que puede expresarse como:

\begin{equation}
 f(x) = \frac{1}{nh} \sum_{i=0}^n K \left( \frac{x - x_i}{h} \right)
\end{equation}
donde como es usual, $n$ es el tamaño de la muestra (el número de valores), $x_i$ son los valores que componen la muestra, $K$ es una función \textit{kernel} y $h$ es el ancho de banda o \textit{bandwidth}. Un kernel es una función de distribución de probabilidad que debe satisfacer tres condiciones:
\begin{enumerate}
 \item Simetría: $K(x) = K(-x)$.
 \item No negativa: $\forall x, K(x) \geq 0$.
 \item Área igual a $1$: $\int_{-\infty}^{\infty} K(x) \, dx = 1$.
\end{enumerate}

Las funciones kernel determinan la forma en que se distribuye la influencia de cada observación. En la gran mayoría de los casos se usa un kernel gaussiano, pero existen otras posibilidades como las que se enumeran en la Tabla \ref{tbl:estad02} y sus correspondientes representaciones gráficas en la figura \ref{fig:estad03}. En SciPy está disponible el kernel gaussiano, mientras que todas las que aparecen en la Tabla \ref{tbl:estad02} pueden utilizarse con el módulo scikit-learn\footnote{Se puede ver la documentación \href{https://scikit-learn.org/stable/modules/density.html}{aquí}.}

\begin{figure}[t]
 \centering
 \includegraphics[width=1.0\textwidth]{Chapters/estadistica/figs/kdes.pdf}
 \caption{Algunos kernels más utilizados. a) Gaussiano; b) uniforme; c) Epanechnikov; d) exponencial; e) lineal y f) coseno.}
 \label{fig:estad03}
\end{figure}


\begin{table}[th] \small
\centering
 \caption{Kernels usuales.}
 \label{tbl:estad02}
 \begin{tabular}{p{0.25\textwidth}  p{0.35\textwidth}}
  \toprule
  \textbf{Kernel} & \textbf{Ecuación} \\
  \midrule
  Gaussiano & $\dfrac{1}{h \sqrt{2 \pi}} \exp(-x^2 / (2 h^2))$ \\[1em]
  Uniforme & $\dfrac{1}{2h}, \quad |x| \leq h$ \\[1em]
  Epanechnikov & $\dfrac{3}{4h}(1-x^2/h^2), \quad |x| \leq h$ \\[1em]
  Exponencial & $\dfrac{1}{2h} \exp(-|x|/h)$ \\[1em]
  Lineal & $\dfrac{1}{2h} (1-x/h), \quad |x| \leq h$ \\[1em]
  Coseno & $\dfrac{\pi}{4h} \cos(\pi x / 2h), \quad |x| \leq h$ \\
  \bottomrule
\end{tabular}
\end{table}

El ancho de banda $h$ controla el ancho de las distribuciones que se ubican alrededor de cada $x_i$, lo que afecta en forma importante el suavizado del KDE. Para muestras pequeñas, es recomendable usar un $h$ grande de modo que la inclusión de más puntos provea un contexto adecuado. Para muestras grandes, es mejor usar un $h$ pequeño ya que cada punto está rodeado de muchos otros, lo cual brinda un buen contexto. Existen diversos criterios para obtener el $h$ óptimo, pero para KDE gaussianos existen algunas ``reglas del pulgar'' como la regla de Silverman:
\[ h = \frac{0.9}{\sqrt[5]{n}} \min \left( s, \frac{IQR}{1.349} \right) \]
(recordemos que $IQR$ es el rango intercuartílico, ver subsección \ref{sec:est-cuantiles}). Veamos cómo generamos un KDE gaussiano a partir de una muestra y el efecto que tiene la elección de $h$.

En la celda 46 definimos la función \mip{k_gauss()} que utilizaremos para graficar las funciones gaussianas que componen la KDE (notar que no están normalizadas, ya que las usaremos solo con fines de visualización) y una función auxiliar, \mip{hacer_figura()}, para construir los gráficos de las celdas siguientes. Esta última función grafica un histograma que agrupa los datos en \mip{n_bins}, traza la KDE y por defecto traza también los kernels sobre los valores individuales (que podemos desactivar con \mip{plt_kernels=False}). Adicionalmente, esta función añade una pequeña marca en cada valor de la muestra sobre el eje horizontal (lo que se denomina un \textit{rug plot}). En la celda 49 generamos una muestra pequeña (\mip{X_chica}), establecemos el valor de $h = 0.05$ y calculamos la KDE sobre la muestra).

\jupynotex[46-47]{Chapters/estadistica/code/estadistica.ipynb}

En la figura generada, las curvas grises tenues representan cada gaussiana centrada en los valores de la muestra, que componen (cuando están correctamente normalizadas, no como en la figura) la función KDE que se muestra en línea roja. Dado que usamos un valor de $h$ muy chico, cada función gaussiana abarca una zona angosta alrededor de los valores $x_i$, dando como resultado una KDE excesivamente estructurada.

Si ahora elegimos un valor de $h$ exageradamente grande, como en la celda 48, vemos que las gaussianas individuales son tan anchas que todas abarcan al conjunto entero que compone la muestra, y en consecuencia la KDE resulta muy ``plana''.
\jupynotex[48]{Chapters/estadistica/code/estadistica.ipynb}


Finalmente, en la celda 49 seleccionamos como argumento de \mip{bw_method} la cadena \mip{'siverman'}, que aplicará esa regla para determinar el $h$ óptimo. Podemos mostrar el valor de $h$ obtenido mediante el atributo \mip{factor} de la KDE, que ahora resulta una distribución que representa mejor la distribución de los datos. Tal como dijimos previamente, dado que la muestra es chica resulta un ancho de banda relativamente grande de modo que las gaussianas individuales son relativamente anchas.

\jupynotex[49]{Chapters/estadistica/code/estadistica.ipynb}

Para finalizar, veamos un ejemplo en el que tenemos una muestra bimodal de $n = 1000$ valores que construimos en la celda 50. En este caso, el método por defecto para el cálculo del ancho de banda óptimo es el de Scott, utilizado en el cálculo de la KDE de la celda 51. Se puede ver que la KDE obtenida representa de una manera suave el histograma de los datos.

\jupynotex[50-51]{Chapters/estadistica/code/estadistica.ipynb}

\section{Lecturas recomendadas}
\begin{itemize}
 \item \fullcite{haslwanter2016}.
 \item \fullcite{schiller2012}.
\end{itemize}

