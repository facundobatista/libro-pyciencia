
% Copyright 2020-2024 Facundo Batista y Manuel Carlevaro
% Licencia CC BY-NC-SA 4.0
% Para más info visitar https://github.com/facundobatista/libro-pyciencia/

\chapter{Optimización} \label{ch:optimizacion}   

\begin{wraptable}{r}{5cm}
\begin{modulesinfo}
\begin{center}
{\small
    \begin{tabular}{l r}
        \toprule
        \textbf{Módulo} & \textbf{Versión} \\
        \midrule
        Matplotlib & 3.9.2 \\
        SciPy & 1.14.1 \\
        SymPy & 1.13.3 \\
        \bottomrule
    \end{tabular}
    \vspace{0.75em}
    
    \href{https://github.com/facundobatista/libro-pyciencia/tree/master/código/optimizacion/}{Código disponible}
}
\end{center}
\end{modulesinfo}
\end{wraptable}

Los problemas en los cuales se pretende encontrar la mejor solución aparecen frecuentemente en casi todos los ámbitos de la ciencia, la ingeniería, la tecnología y por supuesto, también en la vida cotidiana. Diariamente, para ir desde nuestro hogar al trabajo, tenemos que decidir si queremos ir por el camino más corto, más rápido, menos costoso, menos transitado, más lindo, etc. A menudo, a este requerimiento de obtener la ``mejor solución'' sobre la base de un criterio dado, le agregamos algunas restricciones adicionales que se deben cumplir también: tenemos que llegar a destino antes de una determinada hora, cruzar la menor cantidad de puentes, que nuestra ruta incluya un lugar donde tomar un desayuno. 

En general, la optimización es un proceso por el cual queremos encontrar y seleccionar el elemento óptimo de un conjunto de candidatos posibles. Utilizando el formalismo matemático, este problema se expresa como la determinación del valor extremo de una función en un dominio dado. Tal valor extremo, o valor óptimo, puede referirse al máximo o mínimo de una función, dependiendo de la aplicación y el problema específico. En este contexto, un problema de optimización tiene dos ingredientes:
\begin{itemize}
 \item Una \textbf{función objetivo} que debe ser maximizada o minimizada. Por ejemplo, minimizar la distancia en el recorrido de un vehículo de entrega de productos.
 \item Un \textbf{conjunto de restricciones} (posiblemente vacío) que se deben satisfacer. Por ejemplo, no cruzar más de dos veces un río.
\end{itemize}

Sin perder generalidad podemos considerar la optimización como un problema de minimización\footnote{Dado que es válido: $f(x_0) \geq f(x) \Leftrightarrow -f(x_0) \leq -f(x)$ es suficiente presentar el formalismo para problemas de minimización.}. La forma estándar de un problema de optimización continua es: 

\begin{align}
 \begin{split}
 \text{minimizar}& \quad f(\bm{x}) \\
 \text{sujeto a}& \quad g_i(\bm{x}) \leq b_i, \quad i = 1, \ldots, m
 \end{split}
 \label{eq:opt01}
\end{align}

Aquí, el vector $\bm{x} = (x_1, \ldots, x_n)$ es la variable de optimización del problema (también denominadas usualmente ``variables de decisión''), la función $f: \mathbb{R}^n \rightarrow \mathbb{R}$ es la función objetivo, las funciones $g_i: \mathbb{R}^n \rightarrow \mathbb{R}, \; i = 1, \cdots, m$ son las funciones de restricción, y las constantes $b_1, \cdots, b_m$ son los límites de las restricciones. Un vector $\bm{x}^*$ se denomina óptimo, o solución del problema \eqref{eq:opt01}, si tiene el menor valor objetivo entre todos los vectores que satisfacen las restricciones: para todo $\bm{z}$ con $g_1(\bm{z}) \leq b_1, \ldots, g_m(\bm{z}) \leq b_m$ tenemos que $f(\bm{x}^*) \leq f(\bm{z})$. 

Las formas particulares que presentan las funciones objetivo y de restricción caracterizan familias o clases de problemas de optimización muy variados. Un abordaje matemático general es difícil de tratar y no hay métodos eficientes para resolver completamente problemas genéricos de optimización. Sin embargo, existen métodos muy eficientes para muchos casos de importancia teórica o práctica, en los cuales resulta valioso conocer todo lo posible sobre las funciones objetivo y de restricciones para resolver con éxito el problema.

Los problemas de optimización se clasifican según las propiedades de $f(\bm{x})$ y $g_i(\bm{x})$. En primer lugar, si $x$ es un escalar ($x \in \mathbb{R}$) el problema es unidimensional o univariado, mientras que si $\bm{x}$ es un vector ($\bm{x} \in \mathbb{R}^n$), el problema es multidimensional o multivariado. Naturalmente, cuanto mayor es $n$, más difícil es resolver el problema de optimización, y requiere más potencia de cálculo.

Cuando la función objetivo y las funciones de restricción son lineales, es decir, satisfacen que
\[ f(\alpha \bm{x} + \beta \bm{y}) = \alpha f(\bm{x}) + \beta f(\bm{y}) \]
se suele llamar al problema de optimización \eqref{eq:opt01} como \textbf{programación lineal}\footnote{Pese al nombre, no tiene que ver con programación de computadoras.}. Si la función objetivo o las de restricción son no lineales, el problema de optimización se denomina \textbf{programación no lineal}. Cuando se requiere que el vector $\bm{x}$ solo tome valores enteros, se trata de un problema de \textbf{programación entera}, mientras que si la restricción consiste en que solo tome dos valores (por ejemplo, 0 y 1), estamos frente a un problema de \textbf{programación binaria}. En relación con las funciones de restricción $g_i$, los problemas de optimización pueden ser con o sin restricciones, y en este último caso dichas restricciones pueden expresarse en forma de igualdades o desigualdades.

Los problemas no lineales presentan una complejidad mayor que los lineales, debido a que tienen una variedad muy amplia de posibles comportamientos. En general, estos problemas pueden tener mínimos globales y locales, lo que hace difícil indentificar al mínimo global. 


\section{Optimización unidimensional}
La forma más obvia de encontrar el mínimo de una función es diferenciarla y encontrar el valor de la variable independiente que hace la derivada igual a cero. Sin embargo, en muchas situaciones no es práctico encontrar la derivada directamente. El submódulo \mip{optimize} de SciPy provee dos métodos para encontrar el mínimo de una función unidimensional: \mip{brent} y \mip{golden}, aunque en la documentación de SciPy sugieren no utilizar este último, incluído solo con fines académicos. Estos métodos solo utilizan evaluaciones de la función a optimizar, sin requerir del cálculo de la derivada, y se aproximan al valor óptimo mediante técnicas de \textit{bracketing} o acotamiento. Para funciones con un único mínimo en un intervalo dado, estas metodologías de \textit{bracketing} garantizan la convergencia al punto óptimo, pero esto no está asegurado para funciones más complejas.

Como ejemplo de optimización unidimensional consideraremos el ejemplo clásico de minimizar el área de un cilindro con un volumen $V_0$ dado. Las variables a considerar son el radio $r$ y la altura $h$ del cilindro, y la función objetivo es $f(r, h) = 2 \pi r^2 + 2 \pi r h$, con la condición de restricción $g(r, h) = \pi r^2 h - V_0 = 0$. El problema así planteado presenta dos variables, pero a partir de la restricción podemos reducirlo a una sola dimensión usando $h = V_0/\pi r^2$ y substituyendo en $f(r, h)$, lo que conduce a minimizar $f(r) = 2 \pi r^2 + 2 V_0 / r$.

Comenzaremos solucionando el problema en forma analítica aprovechando las capacidades de cálculo simbólico de SymPy. En la primera celda declaramos las variables y ecuaciones que representan el problema, obteniendo el valor exacto de $r$ que minimiza el área para un volumen $V_0$ dado:
\jupynotex[1]{Chapters/optimizacion/code/optimizacion.ipynb}

Podemos verificar que es un mínimo evaluando las derivadas primera y segunda de \mip{Area_r} en el valor obtenido para \mip{r_sol}:
\jupynotex[2]{Chapters/optimizacion/code/optimizacion.ipynb}
\noindent dando un valor positivo ($12 \pi$) para la derivada segunda confirmando que \mip{r_sol} es un mínimo. El área que corresponde a esta solución es:
\jupynotex[3]{Chapters/optimizacion/code/optimizacion.ipynb}

Utilizaremos el valor exacto del mínimo obtenido para $r$ (\mip{r_sol}) en la celda 1 para compararlo con el que obtendremos mediante una optimización numérica. Para ello evaluamos \mip{r_sol} para el caso en que $V_0 = 1$: 
\jupynotex[4]{Chapters/optimizacion/code/optimizacion.ipynb}

Para problemas simples la resolución analítica es posible, pero en problemas reales es común tener que recurrir a métodos numéricos (por ejemplo cuando no es posible resolver la ecuación $f'(x) = 0$). Para abordar este problema con el submódulo \mip{optimize} de SciPy, primero debemos definir la función \mip{f()} que implementa la función objetivo (en este caso asumimos que $V_0 = 1$), y luego pasamos esta función como argumento de \mip{minimize_scalar} para realizar la minimización numérica, junto con un intervalo de \textit{bracketing} y la selección del método de optimización. El método de Brent utiliza una combinación de interpolación parabólica y el método de sección de oro. En la celda siguiente realizamos la optimizaciòn de nuestro problema con este método:

\jupynotex[5]{Chapters/optimizacion/code/optimizacion.ipynb}

Como resultado de la optimización, el método devuelve un objeto (asignado en este caso a la variable \mip{solucion}) que en sus atributos contiene informción acerca de cómo fue la ejecución de la optimización, tal como un mensaje que describe el motivo de la finalización, una variable booleana que indica si hubo convergencia (\mip{success}), el valor de la función evaluada en el valor óptimo (\mip{fun}), el valor óptimo hallado (\mip{x}), el número de iteraciones (\mip{nit}) y las llamadas a la función objetivo (\mip{nfev}). Podemos ver que el resultado numérico coincide con el valor ``exacto'' obtenido con SymPy hasta la precisión informada. La figura \ref{fig:opt-1d} muestra la función y el valor óptimo hallado. Siempre es recomendable visualizar la función antes de aceptar el valor obtenido como válido.

La función \mip{minimize_scalar} permite establecer, además, la tolerancia requerida para la convergencia o el número máximo de iteraciones. Sugerimos ver las opciones en su documentación\footnote{\url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize_scalar.html\#scipy.optimize.minimize_scalar}.}.
\begin{figure}[t]
 \centering
 \includegraphics[scale=0.75]{Chapters/optimizacion/figs/optim-1d.pdf}
 % optim-1d.pdf: 0x0 px, 300dpi, 0.00x0.00 cm, bb=
 \caption{Optimización de la superficie de un cilindro con volumen dado.}
 \label{fig:opt-1d}
\end{figure}

\section{Optimización multidimensional sin restricciones} \label{sec:omsr}
La optimización multidimensional es más compleja que la correspondiente a la de una sola dimensión que vimos en la sección anterior. Los métodos de \textit{bracketing} no son aplicables y en general se deben aplicar métodos que comienzan en algún punto en el espacio de coordenadas y mediante alguna estrategia se buscan nuevos puntos que sucesivamente se aproximen al mínimo.

Como abordaremos el caso de una minimización sin restricciones, podemos expresar nuestro problema como
\[ \text{Minimizar } f(\bm{x}) \quad \forall \bm{x} \in \mathbb{R}^n \]

donde $f(\bm{x})$ es una función no lineal de $\bm{x} = (x_1, \ldots, x_n)$. Estos problemas son frecuentes en muchas aplicaciones, por ejemplo en problemas de redes neuronales donde es necesario encontrar los pesos que minimizan la diferencia entre la salida de la red y la respuesta conocida a la entrada.

Iniciamos entonces asumiendo una aproximación inicial $\bm{x}_0$, y procedemos a generar una mejor aproximación utilizando una fórmula iterativa de la forma
\[ \bm{x}_{k + 1} = \bm{x}_k + s \bm{d}_k, \quad \text{ para } k = 0, 1, 2, \ldots \]

Para utilizar esta fórmula es necesario determinar el vector $\bm{d}_k$, que representa la dirección en la que buscamos el nuevo valor $\bm{x}_{k+1}$, y el escalar $s$ que representa la distancia en que nos movemos en esa dirección. Se pueden implementar muchas estrategias para definir estas cantidades. Tal vez la elección más simple consiste en tomar la dirección $\bm{d}_k$ como el negativo del vector gradiente de $f$ en $\bm{x}_k$. Para un paso $d$ suficiente pequeño, esta estrategia garantiza una reducción en el valor de la función. Esto conduce al algoritmo del descenso del gradiente:
\[ \bm{x}_{k+1} = \bm{x}_k - s \nabla f(\bm{x}_k) \]
donde $\nabla f(\bm{x}) = (\partial f / \partial x_1, \partial f / \partial x_2, \cdots, \partial f / \partial x_n)$, y $s$ es un valor constante pequeño. El mínimo se alcanza cuando el gradiente se anula. Aunque en cada paso nos movemos hacia un valor menor de la función, utilizando valores chicos de $s$ el algoritmo es muy lento. Por otra parte, un valor grande de $s$ pone en riesgo la convergencia hacia el mínimo.

El valor de $s$ se puede optimizar considerando que el valor que produce la máxima reducción de $f$ es el que se obtiene de minimizar la función a lo largo de la dirección establecida por $-\nabla f(\bm{x}_k)$. Formalmente, esto significa que para cada $k$ debemos hallar el valor de $s$ que minimiza
\begin{equation}
 f[\bm{x}_k - s \nabla f(\bm{x}_k)] 
 \label{eq:opt02}
 \end{equation}
que también constituye un problema de minimización denominado ``búsqueda lineal'', dado que en definitiva es una minimización en una sola variable, $s$, a lo largo de la dirección del gradiente, ya que $\bm{x}_k$ es conocido. Claramente, el valor que minimiza $f[\bm{x}_k - s \nabla f(\bm{x}_k)]$ es tal que la derivada de $f[\bm{x}_k - s \nabla f(\bm{x}_k)]$ es cero. Entonces, diferenciando $f[\bm{x}_k - s \nabla f(\bm{x}_k)]$ respecto de $s$ tenemos:
\[ \frac{d f[\bm{x}_k - s \nabla f(\bm{x}_k)]}{ds} = -\left( \nabla f(\bm{x}_{k+1}) \right)^{\intercal} \nabla f(\bm{x}_k) = 0 \]
lo que muestra que las direcciones sucesivas de búsqueda son ortogonales. Si bien este método genera una secuencia que se aproxima al mínimo de $f$, no presenta la mejor forma de hacerlo dado que los cambios en las direcciones de búsqueda son grandes. 

Una mejora consiste en tomar una combinación de la dirección previa y de la nueva dirección para aproximarnos al valor óptimo en forma más directa, lo que da origen al método del gradiente conjugado. Utiliza el mismo paso $s$ obtenido en \eqref{eq:opt02}, y, llamando $\bm{g}_{k+1} = \nabla f(\bm{x}_{k+1})$, la nueva dirección resulta de
\[ \bm{d}_{k+1} = -\bm{g}_{k+1} + \beta \bm{d}_k \]

Esto es, la nueva dirección es una combinación del negativo del gradiente en la nueva posición mas el escalar $\beta$ por la dirección previa de búsqueda. Ahora el problema es establecer el valor de $\beta$, y el criterio utilizado es que las sucesivas direcciones de búsqueda sean conjugadas, esto es, que $(\bm{d}_{k+1})^{\intercal} \bm{A} \bm{d}_k = 0$ para alguna matriz dada $\bm{A}$.

Esta elección posee propiedades adecuadas de convergencia para el método del gradiente conjugado. Se puede mostrar que conduce a un valor de $\beta$ dado por:
\[ \beta = \frac{(\bm{g}_{k+1})^{\intercal} \bm{g}_{k+1}}{(\bm{g}_{k})^{\intercal} \bm{g}_k} \]

El método de Newton para optimización multidimensional se puede considerar como una modificación del descenso del gradiente que puede mejorar la convergencia. Del mismo modo que para el caso unidimensional, el método de Newton consiste en una aproximación cuadrática local de la función, que cuando se minimiza da origen a un esquema iterativo. Escribiendo una expansión en serie de Taylor de segundo orden para $f(\bm{x})$ alrededor de $\bm{x} = \bm{x}_i$:
\[ f(\bm{x}) = f(\bm{x}_i) + \nabla f^{\intercal}(\bm{x}_i)(\bm{x} - \bm{x}_i) + \frac{1}{2}(\bm{x} - \bm{x}_i)^{\intercal} H_i(\bm{x} - \bm{x}_i) \]
donde $H_i$ es la matriz hessiana\footnote{La matriz hessiana de un campo escalar $f : \mathbb{R}^n \rightarrow \mathbb{R}$ es la matriz cuadrada de tamaño $n \times n$ que tiene como entradas las derivadas parciales de segundo orden. Ver \href{https://es.wikipedia.org/wiki/Matriz_hessiana}{entrada} en Wikipedia.}. En el mínimo, 
\[ \frac{\partial f(\bm{x})}{\partial \bm{x}_j} = 0, \quad \text{para } j = 1, 2, \cdots, n \]

Entonces, 
\[ \nabla f = \nabla f(\bm{x}_i) + H_i(\bm{x} - \bm{x}_i) = 0 \]

Si $H_i$ no es singular,
\[ \bm{x}_{i+1} = \bm{x}_i - H_i^{-1} \nabla f(\bm{x}_i) \]
que se puede mostrar converge cuadráticamente cerca del mínimo. Este procedimiento altera, por lo general, tanto la dirección como la longitud del paso, por lo que no es estrictamente un método del descenso del gradiente, y puede no converger si el punto de partida no está suficientemente cerca del mínimo. Tal como está planteado aquí, el método de Newton requiere tanto el cálculo como el almacenamiento del gradiente y del hessiano de la función. Esto puede resultar impractible para funciones de muchas variables (particularmente para optimización de la función de pérdida durante el entrenamiento de redes neuronales), por lo que existen métodos más apropiados, como los algoritmos cuasi-Newton, siendo uno de los más populares el BFGS (Broyden - Fletcher - Goldfarb - Shanno).

El submódulo \mip{optimize} de SciPy contiene la función \mip{minimize}, que permite utilizar, entre otros, los métodos mencionados aquí. Vamos a mostrar un ejemplo de optimización de la función de dos variables:
\[ f(x_1, x_2) = \frac{1}{2} \sum_{i=1}^2 (x_i^4 - 16 x_i^2 + 5 x_i) \]
que tomamos de \citeauthor{styblinski1990} \cite{styblinski1990} y tiene cuatro mínimos locales, con el mínimo global localizado en $x_i^* = -2.903534, i= 1, 2$, y un máximo local ubicado cerca del origen. Para utilizar el método de Newton, necesitamos calcular el gradiente de la función y también el hessiano. En el ejemplo que estamos analizando es simple realizar estos cálculos manualmente, pero aprovecharemos la oportunidad para utilizar la potencia de cálculo simbólico de SymPy, y veremos cómo utilizar las expresiones simbólicas obtenidas en el contexto numérico de NumPy. 

Comenzamos entonces definiendo los símbolos a utilizar , la función $f$ (simbólica) y sus derivadas parciales, tal como se muestra en la celda 6:
\jupynotex[6]{Chapters/optimizacion/code/optimizacion.ipynb}
Podemos ver el gradiente representando en forma matricial (aunque estrictamente es un vector columna en este caso) utilizando la función \mip{Matrix} de SymPy:
\jupynotex[7]{Chapters/optimizacion/code/optimizacion.ipynb}

Análogamente podemos obtener la matriz hessiana:
\jupynotex[8-9]{Chapters/optimizacion/code/optimizacion.ipynb}

Habiendo realizado estos cálculos, necesitamos ahora obtener la representación vectorizada de estas funciones, de modo de que puedan ser manipuladas por NumPy y SciPy. Para ello invocamos la función \mip{lambdify} de SciPy:

\jupynotex[10]{Chapters/optimizacion/code/optimizacion.ipynb}

\noindent con lo que ahora \mip{f_num}, \mip{f_d1_num} y \mip{f_hess_num} representan a $f$, su gradiente y su matriz hessiana, respectivamente. Sin embargo, las funciones generadas por \mip{lambdify} toman un argumento por cada variable en la correspondiente expresión, mientras que las funciones de optimización de SciPy esperan funciones vectorizadas en las que todos los argumentos se encuentran empaquetados en un array. Por lo tanto, para obtener funciones que sean compatibles con las rutinas de optimización de SciPy, debemos ``envolver'' las funciones generadas por \mip{lambdify} con funciones de Python que reacomoden los argumentos, tal como mostramos en la celda 11:
\jupynotex[11]{Chapters/optimizacion/code/optimizacion.ipynb}

Finalmente tenemos todo listo para proceder con la optimización. Esto lo hacemos con la función \mip{minimze} de \mip{scipy.optimize}, pasándole como argumentos la función a minimizar (\mip{f}), su gradiente (\mip{f1}, argumento \mip{jac}) y la matriz hessiana (\mip{fh}, argumento \mip{hess}), así como el punto de partida de la búsqueda: \mip{(1, 1)}:

\jupynotex[12]{Chapters/optimizacion/code/optimizacion.ipynb}

La función encuentra un mínimo en $(x_1, x_2) = (2.747, 2.747)$, y muestra información sobre cómo ha resultado la ejecución del algoritmo, incluyendo el número de iteraciones, el valor de la función en el mínimo, el valor del gradiente (\mip{jac}) y la cantidad de evaluaciones de la función, del gradiente y del hessiano. Podemos verificar el valor de la función en el mínimo hallado:

\jupynotex[13]{Chapters/optimizacion/code/optimizacion.ipynb}

Como es usual en estos casos, resulta útil visualizar el problema. Para ello realizamos un mapa de calor donde el color representa el valor de la función y marcamos con una estrella roja el óptimo hallado:

\jupynotex[14]{Chapters/optimizacion/code/optimizacion.ipynb}

En la práctica no siempre suele ser factible calcular el gradiente o el hessiano de la función, por lo que pueden utilizarse otros métodos que estiman estas funciones numéricamente. Un método popular mencionado anteriormente es el BFGS. Para utilizar este método podemos invocar la función \mip{optimize} con el argumento \mip{method='BFGS'}:
\jupynotex[15]{Chapters/optimizacion/code/optimizacion.ipynb}
Vemos que en este caso no es necesario suministrar el gradiente ni la matriz hessiana, y tambien podemos notar que, en consecuencia, el número de evaluaciones de la función aumenta significativamente (30) respecto del método de Newton (7 evaluaciones).

Otro método del que dispone SciPy es el del gradiente conjugado. En este caso podemos usar el gradiente de la función, mientras que el hessiano no es requerido:
\jupynotex[16]{Chapters/optimizacion/code/optimizacion.ipynb}

Si no se conocen ni el gradiente ni el hessiano, el método BFGS suele ser un buen punto de partida para la búsqueda del óptimo. En caso de poder calcular el gradiente, el método del gradiente conjugado resulta más eficiente (en nuestro caso solo necesita 17 evaluaciones de la función para hallar el mínimo). Naturalmente, cuanta más información le podamos aportar al método, más eficiente será el cálculo, tal como se muestra al utilizar el método de Newton, que provee la convergencia más rápida. En cualquier caso es necesario también evaluar la complejidad de las funciones para el cálculo del gradiente o del hessiano, debido a que puede resultar costosa la evaluación numérica de ellas, haciendo más conveniente el uso del método BFGS.

En general, los métodos que hemos visto hasta aquí convergen a mínimos locales. En los problemas con muchos mínimos locales, es muy posible que estos métodos converjan a uno de estos mínimos locales (dependiendo, naturalmente, del valor inicial de la iteración), y que nos resulte difícil localizar el mínimo global. En estos casos es recomendable realizar una primera búsqueda utilizando un método de fuerza bruta, que consiste simplemente en discretizar el dominio en una grilla y evaluar la función en los nodos de esta discretización, y utilizar el resultado de esta búsqueda como punto incial de un método iterativo. SimPy provee la función \mip{brute} de \mip{optimize} para realizar esta búsqueda inicial, a la que le pasamos como argumento la función \mip{f}, y una tupla de objetos \mip{slice}, con un elemento por cada dimensión del espacio de parámetros (valor inicial, valor final y paso para cada coordenada), que especifica la grilla sobre la que se evaluará la función. Como tercer argumento especificamos \mip{finish=None} para evitar que la función \mip{brute} refine la búsqueda (dejaremos esa tarea a uno de nuestros algoritmos, posteriormente):
\jupynotex[17-18]{Chapters/optimizacion/code/optimizacion.ipynb}
Vemos que el punto de la grilla $(-3, 3)$ ofrece el menor valor de \mip{f} ($-78$). Ahora usamos ese punto como valor inicial del método BFGS:
\jupynotex[19]{Chapters/optimizacion/code/optimizacion.ipynb}
\noindent que converge al valor del mínimo global que anticipamos al inicio. Dado que nuestro punto de partida es bastante cercano al óptimo, el número de evaluaciones requeridas por el algoritmo BFGS es la mitad de la requerida para el punto inicial $(1, 1)$, que converge a un mínimo local.

\section{Optimización con restricciones}

La incorporación de restricciones a los problemas de optimización incrementa el nivel de complejidad de los métodos para obtener las soluciones posibles. No existe una única forma de categorizar estos problemas, en este capítulo abordaremos tres formas de considerar restricciones: límites en las variables de decisión, programación lineal y programación no lineal.

\subsection{Restricciones en las variables de decisión}
Tal vez la manera más simple de incluir restricciones en un problema de optimización consiste en limitar el dominio de búsqueda de los valores óptimos de las variables de decisión. Por ejemplo, encontrar el mínimo de $f(x, y)$ sujeto a $a \leq x \leq b$ y $c \leq y \leq d$. La restricción $a \leq x \leq b$ es simple dado que solo restringe los valores de la variable $x$ independientemente de las otras variables del problema.

SciPy puede resolver estos problemas de optimización con restricciones simples por medio del algoritmo L-BFGS-B, que es una variante del método BFGS que utilizamos en la sección \ref{sec:omsr}. Veamos cómo aplicamos este método para obtener el mínimo de la función de Rosembrock\footnote{Ver \url{https://es.wikipedia.org/wiki/Función_de_Rosenbrock}.}:
\[ f(x_1, x_2) = (a - x_1)^2 + b (x_2 - x_1^2)^2 \]
donde usaremos $a = 1$ y $b = 1$. Esta función tiene un mínimo global en $(a, a^2)$ en donde la función se anula. Intentaremos localizar los mínimos en el problema sin restricciones, y restringiendo el dominio de las variables de decisión al rectángulo $2 \leq x_1 \leq 3$ y $0 \leq x_2 \leq 2$:

\jupynotex[20]{Chapters/optimizacion/code/optimizacion.ipynb}

Aquí podemos ver que cuando optimizamos sin restringir los valores de $x_1$ y $x_2$ localizamos el mínimo global de la función de Rosembrock, pero al establecer la optimización limitada a los intervalos especificados para cada variable, el valor óptimo se establece en $(2, 2)$ en donde $f = 5$. Dado que este ejemplo presenta una optimización de dos variables, podemos realizar un gráfico para visualizar cómo se ubican los valores óptimos en cada caso. En la celda siguiente construimos esta visualización, realizando previamente una definición de la función de Rosembrock para que acepte como parámetros de entrada dos valores en vez de una tupla, ya que de este modo necesitamos ingresar los valores de la grilla de visualización. Marcamos también en gris el rectángulo que comprenden los intervalos de la restricción. La estrella azul representa los valores que minimizan la función sin restricciones, mientras que la roja lo hace al establecer las restricciones.

\jupynotex[21]{Chapters/optimizacion/code/optimizacion.ipynb}

\subsection{Programación lineal} \label{subsec:proglin}
La programación lineal aborda el problema de optimizar una función objetivo lineal sujeta a restricciones lineales en forma de igualdades y desigualdades sobre las variables de decisión. Estos problemas se pueden expresar en una variedad de formas equivalentes. Veremos en un ejemplo simple cómo abordar un problema de programación lineal.

Supongamos que disponemos de tres tipos de alimentos, $A_1$, $A_2$ y $A_3$, que contienen tres tipos de nutrientes (carbohidratos, proteínas y vitaminas) en las siguientes cantidades:

\begin{center}
\begin{tabular}{ccccc}
\toprule
 Alimento & Carbohidratos & Proteínas & Vitaminas & Costo (\$/kg) \\
 \midrule
 $A_1$ & 1 & 4 & 3 & 0.82 \\
 $A_2$ & 7 & 2 & 2 & 0.50 \\
 $A_3$ & 2 & 2 & 0 & 0.40 \\
 \bottomrule
\end{tabular}
\end{center}

El requerimiento diario de carbohidratos, proteínas y vitaminas es 10, 15 y 6, respectivamente. El problema es determinar cuánto es necesario consumir de cada alimento para obtener las cantidades requeridas de cada nutriente al menor costo, por lo tanto, la elección natural de las variables de decisión a optimizar será $x_1$, $x_2$ y $x_3$ como las cantidades a consumir por día de cada alimento.

El paso siguiente es determinar la función objetivo a maximizar o minimizar. En este ejemplo, queremos minimizar el costo diario total, que se obtiene mediante el cálculo $0.82 \, x_1 + 0.50 \, x_2 + 0.40  \, x_3$.

Por último necesitamos describir las restricciones que deben satisfacer $x_1$, $x_2$ y $x_3$. En primer lugar, estas variables deben ser positivas (no podemos comer cantidades negativas de alimentos). Estas restricciones se denominan ``restricciones de no negatividad'', y se encuentran muy frecuentemente en los problemas de programación lineal. Además, no todos los valores de $x_1$, $x_2$ y $x_3$ dan lugar a una dieta con las cantidades diarias necesarias de nutrientes. La cantidad de proteínas en $x_1$ unidades de $A_1$, $x_2$ de $A_2$ y $x_3$ de $A_3$ es $4 \, x_1 + 2 \, x_2 + 2 \, x_3$, y esa cantidad debe ser al menos de 15 por día. Esto significa que $x_1$, $x_2$ y $x_3$ deben satisfacer $4 \, x_1 + 2 \, x_2 + 2 \, x_3 \geq 15$.

Entonces, este problema de optimización dietaria se puede formular de la siguiente forma:

\begin{align}
 \begin{split}
 \text{minimizar:}& \quad f(x_1, x_2, x_3) = 0.82 \, x_1 + 0.50 \, x_2 + 0.40  \, x_3 \\
 \text{sujeto a:}& \quad x_1 + 7 \, x_2 + 2 \, x_3 \geq 10 \\
 & \quad  4 \, x_1 + 2 \, x_2 + 2 \, x_3 \geq 15 \\
 & \quad  3 \, x_1 + 2 \,  x_2 \geq 6 \\
 & \quad  x_1 \geq 0; \; x_2 \geq 0; \; x_3 \geq 0
 \end{split}
 \label{eq:opt03}
\end{align}

Se dice que una solución $\bm{x} = (x_1, x_2, x_3)$ es factible con respecto al programa lineal anterior si satisface todas las restricciones anteriores. El conjunto de soluciones factibles se denomina espacio factible o región factible. Una solución factible es óptima si el valor de su función objetivo es igual al valor más pequeño que $f$ puede tomar en la región factible.

Para obtener la solución a este problema de programación lineal utilizaremos la función \mip{linprog} de \mip{scipy.optimize}, que puede resolver problemas con la forma:

\begin{align}
 \begin{split}
 \min_{\bm{x}} & \; \bm{c}^{\intercal} \, \bm{x} \\
 \text{tal que:}& \quad \bm{A}_{ub} \, \bm{x} \leq \bm{b}_{ub} \\
 & \quad \bm{A}_{eq} \, \bm{x} \leq \bm{b}_{eq} \\
 & \quad  \bm{l} \leq \bm{x} \leq \bm{u}
 \end{split}
 \label{eq:opt04}
\end{align}
donde $\bm{x}$ representa el vector con las variables de decisión, $\bm{c}$ es el vector con los coeficientes de la función lineal $f$, $\bm{A}_{ub}$ es la matriz con los coeficientes de las restricciones de desigualdad, en la que cada fila especifica los coeficientes de una desigualdad lineal sobre $\bm{x}$, $\bm{b}_{ub}$ es el vector de las desigualdades lineales en el que cada elemento representa el límite superario del correspondiente valor de $\bm{A}_{ub} \, \bm{x}$. Del mismo modo, $\bm{A}_{eq}$ y $\bm{b}_{eq}$ representan restricciones mediante igualdades (que en este problema no tenemos). Finalmente, $\bm{l}$ y $\bm{u}$ son los límites inferiores y superiores del dominio de búsqueda de los valores óptimos de $\bm{x}$ (intepretando la desigualdad elemento por elemento).

Podemos ver que en nuestro problema representado en la forma \eqref{eq:opt03} las restricciones de desigualdad tienen el sentido inverso al requerido por \mip{linprog}. Esto no representa ninguna dificultad, ya que multiplicando cada ecuación miembro a miembro por $(-1)$ las adaptamos para usarlas en la función. 

La función \mip{linprog} utiliza los métodos \mip{highs-ds} y \mip{highs-ipm} para resolver el problema de minimización. que consisten en rutinas desarrolladas en C++ de alto desempeño\footnote{Ver \url{https://highs.dev/}.} que implementan los métodos simplex dual revisado y de punto interior, cuyas descripciones exceden el alcance de este libro \cite{huangfu2018}. En la celda 22 definimos \mip{c} como una lista con los coeficientes de $f$, \mip{A} representa la matriz $\bm{A}_{ub}$, la lista \mip{b} contiene los elementos del vector $\bm{b}_{ub}$, y finalmente establecemos los límites para cada variable de decisión con la tupla \mip{(0, None)} que significa que el límite inferior en cada variable es cero, y el superior es infinito (todas las variables de decisión son no negativas):

\jupynotex[22]{Chapters/optimizacion/code/optimizacion.ipynb}

Ahora podemos invocar la función \mip{linprog}, guardando su resultado en un objeto \mip{res} que contiene el valor mínimo de la función objetivo (\mip{res.fun}) evaluada en los valores óptimos obtenidos (\mip{res.x}):

\jupynotex[23]{Chapters/optimizacion/code/optimizacion.ipynb}

\noindent que verificamos mostrando el valor del producto \mip{c @ res.x}. El objeto \mip{res} contiene más información acerca de la ejecución de la rutina de optimización:

\jupynotex[24]{Chapters/optimizacion/code/optimizacion.ipynb}

Para la interpretación de la información contenida en \mip{res}, invitamos a ver la documentación\footnote{\url{https://docs.scipy.org/doc/scipy/reference/optimize.linprog-highs.html\#optimize-linprog-highs}.}.

\subsection{Programación no lineal}
Una vez más, cuando en el problema intervienen relaciones no lineales entre las variables de decisión, la complejidad aumenta y son necesarios métodos más sofisticados para encontrar el valor óptimo, y en muchas oportunidades no es posible obtener una minimización o maximización que respete todas las restricciones. 

Una de las técnicas que se pueden utilizar para problemas de optimización con restricciones no lineal es la de los multiplicadores de Lagrange, que convierte un problema con restricciones en uno sin restricciones mediante la incorporación de variables adicionales. La idea es incorporar cada restricción a la función objetivo y resolver el problema como uno sin restricciones.

Por ejemplo, consideremos el problema de optimización $\min f(\bm{x})$ sujeto a restricciones de igualdad $g(\bm{x}) = 0$. Si no consideramos esta restricción, el gradiente de $f(\bm{x})$ se anula en los puntos extremos, $\nabla f(\bm{x}) = 0$. Se puede mostrar que al incorporar la restricción, el negativo del gradiente pertenece al espacio soportado por la restricción normal, es decir, $-\nabla f(\bm{x}) = \bm{\lambda } J_g^{\intercal}(\bm{x})$, donde $J_g(\bm{x})$ es la matriz jacobiana de la función de restricción $g(\bm{x})$ y $\bm{\lambda}$ son las nuevas variables representadas por el vector de los multiplicadores de Lagrange. Esta condición tiene su origen en igualar a cero la función $L(\bm{x}, \bm{\lambda}) = f(\bm{x}) + \bm{\lambda}^{\intercal} g(\bm{x})$ denominada ``función lagrangiana''. Si ambas funciones $f$ y $g$ son continuas y diferenciables, un punto estacionario $(\bm{x}_0, \bm{\lambda}_0)$ de $L$ corresponde a un $\bm{x}_0$ que es un óptimo del problema original con restricciones.

Veamos un ejemplo minimizando nuevamente la función de Rosembrock 
\[ f(x_1, x_2) = (a - x_1)^2 + b (x_2 - x_1^2)^2 \]
con la condición $g(x_1, x_2) = x_1^2 + x_2^2 = 1$. Al igual que en el ejemplo de la subsección \ref{subsec:proglin}. tomamos $a = 1$ y $b = 1$. Esta vez utilizaremos como argumento de la función \mip{minimize} el método SLSQP (\textit{Sequential Least Squares Programming}), que implementa en forma iterativa la optimización de un modelo cuadrático utilizando aproximaciones de segundo orden del lagrangiano. La condición de restricción debe pasarse como diccionario en el argumento \mip{constraints} de \mip{optimize}, declarando en la definición que se trata de una restricción de igualdad a cero (\mip{type='eq'}):

\jupynotex[25]{Chapters/optimizacion/code/optimizacion.ipynb}

Ahora hacemos la minimización del mismo modo que en los ejemplos anteriores:

\jupynotex[26]{Chapters/optimizacion/code/optimizacion.ipynb}

Y visualizamos el proceso mostrando con una estrella azul el mínimo de la función de Rosembrock sin restricciones, y el nuevo óptimo (en rojo) cuya restricción exige que se encuentre sobre la circunferencia de radio $1$:

\jupynotex[27]{Chapters/optimizacion/code/optimizacion.ipynb}

\section{Algoritmo genético}

En esta sección tomaremos un abordaje cualitativamente diferente al que hemos usado hasta ahora para resolver problemas de optimización. Introduciremos en forma sintética el algoritmo genético (AG), como ejemplo de la clase de métodos heurísticos de optimización. En particular, los AG están inspirados en el proceso de selección natural en el que una ``población'' de posibles soluciones (denominados ``individuos'' o ``cromosomas'') evolucionan hacia soluciones mejores.

Los individuos de un AG codifican las variables de decisión del problema de optimización en cadenas de longitud finita de algún alfabeto. Estas cadenas que representan candidatos a la solución están compuestas de ``genes'' dados por el alfabeto utilizado. Para evolucionar hacia mejores soluciones e implementar un mecanismo de selección natural, es necesario una medida que distinga buenas soluciones de las que no son tan adecuadas para resolver el problema. Por lo general, esta medida es una función objetivo, que puede estar dada tanto por una expresión matemática como por el resultado de una simulación, y a la que generalmente se la denomina \textit{fitness}.

Una vez que el problema ha sido codificado a través de un determinado alfabeto en forma de cromosomas, y que disponemos de la correspondiente función de \textit{fitness}, el algoritmo de evolución se implementa de la siguiente manera:

\begin{enumerate}
 \item Inicialización: se establece una población inicial de inidividuos, generalmente generados de forma aleatoria sobre el dominio de búsqueda. Si es posible incopororar conocimiento específico del problema en cuestión, se puede hacer en este paso.
 \item Evaluación: una vez que la población ha sido inicializada, o cuando se obtiene una nueva generación de hijos, es necesario evaluar el \textit{fitness} de cada individuo.
 \item Selección: Un conjunto de individuos de la población, priorizando aquellos con mejor \textit{fitness}, es elegido para formar el conjunto de padres de la siguiente generación de individuos.
 \item Recombinación (o \textit{crossover}): el cromosoma de dos padres elegidos en el paso anterior es recombinado para obtener nuevos individuos hijos.
 \item Mutación: en forma aleatoria se modifica un individuo de la población.
 \item Reemplazo: la generación de hijos generada por selección, recombinación y mutación reemplaza (total o parcialmente) la generación anterior de individuos.
 \item Se repiten los pasos 2 -- 6 hasta que se satisfaga una condición de finalización.
\end{enumerate}

Cada uno de estos pasos requiere de la implementación de operadores (selección, \textit{crossover}, mutación y reemplazo) que puede hacerse de diversas maneras, y de la elección de cada uno de ellos dependerá la eficacia con la que el AG evolucione hacia el óptimo. Mencionaremos algunas opciones a continuación.

Los métodos de selección se pueden clasificar en selección proporcional al \textit{fitness} o en métodos ordinales. Un ejemplo de implementación del primer caso lo constituye el método de la ruleta, en el que a cada individuo se le asigna una ``caja'' cuyo tamaño es proporcional a su \textit{fitness}, de modo que al lanzar la bola de la ruleta haya más probabilidades de que caiga en la caja de los individuo con mayor fitness. Un ejemplo de método ordinal es la selección por torneo, en el que se selecciona un número arbitrario de candidatos (mínimamente dos), con o sin reemplazo, y el que tiene el mayor \textit{fitness} gana el torneo y es seleccionado como padre para la próxima generación. El torneo debe repetirse hasta seleccionar la cantidad  de padres necesaria.

La operación de recombinación o \textit{crossover} consiste en mezclar los genes de los cromosomas de dos padres. Típicamente, esto se logra intercambiando tramos de genes de los padres para formar a los hijos de la nueva generación, a través del ``\textit{crossover} de uno o dos puntos'', en los que el o los puntos de corte se eligen aleatoriamente a lo largo del cromosoma; o del ``\textit{crossover} uniforme'', en el que se genera una máscara binaria y se intercambian los genes a los que les corresponde el valor \mip{1} en la posición de la máscara. Los siguientes esqueman ejemplifican estas variantes del operador de \textit{crossover}.

\begin{center}
\textit{Crossover} de un punto: $\left\lbrace
\begin{tabular} {c @{ } c @{ } c}
Padres & & Hijos \\
  \color{red}{$a_1a_2a_3a_4|a_5a_6a_7$} &   & {\color{red}$a_1a_2a_3a_4$}$b_5b_6b_7$ \\ 
  & $ \Longrightarrow $ \\
  $b_1b_2b_3b_4|b_5b_6b_7$  &   & $b_1b_2b_3b_4$\color{red}{$a_5a_6a_7$} \\
\end{tabular} \right. $
\end{center}
 
\begin{center}
 \textit{Crossover} de dos puntos: $\left\lbrace
\begin{tabular} {c @{ } c @{  } c}
Padres & & Hijos \\
  \color{red}{$a_1a_2|a_3a_4|a_5a_6a_7$} &   & {\color{red}{$a_1a_2$}}$b_3b_4${\color{red}$a_5a_6a_7$} \\ 
  & $ \Longrightarrow $ \\
  $b_1b_2|b_3b_4|b_5b_6b_7$  &   & $b_1b_2${\color{red}$a_3a_4$}$b_5b_6b_7$ \\
\end{tabular} \right.$
\end{center}

\begin{center}
 \textit{Crossover} uniforme: $\left\lbrace 
\begin{tabular} {c @{ } c @{  } c}
Padres & & Hijos \\
   0\phantom{$_1$}1\phantom{$_1$}1\phantom{$_1$}0\phantom{$_1$}1\phantom{$_1$}0\phantom{$_1$}1\phantom{$_1$} & $\longleftarrow$ & \textit{Máscara} \\
  {\color{red}$a_1a_2a_3a_4a_5a_6a_7$} &   & {\color{red}$a_1$}$b_2b_3${\color{red}$a_4$}$b_5${\color{red}$a_6$}$b_7$ \\ 
  & $ \Longrightarrow $ \\
  $b_1b_2b_3b_4b_5b_6b_7$  &   & $b_1${\color{red}$a_2a_3$}$b_4${\color{red}$a_5$}$b_6${\color{red}$a_7$} \\
\end{tabular} \right.$
\end{center}

También existen diferentes maneras de implementar el operador de mutación. Tal vez la más simple consiste en reemplazar aleatoriamente un gen del cromosoma por un valor arbitrario. Preferentemente este valor debe estar dentro del dominio de búsqueda de la variable asociada al gen por medio del procedimiento de codificación. Si no es el caso, es posible obtener cromosomas que no representen una solución factible. Un método más sofisticado consiste en reemplazar un gen aleatorio por un valor obtenido mediante alguna distribución de probabilidad alrededor del valor del gen que estamos reemplazando. Una representación de este mecanismo se muestra en el siguiente esquema:

\begin{center}
Mutación: $\left\{
\begin{tabular} {c @{ } c @{  } c}
Antes & & Después \\
  $a_1a_2a_3a_4a_5a_6a_7$ & $ \Longrightarrow $ & $a_1a_2a_3${\color{red}$b_4$}$a_5a_6a_7$ \\ 
\end{tabular} 
\right.$
\end{center}

Por último, también existen variantes para el operador de reemplazo. El más simple consiste en reemplazar completamente la población actual con la nueva población de hijos. Esto es sencillo de realizar, pero puede suceder que en la nueva generación los individuos no tengan \textit{fitness} tan buenos como el de algunos padres. Una forma de sortear esta posibilidad es implementar ``elitismo'', que consiste en reemplazar solamente una porción $r$ de los individuos originales, dejando sobrevivir en la nueva generación la porción $1 - r$ de los que tienen mejor \textit{fitness}. Es usual que a través de la evolución de la población se mantenga constante el número de individuos que la componen.

Implementaremos ahora un AG básico para resolver un problema que, según su tamaño (es decir, el número de variables de decisión), puede ser muy difícil de abordar. El ``problema de la mochila'' se puede enunciar de la siguiente forma:
\begin{quote}
 Dado un conjunto de ítems, cada uno con un peso y un valor, determinar qué ítems incluir en una colección de forma tal que el peso total sea menor o igual a la capacidad máxima de la mochila, maximizando el valor de los objetos incluidos en la colección.\footnote{Recomendamos la lectura de \fullcite{kellerer2004}.}
\end{quote}

Formalmente, si tenemos $n$ ítems cuyos valores son $v_i$ y sus pesos $p_i$ ($0 \leq i \leq n$), con un valor máximo $P$ de carga, el problema consiste en:

\begin{align}
 \begin{split}
 \text{maximizar:}& \quad f(\bm{x}) = \sum_{i=1}^n v_i \, x_i \\
 \text{tal que:}& \quad \sum_{i=1}^{n} p_i \, x_i \leq P \\
 \text{y} & \quad  0 \leq x_i \leq q_i, \text{ para } 1 \leq i \leq n 
 \end{split}
 \label{eq:opt05}
\end{align}

Si $q_i = 1$ para cada ítem, se trata del problema de la mochila 0-1. Si uno o más $q_i$ es infinito se trata del problema de la mochila no acotado, mientras que en otro caso se llama problema de la mochila acotado. Abordaremos el caso en el que $x_i \in \{0, 1\}$.

El código a continuación contiene dos clases, que permiten representar objetos de tipo \mip{Individual} y \mip{Population}, siendo este último el objeto que contiene una lista de individuos y métodos que implementan los operadores del AG. Comenzamos describiendo la clase \mip{Individuo}

\pyfile[firstline=1, lastline=33]{Chapters/optimizacion/code/ag.py}

En la clase \mip{Individual} comenzamos estableciendo una tupla que indica el conjunto de genes con el que codificaremos el cromosoma (nuestro alfabeto), que está limitado a \mip{0} y \mip{1}. También indicamos cuál es la capacidad de carga de la mochila, determinado su valor en la variable \mip{max_weight}. 

Para la inicialización de los objetos necesitamos pasar como argumentos dos listas: \mip{elements}, y \mip{chromosome}. La primera contiene tuplas de pares \mip{(valor, peso)} que son necesarios para la evaluación del \textit{fitness}, mientras que la segunda lista permite inicializar un individuo con un cromosoma dado. En caso que no suministremos un cromosoma, creamos uno seleccionando al azar elementos de \mip{self.genes}, la cantidad de veces necesaria para completar el tamaño del cromosoma, que queda determinado por la longitud de la lista de los valores.

El método \mip{evaluate} calcula el \textit{fitness} del individuo según \eqref{eq:opt05}, y también guarda el valor del peso total de los objetos que incluye el cromosoma. Nótese que si el peso del individuo es mayor que la capacidad de carga, asignamos un fitness de \mip{-1}, indicando de este modo que el individuo no es apto como solución del problema ya que no será favorable un cromosoma que represente un valor negativo.

\pyfile[firstline=35, lastline=51]{Chapters/optimizacion/code/ag.py}

El método \mip{mutate} implementa el operador de mutación. Simplemente, determina un gen aleatorio dentro del cromosoma y le asigna una elección al azar de \mip{self.genes}. En este ejemplo, tenemos igual probabilidad de cambiar el valor del gen o dejarlo como estaba. También hacemos \mip{None} los valores de \mip{self.fitness} y \mip{self.weight}, ya que ante un eventual cambio en el cromosoma es necesario evaluar nuevamente estas cantidades. Finalmente, los dos métodos siguientes permiten comparar individuos según su \textit{fitness} (\mip{__lt__}), evaluando sus \textit{fitness} de ser necesario, y realizar una representación prolija de un individuo (\mip{__str__}).

Describiremos ahora la clase \mip{Population}. En su inicialización pasamos como argumentos (con valores por defecto) los parámetros que controlan la ejecución del algoritmo y los correspondientes operadores genéticos, así como una lista con los pares \mip{(valor, peso)} de cada ítem a considerar durante el proceso de optimización. Para generar la población inicial, construimos una lista de individuos cuyos genes se seleccionan aleatoriamente, hasta obtener el número de elementos establecidos en \mip{size}

\pyfile[firstline=54, lastline=79]{Chapters/optimizacion/code/ag.py}


Dado que ordenaremos la población desde el mejor individuo en orden decreciente del \textit{fitness}, el método \mip{best} nos permite obtener la mejor solución de la generación actual de la población:

\pyfile[firstline=81, lastline=83]{Chapters/optimizacion/code/ag.py}

Para evolucionar la población utilizamos el método \mip{run} que, mientras no se alcance una situación de finalización, invoca al método \mip{step}. Las condiciones de finalización para este ejemplo son alcanzar el número máximo de iteraciones o el valor óptimo del \textit{fitness}.

\pyfile[firstline=85, lastline=90]{Chapters/optimizacion/code/ag.py}

El método \mip{step} mencionado arriba consiste en aplicar cada uno de los operadores genéticos selección, \textit{crossover}, mutación, y reemplazo de la población en orden secuencial, e incrementar en uno la variable \mip{self.generation}:

\pyfile[firstline=92, lastline=109]{Chapters/optimizacion/code/ag.py}

El operador de selección, implementado en el método \mip{selection}, devuelve una lista de padres seleccionados mediante un torneo, conteniendo la misma cantidad de individuos que la población inicial. Es posible que los individuos con mejor \textit{fitness} sean seleccionados múltiples veces en la lista de padres, pero el torneo permite, en forma aleatoria y con baja probabilidad, que cada tanto un individuo con un \textit{fitness} no muy bueno integre la selección de padres, agregando de esta forma diversidad genética en la población. El torneo implementado en este ejemplo consiste en seleccionar aleatoriamente \mip{size} individuos de la población (por defecto, \mip{size} vale tres), y con una probabilidad definida en la variable \mip{goliat} (con un valor por defecto del 90\%) gana el torneo el individuo con mayor \textit{fitness}, y con una probabilidad de $(1 - $\mip{goliat}$)$ es seleccionado alguno de los restantes participantes del torneo.

El método \mip{do_crossover} implementa el operador de recombinación de dos puntos. Comienza generando una lista vacía de hijos que se irá completando hasta alcanzar la cantidad de individuos originales en la población. El método selecciona al azar dos padres de la lista que constituye el argumento de \mip{do_crossover} (generada por \mip{selection}), y decide si los recombina con una probabilidad \mip{self.crossover_p} (con un valor por defecto de \mip{0.9}), o si incluye en la lista de hijos los cromosomas de los padres sin recombinar. En el caso en que efectivamente se produzca la recombinación, se seleccionan al azar dos puntos en la cadena de cromosomas de los padres y se intercambia el segmento medio de genes entre estos puntos, dando origen a los nuevos cromosomas de los hijos (dos padres generan dos hijos).

\pyfile[firstline=111, lastline=134]{Chapters/optimizacion/code/ag.py}

El operador de mutación se implementa en el método \mip{do_mutation} de \mip{Poblacion}. Simplemente, un bucle recorre la lista de hijos, y con una probabilidad dada por \mip{self.mutation_p} decide mutar o no cada uno de ellos:

\pyfile[firstline=136, lastline=140]{Chapters/optimizacion/code/ag.py}

Finalmente, el método \mip{do_new_population} genera la nueva población implementando elitismo, esto es, determina qué fracción de la población debe ser reemplazada por los hijos en la generación siguiente. Mantiene entonces a los mejores padres y a los mejores hijos según sus \textit{fitness}.

\pyfile[firstline=142, lastline=147]{Chapters/optimizacion/code/ag.py}

Utilizamos ahora nuestro AG para resolver el ejemplo 2.1 del libro de Martello y Toth \cite{martello1990}, que consiste en un conjunto de 8 elementos cuyos valores y pesos están definidos en la lista \mip{elements} del código a continuación, y que tiene como solución óptima $x = (1, 1, 1, 1, 0, 1, 0, 0)$ con valor $280$:

\pyfile[firstline=150, lastline=168]{Chapters/optimizacion/code/ag.py}

Al ejecutar este código obtenemos:
\begin{shell}
$ ./ag.py 
Mejor individuo:
[1, 1, 1, 1, 0, 1, 0, 0]
Valor: 280.0
Peso: 102.0
\end{shell}

Dado el carácter estocástico del AG, este método de optimización no garantiza que se obtenga el valor óptimo global del problema. Es posible que si ejecutamos nuevamente el programa, con otra semilla para el generador de números aleatorios, obtengamos otro resultado:

\begin{shell}
$ ./ag.py 
Mejor individuo:
[1, 1, 1, 0, 1, 0, 0, 1]
Valor: 246.0
Peso: 92.0
\end{shell}

Este efecto se vuelve más crítico cuanto mayor sea la dimensionalidad del problema, por lo que es recomendable realizar múltiples ejecuciones para observar la distribución de los resultados. Por otra parte, para cada problema constituye una cuestión casi artesanal la determinación de los parámetros de ejecución tales como el tamaño de la población, las probabilidades de recombinación y mutación, y la tasa de reemplazo. Cuando el problema es complejo, es conveniente también incorporar otras representaciones de los operadores genéticos. La experiencia en el uso de esta técnica resulta una buena guía para generar el algoritmo genético más eficiente para cada problema.



\section{Lecturas recomendadas}
\begin{itemize}
 \item Un muy buen libro de optimización convexa es el de \fullcite{boyd2004}, que se puede descargar desde \url{https://web.stanford.edu/~boyd/cvxbook/}.
 \item \fullcite{winston2004}
 \item \fullcite{nocedal2006}. Una descripción muy completa de una amplia variedad de métodos de optimización.
 \item El libro clásico de \fullcite{goldberg1989}.
 \item Una excelente introducción a los algoritmos genéticos: \fullcite{mitchell1996}.
 \item El libro de \fullcite{kellerer2004}, presenta con mucho detalle las diversas variantes del problema de la mochila y numerosas técnicas de optimización.
\end{itemize}

