
% Copyright 2020-2024 Facundo Batista y Manuel Carlevaro
% Licencia CC BY-NC-SA 4.0
% Para más info visitar https://github.com/facundobatista/libro-pyciencia/

 \chapter{Manipulación de datos} \label{ch:datos}

\begin{wraptable}{r}{5cm}
\begin{modulesinfo}
\begin{center}
{\small
    \begin{tabular}{l r}
        \toprule
        \textbf{Módulo} & \textbf{Versión} \\
        \midrule
        h5py & 3.11.0 \\
        netCDF4 & 1.7.1 \\
        numpy & 1.26.4 \\
        pandas[all] & 2.2.2 \\
        requests & 2.32.3 \\
        \bottomrule
    \end{tabular}
    \vspace{0.75em}

    \href{https://github.com/facundobatista/libro-pyciencia/tree/master/código/manipulacion_datos/}{Código disponible}
}
\end{center}
\end{modulesinfo}
\end{wraptable}

En este capítulo hablaremos en cómo manipular datos, independientemente del procesamiento de los mismos. Nuestro foco estará en cómo cargar o leer datos y herramientas para procesarlos, no en las operaciones que se realizarán sobre esos datos. En otras palabras y a modo de ejemplo, veremos cómo cargar una lista de datos en memoria como para calcular su promedio, pero no hablaremos del cálculo que se necesita para obtener el promedio.

La fuente natural de datos son archivos si los datos ya están generados en una etapa previa, o algún dispositivo de entrada si los datos se están generando en el momento de forma continua. Notablemente, cuando queremos descargar datos de internet nos podemos encontrar con la misma situación: si los datos son pocos los podemos leer y procesar todos en la misma acción, o también podemos irlos bajando y procesando de a poco.

En cualquier caso los datos pueden estar en forma de ``texto'' o ser ``binarios''. En general (aunque no siempre) si los datos están en formato de texto estarán pensados para ser consumidos línea a línea, mientras que los datos binarios se procesarán byte a byte. Por razones pedagógicas nos enfocaremos en ambos casos por separado.


\section{Leyendo archivos}\label{sec:archivos}

Usamos el concepto ``archivo'' de forma muy genérica y al tiempo de forma muy poderosa, porque los sistemas informáticos utilizan ese concepto para proveer información desde varias fuentes. Entonces, trabajaremos como si fuesen archivos no sólo a archivos reales que tenemos en el dispositivo de almacenamiento de la computadora sino también a otras representaciones como dispositivos de entrada/salida de la computadora o respuestas de pedidos por la red.

En particular está el concepto de \textit{file-like object}, ``objetos que se parecen a archivos'', que son objetos que podemos tener como resultados de operaciones en nuestro programa y que por simplicidad pueden tratarse como archivos aunque realmente no lo sean.

La forma más simple de abrir un archivo es utilizando la función integrada \texttt{open}. Sus parámetros más comunes son: el nombre del archivo, el modo, y la codificación.

El nombre del archivo (o el \textit{path} completo) apunta al archivo en disco, que puede existir o no según cómo vayamos a abrirlo.

El modo es justamente la forma en que vamos a abrir el archivo; es una cadena de varias letras cada una con un significado específico que se pueden combinar de algunas maneras puntuales.

\begin{itemize}
    \item \texttt{'r'}: abre para solamente leer
    \item \texttt{'w'}: abre para solamente escribir, truncando el archivo a cero si es que ya existía
    \item \texttt{'x'}: abre para solamente escribir, pero exclusivamente creando el archivo (falla si el archivo ya existía)
    \item \texttt{'a'}: abre para solamente escribir, agregando el nuevo contenido al final
    \item \texttt{'b'}: modo binario
    \item \texttt{'t'}: modo texto
    \item \texttt{'+'}: abre para actualizar (leer y escribir)
\end{itemize}

En general construimos el modo indicando lectura/escritura y binario/texto, por ejemplo \texttt{'wb'} para escribir un archivo en binario, \texttt{'rt'} para leer un archivo en texto, etc. El \texttt{'+'} se usa relativamente poco, es para aquellos casos en que queremos escribir y leer al mismo tiempo, y se combina con las otras letras de lectura/escritura más para indicar qué suecedería con un archivo que ya existe o no.

\jupynotex[1-2]{Chapters/manipulacion_datos/code/archivos.ipynb}

En realidad los archivos siempre son binarios; sólo pueden tener bytes dentro porque los medios de almacenamiento guardan bytes. Abrir el archivo en modo ``texto'' es solamente una forma cómoda de trabajar con texto Unicode en nuestros programas sin pasar por una etapa donde el contenido está en binario: entonces en el programa siempre manejaremos texto y la conversión a o desde bytes la hará Python automáticamente, y es por eso que en estos casos hay que indicarle la codificación que usará para codificar o decodificar (en realidad es opcional, pero el default depende del sistema, entonces siempre es recomendable que esté indicado de forma explícita).

\jupynotex[3-5]{Chapters/manipulacion_datos/code/archivos.ipynb}

Hay algunas opciones más para lidiar con distintas especificidades (que surgen especialmente en distintas plataformas) pero ya con estas podemos comenzar a trabajar. Para profundizar lo mejor es \href{https://docs.python.org/es/dev/library/functions.html#open}{su documentación}.

La metodología de abrir y cerrar el archivo como mostramos antes es muy útil cuando el archivo se utiliza por tiempo indefinido o estructuramos algún objeto alrededor de su uso y los dos ciclos de vida están atados. Pero en general se usa otra técnica, ya que es muy común cerrar el archivo lo antes posible en el mismo bloque de código que se abrió; recordemos que mientras el archivo está abierto se está utilizando un recurso del sistema operativo, que hay límites con respecto a la cantidad de archivos abiertos, etc., por lo que siempre conviene prestar atención en cerrar el archivo lo antes posible cuando ya no lo utilizamos más.

Para simplificar eso Python nos permite usar un administrador de contexto \ref{sec:contextmanagers} para interactuar con archivos:

\jupynotex[6]{Chapters/manipulacion_datos/code/archivos.ipynb}

Vemos como usando el \mip{with} tenemos al \textit{file handler} abierto dentro del contexto y automáticamente cerrado luego de salir del mismo: de esta manera es clara la zona donde trabajamos con el archivo, con la seguridad que siempre se cerrará al archivo al salir de esa zona (incluso en el caso de una excepción, lo cual no sucedía en los ejemplos anteriores).


\section{Trabajando con datos en formato textual}

Cuando mencionamos archivos de forma genérica en \ref{sec:archivos} vimos la operatoria básica: abrir, cerrar, escribir, leer; entraremos en esta sección en detalles particulares al trabajar con texto.

Habiendo dicho eso, volveremos a mostrar aquí cómo escribir texto en un archivo aunque sea similar a lo que ya vimos. En el siguiente ejemplo bajamos un texto que tenemos como lista de cadenas a un archivo (respetando las líneas, para lo cual tenemos que agregar manualmente los saltos de línea).

\jupynotex[1]{Chapters/manipulacion_datos/code/textos.ipynb}

Para leer, por otro lado, aprovechamos la característica de que el \textit{file handler} que devuelve \mip{open} es iterable y nos devolverá línea por línea del archivo, lo cual es al mismo tiempo la forma más práctica y más eficiente de recorrer las líneas de un archivo:

\jupynotex[2]{Chapters/manipulacion_datos/code/textos.ipynb}

Hay otros métodos para trabajar con el \textit{file handler}, por ejemplo \texttt{.readlines} que leerá todas las lineas del archivo almacenándolas en una lista, pero en general siempre será mejor leer linea por linea y en todo caso armar una lista con el resultado de un primer procesamiento de eso archivo.

Por supuesto, cuando trabajamos con texto también podemos leer carácter a carácter (similar a como vamos a ver cuando trabajemos con binarios, más adelante), buscando el salto de línea y armando las líneas a mano, pero en la casi absoluta cantidad de veces es suficiente la manera ya vista.

La iteración de cada línea ni siquiera tiene que ser explícita, ya que muchas funciones de Python trabajan iterando naturalmente. En el siguiente ejemplo vemos una simple forma de obtener la línea más larga del archivo que recién creamos, usando \mip{max} y aprovechando que se le puede indicar cual es indicador de orden (en nuestro caso el largo de cada línea, que es lo que devolverá la función \mip{len} en cada caso):

\jupynotex[3]{Chapters/manipulacion_datos/code/textos.ipynb}

Veamos un ejemplo más complejo: sacar el top 5 de mantenedores de paquetes de Debian. La fuente de datos será un archivo público, \href{http://ftp.uk.debian.org/debian/indices/Maintainers}{la lista de \textit{maintainers}}, que en cada línea indica el paquete, uno o más espacios, y el nombre y mail de su mantenedor (para más detalles de la estructura que procesa el código ver la descripción en el Jupyter Notebook \texttt{textos.ipynb} de este capítulo).

\jupynotex[4]{Chapters/manipulacion_datos/code/textos.ipynb}

Por supuesto, la estructura que acabamos de procesar es muy específica. Hay muchos formatos textuales que son más estándar, veamos algunos casos.


\subsection{Tres formatos textuales muy comunes}

Mencionaremos particularmente tres formatos que se utilizan mucho en los sistemas informáticos, CSV, JSON y XML.

\subsubsection{CSV}
\label{subs:datosCSV}

Los archivos CSV, por \textit{Comma-Separated Values}, o ``valores separados por coma'' es un tipo de archivo muy utilizado para guardar datos en forma de tabla.

Aunque no tiene prácticamente soporte para manejar distintos tipos de datos (la conversión a Unicode de los textos dependerá de cómo abramos el archivo, y sólo convertirá algunos campos numéricos en algún caso, como veremos luego), pero su simplicidad y facilidad de uso hizo que sea uno de los formatos más utilizados desde siempre.

Su estructura, a priori, es muy sencilla: cada línea es una fila, y cada columna se separa de la otra por comas. Pero, en la práctica, es todo mucho más complicado, porque no hay un estándar fuerte sobre distintos aspectos como qué codificación se usa para guardar textos Unicode, cómo separar las columnas cuando el punto decimal en los números es la coma, o cuando los textos usan comillas o incluso si tienen saltos de línea dentro.

Entonces siempre recomendamos evitar caer en la tentación y procesar un archivo CSV ``a mano'' (leyendo línea por línea asumiendo que sólo hay saltos de línea al final de cada fila y directamente separar por comas para obtener las columnas). El procedimiento correcto es utilizar el módulo \texttt{csv} provisto por la Biblioteca Estándar de Python.

Veamos esto con un ejemplo. Construyamos un caso particularmente complicado donde tenemos cadenas Unicode, un texto vacío, y otros que tienen caracteres que a priori serían ``de control'' en el formato: comillas dobles y coma (y tilde y punto y coma, por las dudas), y un salto de línea.

\jupynotex[5]{Chapters/manipulacion_datos/code/textos.ipynb}

Grabamos el archivo en formato CSV usando el módulo ya mencionado para asegurarnos que el formato final sea el correcto, y luego lo abrimos y mostramos sin procesar. Allí vemos que aunque grabamos dos filas, tenemos efectivamente tres líneas y otros detalles que nos complicarían el procesamiento ``a mano''. Veamos entonces qué sucede si lo procesamos ``mal'':

\jupynotex[6]{Chapters/manipulacion_datos/code/textos.ipynb}

Por supuesto, si utilizamos el módulo \texttt{csv} para procesar el archivo volvemos a tener cada uno de los textos de forma correcta:

\jupynotex[7]{Chapters/manipulacion_datos/code/textos.ipynb}

Decíamos arriba que los archivos CSV pueden tener distintos formatos. El módulo \texttt{csv} trae soporte para interpretar los ``dialectos'' más comunes (\texttt{excel}, \texttt{unix}, etc.), y de esta manera podemos tener control sobre qué se usa para separar columnas, con qué se envuelven los campos que tienen caracteres especiales, cómo se escapan a su vez esos caracteres, el fin de línea, etc.

Podemos elegir cuál dialecto preferimos, armar uno nosotros, o incluso pasar modificadores al escribir o leer el CSV para afectar puntualmente algún comportamiento. En el siguiente ejemplo indicamos que al escribir se agreguen comillas alrededor de todos los campos no numéricos, entonces luego al leer el resto de los campos serán convertidos a \mip{float} directamente:

\jupynotex[8]{Chapters/manipulacion_datos/code/textos.ipynb}

Hasta este momento estamos mostrando fila por fila de datos sin acceder a una columna en particular. Habrán notado que cada fila termina siendo una lista, por lo que para acceder a cada dato debemos hacerlo posicionalmente:

\jupynotex[9]{Chapters/manipulacion_datos/code/textos.ipynb}

Podemos mejorar la legibilidad de nuestro código si en vez de usar el \textit{reader} común usamos \texttt{DictReader}, al cual le pasamos el nombre de los campos y en vez de darnos una tupla nos dará un diccionario:

\jupynotex[10]{Chapters/manipulacion_datos/code/textos.ipynb}

Incluso \texttt{DictReader} soporta tomar el nombre de las columnas de la primer fila del archivo; lo hace automáticamente si evitamos pasarle el parámetro \texttt{fieldnames}.


\subsubsection{JSON}

La ubicuidad del lenguaje JavaScript en la gran mayoría de navegadores web hizo que un formato pensado para ser evaluado de forma sencilla por ese lenguaje se hiciera moneda común a la hora de intercambiar información entre distintas partes.

Este formato es un subconjunto de la notación de JavaScript, llamado JSON por \textit{JavaScript Object Notation}, ``notación de objeto de JavaScript'', y es a la vez flexible (porque soporta los tipos de datos más utilizados por los distintos lenguajes) y simple (porque es una cadena de caracteres mayormente legible y también soporta solamente unos pocos tipos de datos), lo cual impactó fuertemente en su interoperabilidad y por lo tanto en su adopción en el tiempo.

Los siguientes son los tipos de datos soportados por JSON, con la aclaración de a qué tipo corresponden en Python:

\begin{itemize}
    \item Números enteros (\mip{int}) y con parte fraccional separada por punto (\mip{float}), en ambos casos positivos y negativos
    \item Cadenas de texto Unicode (\mip{str})
    \item Booleanos (\mip{True} y \mip{False})
    \item Null (\mip{None})
    \item Arreglos (o \textit{arrays}) con cualquier otro tipo de dato soportado dentro (\mip{list})
    \item Objetos o colecciones no ordenadas de pares clave-valor, donde la clave tiene que ser una cadena sí o sí y el valor puede ser cualquier tipo (\mip{dict}, con el detalle de restricción del tipo de la clave)
\end{itemize}

Veamos algunos ejemplos codificando estructuras de Python al formato JSON:

\jupynotex[11]{Chapters/manipulacion_datos/code/textos.ipynb}

Podemos ver en el ejemplo algunos detalles que vale la pena mencionar. Las cadenas siempre se rodean con comilla doble (cuando la cadena en sí tiene comilla doble se escapa con barra invertida) y cuando tienen caracteres no-ASCII los mismos se codifican a UTF-16 y se incluyen los valores escapados de los bytes. \mip{True}, \mip{False} y \mip{None} se escriben casi igual en JSON, lo que sumado a muchas otras similitudes hacen que sea un formato muy legible si estamos acostumbrados a Python.

Por último, tanto la tupla como la lista se transforman en el mismo tipo de \textit{array} (que usa como delimitador los mismos corchetes que Python para las listas). Un efecto secundario de esto es que podemos inadvertidamente ``perder'' las tuplas al pasar por JSON:

\jupynotex[12]{Chapters/manipulacion_datos/code/textos.ipynb}

Hasta ahora convertimos de Python a JSON como cadena de texto (y volvimos de esa cadena de texto a Python) usando las funciones \texttt{dumps} y \texttt{loads} del módulo \texttt{json}. La mayoría de las veces no nos interesa obtener esa cadena \textit{per se} sino que su destino final es un archivo o enviarla a través de la red; en esos casos podemos  grabar o leer directamente de un archivo (o de un objeto \textit{file like}) con las funciones \texttt{dump} y \texttt{load}.

\jupynotex[13]{Chapters/manipulacion_datos/code/textos.ipynb}

Tambien podemos trabajar con otros objetos que no sean realmente archivos pero que expongan la misma interfaz (los llamamos \textit{file-like objects}). Un caso típico es la respuesta de un request HTTP, que expone el método \texttt{read} sobre el cual podemos operar.

En el siguiente ejemplo usamos la biblioteca \texttt{requests} para obtener la temperatura actual del Museo de Ciencias Naturales de La Plata, Buenos Aires, Argentina:

\jupynotex[14]{Chapters/manipulacion_datos/code/textos.ipynb}

Cuando hacemos el \texttt{get} con \texttt{stream=True} la biblioteca no va a leer toda la respuesta inmediatamente, solamente leerá las cabeceras, y vemos también que tenemos el atributo \texttt{raw} que es  el objeto-respuesta a más bajo nivel, la cual usamos luego para leer directamente decodificando el JSON de la respuesta:

\jupynotex[15]{Chapters/manipulacion_datos/code/textos.ipynb}

JSON soporta todos los tipos de datos que mencionamos antes pero si lo usamos con continuidad notaremos que siempre en algún momento necesitamos enviar en este formato algún tipo de dato no soportado. Incluso en el ejemplo que acabamos de ver tenemos un caso de esto: la ``fecha hora'' es una cadena y no un objeto \texttt{datetime}.

Para este ejemplo alcanza con convertir esa cadena al objeto correspondiente, pero si va a ser recurrente podemos armar un \textit{decoder} específico para nuestras necesidades:

\jupynotex[16]{Chapters/manipulacion_datos/code/textos.ipynb}

Vemos como podemos indicar una función al momento de decodificar que recibirá todos los ``objetos'' a nivel JSON (que en Python conceptualizamos como diccionarios). No hay manera de recibir las cadenas directamente, pero con recibir todo el objeto nos alcanza, podemos iterar las claves y valores del mismo y convertir a fecha-hora lo que se pueda.

Para intercambios de datos más complejos y estructurados es normal utilizar claves con nombres particulares que indican el tipo de objeto que está representado en ese JSON e instanciar un objeto Python de ese tipo con esos atributos.


\subsubsection{XML}

El XML, por sus siglas en inglés \textit{eXtensible Markup Language} (``lenguaje de marcado extensible'') nos permite codificar datos de una manera extremadamente flexible y compartible, ya que se pueden tener reglas para cualquier tipo de dato y al mismo tiempo no está atado a ningún lenguaje de programación o plataforma.

Veremos que su estructura es parecida a la del mucho más conocido HTML. Ambos provienen de un lenguaje adoptado por la ISO llamado SGML (\textit{Standard Generalized Markup Language}, una evolución del GML creado por IBM en la década de 1970).

En comparación con los formatos que vimos anteriormente, XML es por lejos el más flexible, ya que permite codificar no sólo la información que necesitamos almacenar o transmitir sino tambien todo tipo de metadata necesaria para interpretar esa información. Esta flexibilidad tiene el costo asociado de la complejidad del formato y su muy baja legibilidad.

Su estructura se basa en una estructura de árbol donde cada nodo puede tener un dato u otros nodos, además de ``marcas'' que proveen información extra sobre esos datos o nodos.

Para visualizar más fácil esto armemos en formato XML lo que podría ser la respuesta del servicio que usamos antes para saber el clima de una ciudad (escribiendo sólo la parte que usamos de clima actual, por brevedad):

\begin{verbatim}
<?xml version="1.0"?>
<data>
    <current_weather>
        <temperature unit="celsius">20.3</temperature>
        <windspeed unit="knots">10.7</windspeed>
        <winddirection>132.0</winddirection>
        <weathercode>3</weathercode>
        <is_day>1</is_day>
        <time format="ISO">2023-04-22T15:00</time>
    </current_weather>
    <etc>
        ...
    </etc>
</data>
\end{verbatim}

Allí vemos la estructura de árbol que mencionamos, así como también información extra asociada a cada dato, útil tanto para parsear el texto del dato (como en el caso de la fecha-hora) como para también potencialmente mostrarle esa información al usuario (como la unidad de temperatura).

Veamos un caso más real: el XML resultante al exportar la página de Argentina de Wikipedia en castellano:

\jupynotex[17]{Chapters/manipulacion_datos/code/textos.ipynb}

No la mostramos toda (¡117 mil bytes!) pero usemos el módulo \texttt{textwrap} para ver de forma sencilla y prolija la primera parte del contenido:

\jupynotex[18]{Chapters/manipulacion_datos/code/textos.ipynb}

\begin{info}
    Una buena manera de ver un XML completo bien estructurado y con la facilidad extra de ocultar/mostrar distintas partes del árbol es abriendo ese XML en un navegador web; prueben esto abriendo la URL del XML que acabamos de descargar.
\end{info}

Pasemos a procesar este XML.

En Python tenemos distintas herramientas para trabajar con este formato. Veamos los distintos casos todos sobre el mismo contenido que acabamos de descargar, siempre con el objetivo de encontrar el título de la página, el identificador y timestamp de esta revisión en particular, y el largo del texto en sí de la página.

La primer biblioteca que usaremos es ElementTree, que nos da una forma simple y eficiente para parsear XMLs, ya que mapea directamente el concepto de ``árbol de nodos'' de este formato con dos tipos de objetos: el \texttt{ElementTree} que representa el documento XML completo, y \texttt{Element} que nos permite trabajar con cada nodo.

\jupynotex[19]{Chapters/manipulacion_datos/code/textos.ipynb}

Como ya tenemos el XML completo en una cadena lo parseamos de allí (\texttt{ElementTree}, como el resto de las bibliotecas que veremos aquí nos permite tanto leer el contenido desde una cadena como de un archivo). Nos devuelve directamente el primer nodo del documento, que cómo es una estructura de árbol tendemos a llamar ``raiz'', pero atención que en este caso tenemos más de un nodo en el primer nivel.

Vemos que su tag es \texttt{\{http://www.mediawiki.org/xml/export-0.10/\}mediawiki}, no simplemente \texttt{mediawiki}. Esto es porque ElementTree nos muestra siempre (y obliga a trabajar) cada nombre en su espacio de nombre. Los espacios de nombre son para identificar unívocamente a qué documento pertenece un nombre; sería muy util si mezclamos este XML con otro documento que también tiene los tags \texttt{mediawiki} o \texttt{page}, por ejemplo, pero para manejar un sólo documento puede ser un poco engorroso.

Más allá de eso, ElementTree nos deja acceder de forma bastante directa a toda la información del nodo: tenemos sus atributos en \texttt{.attrib}, el texto en \texttt{.text}, y lo iteramos directamente para ir a por los hijos. Y también tenemos el método \texttt{.find} para buscar un nodo hijo en particular, usémoslo para recorrer el árbol y obtener la información que queríamos:

\jupynotex[20]{Chapters/manipulacion_datos/code/textos.ipynb}

La segunda herramienta es muy parecida a la anterior pero su API es estándar, definida por la \textit{World Wide Web Consortium (W3C)}. De allí viene el nombre del módulo, DOM, por \textit{Document Object Model}.

Cuando parseamos el XML nos devuelve un ``documento'', al cual le pedimos el primer nodo hijo:

\jupynotex[21]{Chapters/manipulacion_datos/code/textos.ipynb}

Si comparamos los atributos de este nodo con lo que teníamos en ElementTree vemos que esta otra biblioteca no nos está manejando los espacios de nombres: los tenemos sin expandir en los atributos del primer nodo y no está incluido en el tag.

Al iterar un nodo del DOM obtenemos tanto los nodos hijos como otros ``pseudo nodos'' de tipo \texttt{Text}, lo cual es un poco confuso de trabajar. Este detalle es aún más notorio cuando queremos obtener la información necesaria del XML, donde aparece incluso cuando queremos el largo del texto: en el atributo \texttt{bytes} del nodo \texttt{text} para sacar el valor debemos obtener una especie de nodo hijo y acceder a sus datos:

\jupynotex[22]{Chapters/manipulacion_datos/code/textos.ipynb}

En este caso de uso es explícito que cuando buscamos cada nodo hijo (por ejemplo \texttt{page}) realmente nos estamos quedando con el primero, potencialmente podría haber más. Es lo mismo que pasaba con ElementTree, donde usábamos el método \texttt{.find} que también nos trae el primero que encuentre.

Finalmente, la tercer biblioteca nos provee un conjunto de herramientas que implementan una forma de acceder a la información de cada nodo al parsear el XML: la \textit{Simple API for XML (SAX)}. En este caso no tendremos toda la estructura del XML como un árbol sino que tendremos que implementar nuestros propios mecanismos para acceder a la información correspondiente.

La forma más fácil de lograr esto es heredar del \texttt{ContentHandler} que trae el módulo por default e implementar tres métodos que se irán llamando automáticamente al parsear el XML: \texttt{startElement} para cuando arranca un elemento, \texttt{endElement} para cuando termina, y \texttt{characters} para el texto de cada nodo.

En la siguiente celda vemos como armamos nuestra clase siguiendo ese esquema, y luego usamos esa clase para parsear el XML, mostrando directamente la información necesaria al final:

\jupynotex[23]{Chapters/manipulacion_datos/code/textos.ipynb}

En el método de inicialización tenemos algunas estructuras necesarias para el procesamiento, basadas en el concepto de ``rama'' del árbol del XML: una secuencia de nodos desde una ``raíz'' hasta la ``hoja'' que necesitamos:

\begin{itemize}
    \item \texttt{data}: un diccionario donde guardaremos los resultados del procesamiento
    \item \texttt{for\_text} y \texttt{for\_attribs}: dos diccionarios donde la clave es la rama para indicar la hoja que nos interesa (en el primer caso guardar su texto y en el segundo sus atributos) y el valor es a su vez la clave con que guardaremos esa información en el atributo \texttt{data}.
    \item \texttt{branch}: una lista que nos representa "la rama actual"
\end{itemize}

En \texttt{startElement} y \texttt{endElement} agregamos y sacamos el nodo en cuestión de \texttt{self.branch} para construir/mantener en qué rama estamos; además, en el primero de los dos (donde entramos al nodo y recibimos sus atributos) verificamos si la rama actual es interesante justamente para guardar esos atributos.

En \texttt{characters} recibimos el texto de los distintos nodos, el cual guardaremos si la rama está indicada para ello. En este método hay que tener en cuenta que podemos recibir el contenido de un nodo en varias llamadas, así que no alcanza con guardarlo directamente en el diccionario de resultados: tenemos que concatenar las posibles partes.

Vemos que para casos simples es mucho más fácil utilizar alguna de las dos bibliotecas anteriores. Pero el fuerte de SAX es que no tiene que cargar y procesar el XML completo de una sola vez, ya que irá llamando a nuestros métodos y será responsabilidad de nuestro código de extraer sólo la información relevante. De esta manera podemos procesar eficientemente XMLs muy grandes, lo cual sería muy costoso si hay muchos y grandes, o directamente imposible si alguno es más grande que la memoria de nuestra computadora.


\section{Trabajando con datos binarios}

Cuando mencionamos archivos de forma genérica en \ref{sec:archivos} vimos la operatoria básica: abrir, cerrar, escribir, leer; entraremos en esta sección en detalles particulares al trabajar con formatos binarios.

Habiendo dicho eso, volveremos a mostrar aquí cómo escribir bytes en un archivo aunque sea similar a lo que ya vimos.

\jupynotex[1]{Chapters/manipulacion_datos/code/binarios.ipynb}

Para leer, a diferencia de cuando trabajamos con texto, en general no iteraremos el \textit{file handler}; el mismo sí es iterable, y nos devolverá el valor de cada byte uno a uno, pero es raro necesitar los bytes individualmente.

Además de leer \textit{todo} el archivo (como recién mostramos para visualizar lo que acabamos de escribir) también podemos leer por partes. Si al \texttt{read} le pedimos la cantidad de bytes devolverá solamente lo indicado (a menos que se le termine un archivo, en cual caso devolverá todo lo que pueda), dejando un puntero en donde dejó de leer. Si volvemos a leer, leerá desde allí (y si no indicamos la cantidad, leerá desde esa posición hasta el final).

\jupynotex[2-5]{Chapters/manipulacion_datos/code/binarios.ipynb}

A menos que el archivo sea trivial siempre estaremos leyendo sólo lo que necesitamos y no todo el binario en una sola acción. En función de la estructura de los datos a leer podemos incluso evitar leer secuencialmente el archivo para llegar a la parte que necesitamos: si sabemos en qué posición del archivo está la información que necesitamos podemos mover el puntero hasta allí y leer.

\jupynotex[6]{Chapters/manipulacion_datos/code/binarios.ipynb}

En el ejemplo mostrado el \texttt{seek} posiciona al puntero en la cuarta posición desde el \textit{principio} del archivo. Pero también podemos movernos relativamente a la posición actual o al final del archivo:

\jupynotex[7]{Chapters/manipulacion_datos/code/binarios.ipynb}

No es raro al trabajar con archivos binarios ir ``saltando'' por distintas partes del archivo para leer sólo lo necesario (en contraposición a una lectura puramente secuencial, como es normal en los archivos de texto).

Más allá de la forma de lectura, el siguiente paso es ``interpretar'' la información que estamos leyendo. Normalmente no nos interesan los bytes ``crudos'' al leer un archivo binario, sino convertirlos a números, texto, o las estructuras pertinentes para los tipos de datos con los que estamos trabajando.

Para ilustrar esto tomemos una secuencia de bytes simple (que grabamos primero en el archivo para simular el ejemplo) y luego leemos en función de la siguiente estructura predefinida:

\begin{itemize}
    \item el primer byte nos indica la longitud de un texto guardado (codificado en UTF-8)
    \item el segundo byte nos indica la cantidad de números de 32 bits a encontrar
    \item luego viene el texto (longitud variable, indicada al principio)
    \item y por último los números que mencionamos
\end{itemize}

\jupynotex[8]{Chapters/manipulacion_datos/code/binarios.ipynb}

Vemos que luego de abrir el archivo leemos un sólo byte y lo interpretamos como número para el largo del texto, y luego hacemos lo mismo para la cantidad de números. Para obtener el texto leemos la cantidad necesaria de bytes y decodificamos. Para los números vamos leyendo de a 4 bytes y convirtiendo a entero (indicando que están ordenados según \textit{Big-endian}). Más allá de dos líneas de \textit{debug} que dejamos para que se entienda mejor el proceso, finalmente mostramos los datos de forma ``elegante''.

\begin{info}
Al interpretar una secuencia de bytes como números tenemos que saber si estamos leyendo primero los bytes más o menos significativos. Por ejemplo, el número 23875 se descompone en dos bytes con valores 93 y 67 (porque $93 \times 256 + 67 = 23875$), en hexadecimal \texttt{0x5d} y \texttt{0x43}. Estos dos bytes los podemos escribir como \verb|b'\x5d\x43'| (el más significativo primero, lo que se denomina \textit{Big-endian}) o \verb|b'\x43\x5d'| (el menos significativo primero, llamado \textit{Little-endian}). Dato curioso: estos nombres provienen de la novela ``Los viajes de Gulliver'', del siglo XVIII.
\end{info}

Esta técnica es sencilla y nos puede resultar efectiva para estructuras pequeñas. Pero para casos más complejos tiende a ser muy propensa a errores, porque las distintas lecturas e interpretaciones están distribuidas a lo largo de varias líneas de código.

Hay una forma más simple de realizar esto, usando la función \texttt{unpack} del módulo \texttt{struct}, que nos permite especificar el formato de la estructura que queremos interpretar desde la cadena de bytes y luego hace todo el trabajo:

\jupynotex[9]{Chapters/manipulacion_datos/code/binarios.ipynb}

La primera vez que lo usamos es para sacar las dos cantidades: el largo del texto y cuantos números hay. El formato usado es \texttt{'BB'}, que indica que hay dos enteros sin signo de 1 byte cada uno.

Luego armamos un formato más complejo, donde con \verb|'>'| indicamos que es \textit{Big-endian} (tiene que estar al principio del formato), luego el largo del texto y \texttt{'s'} para indicar una cadena de cuantos bytes, y finalmente la cantidad de números y \texttt{'i'} para la cantidad de enteros con signo de 4 bytes de largo cada uno. Termina armando \texttt{'>5s7i'}, que nos devolverá una cadena de cinco bytes y los siete números. Cabe aclarar que la repetición del formato \texttt{'s'} funciona distinto que la de la mayoría de los otros formatos (como el \texttt{'i'}); en el primer caso devuelve \textit{una} cadena de N de largo, mientras que en el caso más común la repetición devuelve \textit{N} elementos.

Entonces cuando desempacamos usando ese formato complejo obtendremos una cadena de bytes y algunos números, los cuales asignamos respectivamente a \texttt{text} y \texttt{numbers} (que como tiene un \texttt{*} al principio del nombre consume todos los elementos remanentes, armando efectivamente una lista). Podemos validar en las líneas de \textit{debug} que obtenemos la misma información que en el ejemplo anterior, y obviamente la línea final mostrada será también igual.

Un efecto colateral fantástico de usar \texttt{struct} para interpretar estructuras de bytes (o para armarlas) es que una sola llamada a nivel Python alcanza para todo el procesamiento, que se realiza a nivel de C compilado, con lo cual es extremadamente eficiente y permite procesar una gran cantidad de información en poco tiempo.

Veamos otro ejemplo apenas más complejo pero más real: interpretar el contenido de una imagen con formato PNG.

\jupynotex[10]{Chapters/manipulacion_datos/code/binarios.ipynb}

Al principio del código definimos dos estructuras que nos servirán para parsear el campo \texttt{IHDR} de un PNG (que nos da información sobre la imagen como su ancho y alto, que es lo que mostraremos); primero el formato para \texttt{struct} y luego una tupla con nombres para guardar los distintos items resultantes al parsear ese header con el formato indicado. El PNG \href{https://en.wikipedia.org/wiki/PNG}{tiene otros campos} pero en este ejemplo sólo parsearemos el ya nombrado.

Luego abrimos el archivo y antes que nada validamos que los primeros bytes del mismo correspondan a la firma que todos los PNG tienen, para confirmar que el formato es ese. Luego entramos en un bucle ``infinito'' realizando los siguientes pasos:

\begin{itemize}
    \item tratamos de leer ocho bytes; si no obtuvimos nada es que se terminó el archivo y salimos
    \item interpretamos esos ocho bytes como un entero de 32 bits (el largo de la cantidad de bytes a continuación) y una cadena de cuatro bytes-caracteres (su tipo); mostramos ambos datos
    \item leemos la porción de bytes que viene a continuación, usando el largo recién obtenido
    \item si el tipo es \texttt{IHDR}, lo interpretamos como dos enteros de 32 bits y cuatro enteros de 8 bits, según la estructura definida arriba, metiendo todos esos datos directamente en una \texttt{namedtuple} para poder acceder a la info por nombre
    \item de la estructura \texttt{IHDR} mostramos el ancho y alto del PNG
    \item consumimos los 4 bytes de la verificación del bloque (que no usamos en este simple ejemplo)
\end{itemize}


Veamos otro caso donde leemos directamente desde un dispositivo de hardware: el mouse. Al mismo podemos acceder, en Linux, abriendo un \textit{path} especial (con permisos de \textit{root}) como si fuese un archivo:

\pyfile{Chapters/manipulacion_datos/code/raton.py}

Para leer del dispositivo tenemos una estructura de ``bucle infinito hasta que se interrumpe con CTRL-C'', donde en cada ciclo leemos 3 bytes; cada uno es un número entero: el primero tiene información de los botones, y el segundo y tercero el delta X e Y de la posición. Interpretamos y mostramos.

Cabe la pena mencionar que aunque \texttt{struct} nos facilita enormemente el interpretar distintas secuencias de bytes no nos ayuda para interpretar distintos bits dentro de un mismo byte. Es por eso que en las líneas 9 a 11 tenemos que aplicar distintas máscaras para detectar si algunos de los tres bits menos significativos (que corresponden a los tres botones más normales de un mouse) están activados.

Una salida posible de este programa, al mover un poco el mouse y luego apretar el botón derecho, sería algo como lo siguiente:

\begin{shell}
\$ sudo python3 raton.py
x=0, y=-1, left=0, middle=0, right=0
x=-1, y=-1, left=0, middle=0, right=0
x=-1, y=0, left=0, middle=0, right=0
x=-3, y=-2, left=0, middle=0, right=0
x=-1, y=0, left=0, middle=0, right=0
x=-2, y=0, left=0, middle=0, right=0
x=-1, y=-1, left=0, middle=0, right=0
x=-2, y=0, left=0, middle=0, right=0
x=-1, y=0, left=0, middle=0, right=0
x=-2, y=-1, left=0, middle=0, right=0
x=-2, y=0, left=0, middle=0, right=0
x=-2, y=-1, left=0, middle=0, right=0
x=0, y=0, left=0, middle=0, right=1
\end{shell}


\subsection{Un formato binario muy usado: HDF5}

El formato \texttt{HDF5} es la versión 5 del HDF (\textit{Hierarchical Data Format}, ``formato jerárquico de datos''), un formato de archivo diseñado para almacenar y organizar grandes cantidades de datos.

En esta versión la estructura del archivo está simplificada para incluir dos tipos de objetos: los conjuntos de datos (\textit{datasets}) dispuestos como arreglos multidimensionales con el tipo definido, y grupos, que son básicamente contenedores con un nombre que dentro pueden tener a los conjuntos de datos u otros grupos.

La biblioteca \texttt{h5py} representa toda esa estructura como un diccionario, donde cada clave será el nombre de un grupo y el valor su contenido, que será otro diccionario si es un grupo o directamente el conjunto de datos.

Para ejemplificar su uso abriremos y usaremos un archivo HDF5 con \href{https://data.nrel.gov/submissions/70}{información atmosférica de Estados Unidos} (descargamos el \texttt{all.h5} de allí). Abramos entonces el archivo y exploremos cuales son sus grupos en el primer nivel:

\jupynotex[11]{Chapters/manipulacion_datos/code/binarios.ipynb}

Cabe aclarar que al archivo lo podemos abrir usando la sintaxis de administrador de contexto (como ya vimos \ref{sec:contextmanagers} para otros archivos), pero en este caso lo abrimos de forma clásica para poder mostrarlo mejor en estos ejemplos.

Este archivo es bastante simple, tiene directamente los conjuntos de datos sin ningún subgrupo:

\jupynotex[12]{Chapters/manipulacion_datos/code/binarios.ipynb}

Podría ser incluso más simple, ya que casi todos los arreglos tienen una dimensión extra (de valor uno) que no tiene sentido. Veamos cómo acceder en los distintos casos y exploremos apenas los valores del grupo \texttt{latitude}:

\jupynotex[13]{Chapters/manipulacion_datos/code/binarios.ipynb}

HDF5 permite agregarle atributos a cada conjunto de datos (para ``anotarlos'' con cualquier información relevante), así como también nombrar sus distintas dimensiones, pero en este archivo esas facilidades no fueron utilizadas:

\jupynotex[14]{Chapters/manipulacion_datos/code/binarios.ipynb}

Los \textit{datasets} de \texttt{HDF5} son conceptualmente similares a los arreglos de NumPy \ref{ch:numpy} y \texttt{hp5y} nos da un objeto que se les parece mucho, pero no lo son exactamente. De cualquier manera es trivial crear un arreglo de NumPy a partir del \textit{dataset} para hacer análisis más complejos, como el siguiente ejemplo donde calculamos la media de temperatura para los puntos con latitud entre 27 y 28 y en cualquier longitud:

\jupynotex[15-18]{Chapters/manipulacion_datos/code/binarios.ipynb}

Tomamos las latitudes como arreglo de NumPy para poder crear una máscara con \mip{True} cuando el valor está entre 27 y 28, mostrando que hay más de cincuenta mil puntos con esa característica. Luego tomamos los valores medios de cada punto, pero como ese \textit{dataset} tiene una dimensión extra que no queremos tenemos que usar \texttt{squeeze} de NumPy, viendo cómo nos queda un arreglo listo para usar. Luego aplicamos la máscara quedándonos con los valores que corresponden a la latitud que nos interesa, y calculamos el valor medio del total de esos datos.


\section{Pandas} \label{sec:pandas}

La biblioteca Pandas se ha establecido como la gran herramienta para trabajar con tablas de datos, ofreciendo utilidades que permite realizar diversos procesamientos de forma simple pero poderosa.

Se enfoca en permitir manejar tablas de dos dimensiones. A priori parece una restricción pero tengamos en cuenta que esto modela exactamente la mayoría de las fuentes de datos como pueden ser bases de datos, planillas de cálculo, archivos separados por coma, etc. Pandas denomina \textit{DataFrame} a esta tabla de datos.

Cada \textit{dataframe} puede ser considerado entonces como un conjunto de columnas donde cada una de estas últimas será una serie de escalares de algún tipo de datos específico. A estas se las llama \textit{Series}.

Más allá de esas dos denominaciones puntuales y en función de la naturaleza tabular de la estructura de datos, al trabajar con Pandas también es costumbre hablar de ``columnas'' (una serie de datos puntual) y ``filas'' (los datos para la misma posición de las distintas columnas).

\begin{center}
    \includegraphics[width=300pt,keepaspectratio=true]{Chapters/manipulacion_datos/imgs/dataframe.pdf}
\end{center}

Veamos estos conceptos en un ejemplo. Descargamos un CSV de \href{https://www.datos.gob.ar/dataset/salud-tasa-natalidad}{la tasa de Natalidad por 1000 habitantes registrada en la República Argentina} y lo abrimos con Pandas:

\jupynotex[1]{Chapters/manipulacion_datos/code/pandas.ipynb}

Uno de los primeros pasos al trabajar con estructuras de datos es entender exactamente esas estructuras: qué columnas hay y con qué tipo de dato cada una, si contienen valores nulos, cuantas filas hay en total, etc.

Podemos pedirle a Pandas que nos de un resumen:

\jupynotex[2]{Chapters/manipulacion_datos/code/pandas.ipynb}

Vemos que tenemos un dataframe con 23 filas (numeradas del 0 al 22, también podemos verlas con \texttt{df.index}) y 26 columnas (también podemos ver sus títulos con \texttt{df.columns}) donde todas menos la primera (las natalidades) son punto flotante y esa primera (el índice de tiempo) es un objeto genérico; el conteo de ``no nulas'' es igual al total de filas, o sea que tenemos una tabla completa.

A cada una de esas columnas (\textit{series}) podemos referenciarlas por su nombre o título; aquí vemos los valores de la natalidad de toda Argentina (los primeros cinco, por brevedad):

\jupynotex[3]{Chapters/manipulacion_datos/code/pandas.ipynb}

También podemos mencionar varias columnas, lo que efectivamente nos da otro dataframe (¿copiando los valores o es una vista que sólo los referencia? No lo podemos saber de antemano, Pandas hace lo más eficiente según el momento en cada caso):

\jupynotex[4]{Chapters/manipulacion_datos/code/pandas.ipynb}

Para entender las columnas de punto flotante (que son los valores medidos) podemos calcular un análisis de cada serie de manera muy sencilla:

\jupynotex[5]{Chapters/manipulacion_datos/code/pandas.ipynb}

Para filtrar parte de la tabla podemos trabajar con máscaras como ya vimos con NumPy:

\jupynotex[6]{Chapters/manipulacion_datos/code/pandas.ipynb}

Volviendo al \textit{dataframe} original, podemos hacer referencia a secciones de la tabla también por índice a través del método \texttt{iloc} (por \textit{integer-location}), donde usamos la sintaxis de \textit{slicing} también de NumPy (que es la normal de Python llevada a varias dimensiones) que para dos dimensiones podemos señalar como \texttt{rows\_slice,cols\_slice}.

En el siguiente paso entonces hacemos referencia a todas las filas (nada antes de la coma) y desde la segunda hasta la séptima columna, efectivamente obteniendo toda la info de las primeras cinco provincias (mostramos las primeras tres filas por brevedad, como en muchos de los próximos ejemplos):

\jupynotex[7]{Chapters/manipulacion_datos/code/pandas.ipynb}

Los \textit{dataframes} tienen muchos métodos para realizar operaciones en ellos, y en general nos permiten indicar sobre qué eje se realiza la operación. Por ejemplo para sacar el mínimo y máximo de cada fila le indicamos que es en el eje 1, y obtenemos una nueva columna que incorporamos directamente a nuestra tabla general de natalidades:

\jupynotex[8]{Chapters/manipulacion_datos/code/pandas.ipynb}

Allí también vemos como sacamos una nueva tabla de la general (\texttt{reduced}) (porque ya no nos interesa el dato de cada provincia). Esta nueva tabla ``reducida'' de la general tiene solamente las cuatro columnas que indicamos, y podemos seguir operando sobre ella.

Pandas nos permite realizar operaciones entre las columnas (elemento por elemento a través de las filas), por ejemplo para agregar una nueva columna \texttt{range} que es la diferencia entre el máximo y el mínimo de cada fila:

\jupynotex[9]{Chapters/manipulacion_datos/code/pandas.ipynb}

También tenemos acceso al atributo \texttt{str} para manejar cada valor como si fuese una cadena (con los métodos de las cadenas que estamos acostumbrados o directamente obteniendo parte de la misma como estamos haciendo en el ejemplo) e incluso un \texttt{apply} al que le podríamos pasar una función de Python creada por nosotros.

Noten como usamos otra sintaxis para crear una nueva columna en la tabla. Esto es un requerimiento de Pandas porque a priori no se sabe si \texttt{reduced} es efectivamente una vista de \texttt{nats} o una copia de una sección de la misma (Pandas resuelve qué hacer en cada caso en tiempo de ejecución).

Una vez que están los datos procesados en general vamos a querer sacar algún gráfico o guardarlos para un procesamiento posterior.

Pandas tiene una muy buena integración con Matplotlib, la biblioteca de gráficos que vimos en otras secciones del libro. Vemos como podemos plotear directamente desde un \textit{dataframe}, incluso pasando opciones útiles para generar el gráfico según queramos.

\jupynotex[10]{Chapters/manipulacion_datos/code/pandas.ipynb}

Incluso obtenemos el \textit{plot} de Matplotlib, así que en realidad tenemos todo su potencial al alcance de la mano.

\jupynotex[11]{Chapters/manipulacion_datos/code/pandas.ipynb}

Si quisiéramos trabajar los datos a más bajo nivel podemos obtener un arreglo multidimensional de NumPy sin esfuerzo:

\jupynotex[12]{Chapters/manipulacion_datos/code/pandas.ipynb}

También podemos exportar los datos a diversos formatos, como por ejemplo los que ya vimos en este capítulo CSV y HDF5, pero hay muchas más opciones.

\jupynotex[13-15]{Chapters/manipulacion_datos/code/pandas.ipynb}

Para terminar, unas palabras sobre el proceso de instalación de Pandas. Aunque es trivial instalarlo (usando \texttt{pip install} en un entorno virtual como ya vimos en \ref{sec:virtualenvs}) tenemos que ser conscientes que hay distintas ``opciones'', donde cada opción instalará dependencias en función de la utilización que vamos a hacer de la biblioteca. Por ejemplo, si vamos a usar Pandas para realizar gráficos a partir de los datos necesitamos hacer \texttt{pip install pandas[plot]}, si vamos a exportar o importar datos de planillas de cálculo haremos \texttt{pip install pandas[excel]}, etc.

Pueden revisar las distintas \href{https://pandas.pydata.org/docs/getting_started/install.html#optional-dependencies}{dependencias opcionales}, aunque siempre es más sencillo (pero menos eficiente) indicarle que traiga directamente todo con \texttt{pip install pandas[all]}.
